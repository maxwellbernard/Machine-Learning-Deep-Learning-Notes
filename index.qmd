---
title: "ML Class Notes"
author: "Maxwell Bernard"
output-dir: docs
format:
    html:
        toc: true
        toc-depth: 3
        toc-expand: true
        css: styles.css
        code-block-line-numbers: true
        code-block-wrap: true
        code-overflow: wrap
        code-output-overflow: wrap
        theme: default
        code-block-theme: default
        highlight-style: pygments
        self-contained: true
---

# Lecture 9

## Notation for Neural Networks

- **Observed Data**:
  - The input data matrix $X \in \mathbb{R}^{n \times d}$, where $n$ is the number of samples (rows) and $d$ is the number of features (columns). 
  - The output label vector for supervised learning $y \in \mathbb{R}^{n \times c}$, where $c$ is the number of classes.
  - The output label vector for unsupervised learning $y \in \mathbb{R}^{n}$, where $n$ is the number of samples.
  - Each y in the output label vector corresponds to a sample in the input data matrix $X$.

- **Latent Feature**:
  - The latent feature matrix $Z \in \mathbb{R}^{n \times k}$, where $k$ is the number of latent features. 
  - The latent feature matrix is used to represent the input data in a lower-dimensional space. They are hidden variables that are not directly observed but are inferred from the observed data.

- **Model Parameters**:
  - The model parameters are represented by $\theta$, which can include weights, biases, and other hyperparameters. 
  - The model parameters are learned from the training data using optimization algorithms such as gradient descent.
  - The model weights are represented by $W \in \mathbb{R}^{d \times k}$, where $d$ is the number of features and $k$ is the number of latent features. 
  - The model weights are used to transform the input data into the latent feature space.
  - V is the weight matrix for the output layer, which maps the latent features to the output labels, it is represented by $V \in \mathbb{R}^{k \times c}$, where $c$ is the number of classes. Why is it a one for one mapping? Because the output labels are one-hot encoded, meaning that each sample belongs to only one class.

## Supervised Learning Process

- **Standard supervised learning process:**
  - Learn parameters 'w' based on original features $x_i$ and target labels $y_i$.
- **Change of Basis:**
  - Learn parameters 'w' based on change of basis features $z_i$ and target labels $y_i$. So instead of learning the parameters based on the original features, we learn the parameters based on the latent features, $z_i$, which we represent as v, the latent feature matrix.
- **Latent Features:**
  - The latent features are learned from the original features using a transformation weight matrix $W$, so that $z_i = W^T x_i$.
  - W maps the original input features to the latent feature space (a change of basis).
  - The transformation weight matrix $W$ is learned from the training data using optimization algorithms such as gradient descent. The optimization process aims to minimize the loss function, ensuring that the predicted labels align closely with the actual labels.
  - Learn W based on original features $x_i$ (unsupervised) and target labels $y_i$ (supervised).
- **Neural Networks:**
  - Join the two processes together to learn the parameters 'W' and  'v' simultaneously.
  - $W$ is the transformation weight matrix that maps the original input features to the latent feature space
  - $v$ is the weight matrix for the output layer that maps the latent features to the output labels.

## Linearity and Non-Linearity
- **Linear latent-factor model with linear regression:**
  - $y_i = W^T x_i + \epsilon_i$, where $\epsilon_i$ is the error term.
  - The model is linear in both the input features and the latent features.
  - Use features from latent-factor model: $z_i = W x_i$.
  - Make predictions using linear model: $y_i = v^T z_i + \epsilon_i$. $v^T$ is the weight matrix for the output layer that maps the latent features to the output labels. It is transposed as it is a row vector and $z_i$ is a column vector, which means that the two can be multiplied together using dot product.

- **Linear latent-factor model with logistic regression:**
  - $y_i = \sigma(W^T x_i + \epsilon_i)$, where $\sigma$ is the sigmoid function and $\epsilon_i$ is the error term.
  - The model is linear in both the input features and the latent features.
  - Use features from latent-factor model: $z_i = W x_i$.
  - Make predictions using logistic model: $y_i = \sigma(v^T z_i + \epsilon_i)$. $v^T$ is the weight matrix for the output layer that maps the latent features to the output labels. It is transposed as it is a row vector and $z_i$ is a column vector, which means that the two can be multiplied together using dot product.
  
- **Non-linear latent-factor model:**
  -  Transform $z_i$ using a **non-linear activation function $h$** to get $h(z_i)$.
  -  Common choicce of non-linear function is the **sigmoid function**, **tanh function**, or **ReLU function**. And it is applied element-wise to the latent features.
  -  The resulting equation is:
     - $y_i = h(W^T x_i) + \epsilon_i$, where $\epsilon_i$ is the error term.
  -  The model is non-linear in both the input features and the latent features.
  -  Use features from latent-factor model: $z_i = W x_i$. 

- **Training a neural network:**
  - Apply stochastic gradient descent (SGD) for optimization, to compute gradient of the loss function with respect to the model parameters (W, the weights for feature learning and v, the weights for output layer (prediction)).  
  - The loss function is typically the mean squared error (MSE) for regression tasks or cross-entropy loss for classification tasks.
  - The optimization process aims to minimize the loss function, ensuring that the predicted labels align closely with the actual labels.
  - It updates them iteratively based on the gradients computed from the loss function, to minimize the error between the predicted and actual labels. 

- **Neural network with 3 hidden layers:**
  - $y_i = h_3(h_2(h_1(W^T x_i))) + \epsilon_i$, where $\epsilon_i$ is the error term.
  - The model is non-linear in both the input features and the latent features.
  - Use features from latent-factor model: $z_i = W x_i$. 
  - The resulting equation is:
     - $y_i = h_3(h_2(h_1(z_i))) + \epsilon_i$, where $\epsilon_i$ is the error term.
  - For 'm' hidden layers, the equation is:
     - $y_i = h_m(h_{m-1}(h_{m-2}(...h_1(z_i)...))) + \epsilon_i$, where $\epsilon_i$ is the error term.
     - It shows that the network is a composition of multiple non-linear functions, which allows it to learn complex relationships between the input features and the output labels. It is essentially a function composed of many sub-functions, each of which is a non-linear transformation of the previous one.

## Activation Functions
- Defines how weighted sum of inputs is transformed into output of neuron from a node (layer) in a neural network.
- All layers use the same activation function, for simplicity and stability across the network.
- Output layer uses a different activation function from the hidden layers. For example:
- 
  - Sigmoid function for binary classification
  - Softmax function for multi-class classification
  - Linear function for regression tasks

- Large DNN (deep neural network) with non-linear activation functions can approximate any continuous function.
  - This is known as the **universal approximation theorem**, which states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets of $\mathbb{R}^n$, given appropriate weights and biases.
- **Sigmoid** function (S shaped) function, historically the most common activation function, but can cause vanishing gradient problem.
- **Tanh** function (hyperbolic tangent) is a scaled version of the sigmoid function, which outputs values between -1 and 1. It is zero-centered, meaning that it can output negative values, which can help with convergence during training.
- **ReLU (Rectified Linear Unit)** function is a piecewise linear function that outputs the input directly if it is positive; otherwise, it outputs zero. It is *computationally efficient* and helps mitigate the vanishing gradient problem, making it a popular choice for deep learning models.
- **Softmax** function is used in the output layer of multi-class classification problems. It converts the raw output scores (logits) into probabilities by exponentiating each score and normalizing them by dividing by the sum of all exponentiated scores. The softmax function ensures that the output probabilities sum to 1, making it *suitable for multi-class classification tasks*, such as image classification or natural language processing tasks. eg Cat or Dog.

## Bias
Bias, Variance and Irreducible Error (due to noisiness of data) are the three sources of error in a model, which make up the **Generalization Error** (how well the model performs on unseen data).

Bias is the error introduced by approximating a real-world problem, which may be complex, by a simplified model. It is the difference between the expected prediction of the model and the true value we are trying to predict.

- **High Bias** = model too simple, leading to underfitting of the training data.
  - eg fitting a straight line to data thats not linear.
  - leads to high training error and high test error.
- **Low Bias** = model too complex, leading to overfitting of the training data.
  - eg fitting a complex curve to data that is linear.
  - leads to low training error but high test error.

- **Bias-Variance Tradeoff**: 
  - The bias-variance tradeoff is the balance between bias and variance in a model. 
  - A model with high bias pays little attention to the training data and oversimplifies the model, leading to high error on both training and test data (underfitting).
  - A model with high variance pays too much attention to the training data and captures noise in the data, leading to low error on training data but high error on test data (overfitting).
  - The goal is to find a model that minimizes both bias and variance, achieving good generalization performance on unseen data.

- When adding bias, we choose the value of 1 as the bias term, which is a constant value added to the weighted sum of inputs. The bias term allows the model to fit the data better by shifting the activation function to the left or right, enabling it to learn more complex relationships between the input features and the output labels.

- LinearSVC class from sklearn.svm is a linear support vector classifier that uses a linear kernel to separate the data into different classes. It is used for binary classification tasks, where the goal is to find a hyperplane that separates the two classes in the feature space. The LinearSVC class uses the hinge loss function to optimize the model parameters and minimize the classification error. It is suitable for large datasets and high-dimensional feature spaces, as it is computationally efficient and can handle large-scale problems.

- This model in Scikit-learn includes regularization, which helps to prevent overfitting by adding a penalty term to the loss function. The regularization parameter can be adjusted to control the amount of regularization applied to the model.

## TLUs
- a Threshold Logic Unit (TLU) is a simple model of a neuron that computes a weighted sum of its inputs and applies a threshold function to produce an output. The TLU can be represented mathematically as:
  - $y = \begin{cases} 1 & \text{if } w^T x + b > 0 \\ 0 & \text{otherwise} \end{cases}$, where $w$ is the weight vector, $x$ is the input vector, and $b$ is the bias term.
- The TLU can be used to model binary classification problems, where the goal is to classify the input data into two classes based on the weighted sum of the inputs. The TLU can be trained using supervised learning algorithms, such as gradient descent or stochastic gradient descent, to learn the optimal weights and bias term that minimize the classification error.
- Unit has a threshold $\theta$ and each input $x_i$ has a weight $w_i$.
- TLUs are the foundation of Perceptrons and MPLs, simplyfing complex biological neurons to a simple model.
- It is like a light switch, where the input is the current and the output is the light. The TLU turns on (outputs 1) when the weighted sum of inputs exceeds a certain threshold (the current is above a certain level), and turns off (outputs 0) when the weighted sum of inputs is below the threshold (the current is below a certain level).

## Perceptron
- Simpliest ANN (Artificial Neural Network) model, consisting of a single layer of TLUs.
- The Perceptron is a type of linear classifier that uses a single layer of TLUs to classify input data into two classes. It is a supervised learning algorithm that learns the optimal weights and bias term from the training data using the perceptron learning rule.
- Each TLU computes a weighted sum of its inputs and applies a threshold function to produce an output.
- Each TLU connects to all inputs, it computes the linear combination of the inputs and weights, and applies a non-linear activation function to produce the output.
- An extra bias feature is generally added to the input vector, which is a constant value of 1. This allows the model to fit the data better by shifting the activation function to the left or right, enabling it to learn more complex relationships between the input features and the output labels.

-**Training the Perceptron:**
  - Step 1: Perceptron is fed one training example at a time, and the weights are updated based on the error between the predicted output and the actual output.
  - Step 2: For every output neuron that produced a wrong prediction, it updates the weights of the inputs to that neuron that would have contributed to the correct prediction.
  - Uses heavside step function as activation function, which outputs 1 if the weighted sum of inputs is greater than a certain threshold, and 0 otherwise.
  - It is similiar to SGD Classifier as it updates the weights based on the error between the predicted output and the actual output, but it is a simpler model that uses a single layer of TLUs to classify input data into two classes.

-**Limitations:**
  - The Perceptron can only learn linearly separable functions, meaning that it can only classify input data that can be separated by a straight line (or hyperplane) in the feature space. 
  - It cannot learn non-linear functions, such as XOR, which requires a more complex model with multiple layers of TLUs or non-linear activation functions.
  - The Perceptron is sensitive to the choice of learning rate and the initial weights, which can affect the convergence of the algorithm and the final model performance.
  - Perceptrons do not output a class probability, but rather a binary classification (0 or 1). This means that they cannot be used for multi-class classification problems without additional modifications.

## Multi-Layer Perceptron (MLP)

A Multi-Layer Perceptron (MLP) is a type of artificial neural network (ANN) that consists of multiple layers of neurons, including an input layer, one or more hidden layers, and an output layer. Each layer is composed of multiple neurons (or TLUs), and each neuron in a layer is connected to **all neurons in the previous layer**. 

The MLP **uses non-linear activation functions** to introduce non-linearity into the model, allowing it to learn complex relationships between the input features and the output labels.

Every input/hidden layer includes a bias neauron with a constant value of 1, which allows the model to fit the data better by shifting the activation function to the left or right, enabling it to learn more complex relationships between the input features and the output labels.

## Feedforward Neural Network

A feedforward neural network is a type of artificial neural network where the connections between the neurons do not form cycles. The **information flows in one direction**, from the input layer to the output layer, without any feedback loops. 

Each neuron in a layer receives inputs from the previous layer, applies a weighted sum and an activation function, and passes the output to the next layer. The feedforward neural network is trained using supervised learning algorithms, such as backpropagation, to learn the optimal weights and biases that minimize the classification error.

## Backpropagation
- Computes neural network gradients using the chain rule of calculus.
- The backpropagation algorithm is used to compute the gradients of the loss function with respect to the model parameters (weights and biases) in a feedforward neural network. It uses the chain rule of calculus to propagate the error backward through the network, layer by layer, and update the weights and biases using gradient descent or stochastic gradient descent.
- The backpropagation algorithm consists of two main steps:
  - **Forward Pass**: Compute the output of the network for a given input by passing the input through each layer and applying the activation functions. This step calculates the predicted output and the loss function.
  - **Backward Pass**: Compute the gradients of the loss function with respect to the model parameters (weights and biases) by applying the chain rule of calculus. This step updates the weights and biases using gradient descent or stochastic gradient descent.
- The backpropagation algorithm is efficient and can be applied to large neural networks with many layers and neurons. It is a key component of training deep learning models and allows them to learn complex relationships between the input features and the output labels.
- The backpropagation algorithm is crucial for optimizing the performance of neural networks, enabling them to generalize well on unseen data.

**Cost of Backpropagation**:
The total cost of backpropagation over the entire traning set is O(n * m), where n is the number of training examples and m is the number of weights in the network. This assumes a forward pass cost of O(m) and a backward pass cost of O(m) for each training example.

- A more detailed analysis of the cost of backpropagation is as follows:
  - For each layer, the cost of computing the weighted sum of inputs and applying the activation function is O(dk), where d is the number of features in the input layer and k is the number of neurons in the hidden layers.
  - The cost of computing the gradients for each layer is O(mk), where m is the number of layers and k is the number of neurons in the hidden layers.
  - The total **cost of backpropagation per pass is O(dk + mk^2)**, where d is the number of features in the input layer and k is the number of neurons in the hidden layers.

## Gradient Problem
The gradient problem refers to the challenges faced during the training of deep neural networks, particularly when using gradient-based optimization algorithms such as stochastic gradient descent (SGD). The two main issues associated with the gradient problem are:

**Vanishing Gradient Problem**: In deep networks, the gradients of the loss function with respect to the model parameters can become very small as they are propagated backward through the layers. This can lead to slow convergence or even prevent the model from learning altogether, especially in the earlier layers of the network. The vanishing gradient problem is particularly common when using activation functions like **sigmoid or tanh**, which can saturate and produce very small gradients for large input values.

- Solution: 
    - Use activation functions that do not saturate, such as **ReLU (Rectified Linear Unit)** or its variants (e.g., Leaky ReLU, Parametric ReLU).
    - Use techniques like **batch normalization** to normalize the inputs to each layer, which can help mitigate the vanishing gradient problem.
  
**Exploding Gradient Problem**: In contrast to the vanishing gradient problem, the exploding gradient problem occurs when the gradients become excessively large during backpropagation. This can lead to numerical instability and cause the model parameters to diverge, resulting in poor performance or failure to converge. The exploding gradient problem is often encountered in deep networks with many layers or when using certain activation functions.

- Solution:
  - Proper activation function choice such as ReLU which doe not produce large gradients.
  - Use weight regularization techniques, such as L1 or L2 regularization, to penalize large weights and prevent them from growing excessively during training.

# Lecture 10

## BatchNorm

- BN (Batch Normalization) is a technique used inside neural networks to make training faster and more stable.
- It normalizes the input to each layer (i.e., adjusts the values so they have zero mean and unit variance), then scales and shifts it — all during training.

How it works:

- For each mini-batch during training:
  - Calculate the mean and variance of the input to the layer.
  - Normalize the input (subtract mean, divide by standard deviation).
  - Then apply scaling and shifting using learnable parameters $\gamma$ and $\beta$.

Why its useful:

- It helps to reduce internal covariate shift, which is the change in the distribution of layer inputs during training. This means it keeps the inputs to each layer more stable, making it easier for the network to learn.
- It helps solve the vanishing/exploding gradient problem by keeping the inputs to each layer in a reasonable range.
- It can act as a form of regularization, reducing the need for other regularization techniques like dropout.
- It often improves accuracy and allows for higher learning rates, speeding up training.

If you add a BN layer as the very first layer of your NN model, it will normalize the input data before passing it to the first hidden layer, you do not need to standardize the input data before training, as the BN layer will take care of that. However, if you add a BN layer after the first hidden layer, you should standardize the input data before training, as the BN layer will not normalize the input data before passing it to the first hidden layer.

Limitations
- Slower training per epoch due to the extra computations.
- Not easy to use in RNNs because the batch statistics can be different for each time step.
  - **Gradient Clipping** is a technique used to prevent exploding gradients in RNNs by limiting the maximum value of the gradients during backpropagation. It helps to stabilize the training process and prevent numerical instability, but this technique does not help with the vanishing gradient problem.

## RNNs (Recurrent Neural Networks)

- RNNs are a type of neural network designed to process sequential data, such as time series or natural language. 
- They have a feedback loop that allows them to maintain a hidden state, which captures information from previous time steps.
- This hidden state is updated at each time step based on the current input and the previous hidden state, allowing RNNs to learn temporal dependencies in the data.

Computations within a recurrent layer:
- The input at time step t is denoted as $x_t$, the hidden state at time step t is denoted as $h_t$, and the output at time step t is denoted as $y_t$.
- Each neuron in a recurrent layer (RL) recieves as input, output of previous layer and its current input.
- Neurons each have two weight vectors
- Neurons performe an affine transformation on the input and the previous hidden state, followed by a non-linear activation function.
- Within RL, output of neurons is moved to a dens/fully connected layer with a softmax activation function (to turn the output into probabilities), which produces the final output of the RNN at time step t.

## RNN Training: BPTT

- Standard backpropagation algorithm is not suitable for training RNNs because in RNNs, the output at each time step depends on the hidden state from the previous time step, creating a temporal dependency.

- The shared weights and temporal dependencies create a dynamic, time-dependent structure that standard backpropagation, which assumes a static layer-by-layer architecture, cannot properly optimize due to gradient instability over many time steps.

- BPTT (Backpropagation Through Time) is an extension of the backpropagation algorithm used to train RNNs.
- It involves unrolling the RNN through time, treating each time step as a separate layer in a feedforward neural network.
- The gradients are computed for each time step and then accumulated to update the weights of the RNN.
- BPTT can be computationally expensive and may suffer from the vanishing/exploding gradient problem, especially for long sequences.

-**Limitations**:
  - BPTT can be computationally expensive and may suffer from the vanishing/exploding gradient problem, especially for long sequences.
  - It requires storing the hidden states and gradients for each time step, which can lead to high memory usage.
  - BPTT is not suitable for online learning or real-time applications, as it requires processing the entire sequence before updating the weights.
  - Solution: use LTSM (Long Short-Term Memory) networks, which are designed to mitigate the vanishing/exploding gradient problem and improve the training of RNNs on long sequences.

## DL Frameworks
- Deep learning frameworks are software libraries that simplify the development, training, and deployment of deep neural networks, including architectures like CNNs,RNNs and MLPs.
- They provide high-level APIs, pre-built layers, automatic differentiation for gradient computation, and hardware acceleration (e.g., GPU support) to streamline tasks like model design, optimization, and evaluation.
- Some popular deep learning frameworks include:
  - **TensorFlow**: Developed by Google, TensorFlow is an open-source deep learning framework that provides a flexible and comprehensive ecosystem for building and deploying machine learning models. It supports both high-level APIs (like Keras) and low-level operations for custom model development.
  - **PyTorch**: Developed by Facebook's AI Research lab, PyTorch is an open-source deep learning framework that emphasizes dynamic computation graphs, making it easy to build and modify neural networks on-the-fly. It is widely used in research and production environments.
  - **Keras**: Keras is a high-level neural networks API written in Python that runs on top of TensorFlow. It provides a user-friendly interface for building and training deep learning models, making it accessible to beginners and experienced practitioners alike.
  - **MXNet**: An open-source deep learning framework developed by Apache, MXNet is designed for efficiency and flexibility. It supports both symbolic and imperative programming, allowing users to choose the best approach for their specific use case.
  - **Caffe**: Developed by the Berkeley Vision and Learning Center (BVLC), Caffe is a deep learning framework focused on speed and modularity. It is particularly well-suited for image classification tasks and has a strong community support.

## Non-Distributed vs. Distributed ML

-**Non-Distributed ML**: Non-distributed ML involves training a machine learning model on a single machine (e.g., a laptop, server, or GPU) using its local computational resources (CPU/GPU, memory, storage). The entire dataset and model computations, including forward passes, gradient calculations, and weight updates, occur on this single machine. Common frameworks like Scikit-learn, TensorFlow, or PyTorch can be used in non-distributed settings for small to medium-sized datasets and models, such as linear regression, small neural networks, or tree-based models.

-**Distributed ML**: Distributed ML involves training a machine learning model across multiple machines or nodes, leveraging their combined computational resources to handle larger datasets and more complex models. This approach is essential for scaling up training processes, especially in deep learning, where models can be very large and require significant computational power. Distributed ML frameworks like TensorFlow and PyTorch provide tools for parallelizing computations, synchronizing gradients, and managing data across multiple devices or clusters. Distributed ML can be implemented using data parallelism (splitting the dataset across machines) or model parallelism (splitting the model across machines).

## TensorFlow
- TensorFlow is an open-source deep learning framework developed by Google that provides a flexible and comprehensive ecosystem for building and deploying machine learning models. It supports both high-level APIs (like Keras) and low-level operations for custom model development.
- TensorFlow is designed to be efficient and scalable, allowing users to train models on a single machine or across multiple devices (CPUs, GPUs, TPUs) and even in distributed environments. It provides tools for automatic differentiation, optimization, and model evaluation, making it suitable for a wide range of applications, including computer vision, natural language processing, and reinforcement learning.
- Its core is very similar to NumPy, but it uses tensors (multi-dimensional arrays) instead of arrays. Tensors are the fundamental data structure in TensorFlow and can be manipulated using various operations, similar to NumPy arrays.
- Core features include:
  - **Tensors**: Multi-dimensional arrays that represent data in TensorFlow. Tensors can have different ranks (dimensions) and shapes, allowing for flexible representation of data.
  - **Graphs**: TensorFlow uses a computational graph to represent the flow of data and operations. Nodes in the graph represent operations (e.g., addition, multiplication), while edges represent the tensors flowing between them. This allows for efficient execution and optimization of computations.
  - **Sessions**: In TensorFlow 1.x, a session is used to execute the computational graph. In TensorFlow 2.x, eager execution is enabled by default, allowing for immediate evaluation of operations without the need for sessions.
  - **Keras API**: TensorFlow includes a high-level Keras API for building and training deep learning models. Keras provides a user-friendly interface for defining layers, models, and training processes, making it accessible to beginners and experienced practitioners alike.

Python example:
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model using Keras Sequential API
model = Sequential([
    Dense(units=10, activation='relu', input_shape=(20,)),  # Hidden layer: ReLU activation
    Dense(units=1, activation='sigmoid')                    # Output layer: Sigmoid for binary classification
])

# Compile the model
model.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=['accuracy'])

# Notation:
# - X: Input features, X ∈ ℝ^{n × d} (n=1000 samples, d=20 features)
# - y: Labels, y ∈ {0, 1}^n
# - W_1: Weight matrix for hidden layer, W_1 ∈ ℝ^{10 × 20}
# - b_1: Bias for hidden layer, b_1 ∈ ℝ^{10}
# - W_2: Weight matrix for output layer, W_2 ∈ ℝ^{1 × 10}
# - b_2: Bias for output layer, b_2 ∈ ℝ^{1}
# - z_1 = W_1 X + b_1: Pre-activation for hidden layer
# - a_1 = ReLU(z_1): Hidden layer activation
# - z_2 = W_2 a_1 + b_2: Pre-activation for output layer
# - ŷ = sigmoid(z_2): Predicted probability, ŷ ∈ [0, 1]
# - L: Binary cross-entropy loss, L = -1/n ∑ [y_i log(ŷ_i) + (1-y_i) log(1-ŷ_i)]

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)

# Evaluate on test set
y_pred = (model.predict(X_test) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

# Model summary
model.summary()
```

# Lecture 11

## LTSM
- LSTM (Long Short-Term Memory) networks are a type of recurrent neural network (RNN) designed to handle long-term dependencies better than traditional RNNs, particularly the vanishing gradient problem.
- They achieve this through a unique architecture that includes memory cells and gating mechanisms, allowing them to maintain and control information over long sequences.
- Components of LSTM:
  - **Cell State**: The memory of the LSTM cell, which carries information across time steps.
  - **Forget Gate**: Determines what information to discard from the cell state.
  - **Input Gate**: Decides what new information to add to the cell state (c)
  - **Output Gate**: Controls what information to output from the cell state to the next layer.

**Peephole Connections (LSTM Variant)**: Peephole connections are a variant of LSTM architecture that allows the gates to access the cell state directly. This enables the gates to make more informed decisions about what information to keep or discard, improving the model's performance on tasks with long-term dependencies.

- In code, `keras.layers.LSTM` and `keras.layers.LSTMCell` are used to build these models. The LSTMCell provides more control, while LSTM is simpler to use for stacked layers.
  - `keras.layers.LSTM`: A high-level layer that combines the LSTM cell and recurrent logic.
  - `keras.layers.LSTMCell`: A low-level cell that can be used to build custom RNN architectures.

Using `LSTM layer` (simpler and more common):
```python
model = keras.models.Sequential(
    [
        keras.layers.LTSM(20, return_sequences=True, input_shape=(None, 1)),
        keras.layers.LTSM(20, return_sequences=True),
        keras.layers.TimeDistributed(keras.layers.Dense(10)),
    ]
)
```
Explanation of code above:

- `keras.models.Sequential`: A linear stack of layers to build the model.
- `keras.layers.LSTM(20, return_sequences=True, input_shape=(None, 1))`: Adds an LSTM layer with 20 units (neurons). `return_sequences=True` means it returns the full sequence of outputs (hidden states, $h_t$) for each time step, and `input_shape=(None, 1)` specifies the input shape (None for variable time steps and 1 feature per time step (e.g., univariate time series).
- `keras.layers.LSTM(20, return_sequences=True)`: Adds another LSTM layer with 20 units, also returning the full sequence of outputs.
- `keras.layers.TimeDistributed(keras.layers.Dense(10))`: Wraps a Dense layer (fully connected layer) to apply it to each time step independently. The Dense layer has 10 units, meaning it outputs a vector of size 10 for each time step.
- The model can be trained using the same methods as other Keras models, such as `model.compile()`, `model.fit()`, and `model.evaluate()`.

Using `LSTM Celllayer`
```python
model = keras.models.Sequential(
    [
        keras.layers.LSTMCell(20, input_shape=(None, 1)),
        keras.layers.LSTMCell(20),
        keras.layers.TimeDistributed(keras.layers.Dense(10)),
    ]
)
```

## GRU & Bi-LSTM
- **GRU (Gated Recurrent Unit)** is a simplified version of LSTM that combines the forget and input gates into a single update gate, Has no output gate and merges the cell state and hidden state into a single state vector.
- It has fewer parameters than LSTM, making it computationally more efficient while still effectively capturing long-term dependencies in sequential data.
- Implemented in `keras.layers.GRU` and `keras.layers.GRUCell`.

- **Bi-LSTM (Bidirectional LSTM)** is an extension of LSTM that processes the input sequence in both forward and backward directions. This allows the model to capture context from both past and future time steps, improving performance on tasks where the entire sequence context is important, such as natural language processing and speech recognition.

**Comparison Table**:

| Feature              | LSTM                          | GRU                          | Bi-LSTM                      |
|---------------|-------------------------------|------------------------------|------------------------------|
| Architecture  | Three gates (input, output, forget) | Two gates (Update, reset)  | Two LSTMs layers (forward and backward) |
| Advantages     | Better for long-term dependencies and complex sequences | Faster and less memory-intensive | Captures context from both directions |
| Disadvantages | Slower and more memory-intensive | May not capture long-term dependencies as well | More complex and requires more resources |

## Selecting Epochs (Best Practices)
- **Early Stopping**: Monitor the validation loss during training and stop training when the validation loss starts to increase, indicating overfitting. This can be done using the `EarlyStopping` callback in Keras.

``` python
from keras.callbacks import EarlyStopping
early_stop = EarlyStopping(
  monitor='val_loss', # Monitor validation loss
  patience=5, # Stops training after 5 epochs of no improvement
  restore_best_weights=True # roll back to the best weights
)

# Fit the model with early stopping
model.fit(
  X_train,
  y_train,
  epochs=50, 
  validation_split=0.2, # 20% of data for validation
  batch_size=32, # Batch size for training
  shuffle=True, # Shuffle the training data
  verbose=1, # Print training progress
  callbacks=[early_stop]
)
```

- Training halts automatically when your model starts to overfit.

# Letcure 12

## Autoencoders
- Autoencoders are a type of neural network used for unsupervised learning, primarily for dimensionality reduction, feature extraction, and data denoising.
  
- Autoencoders can be thought of as a way to compress and then decompress data, similar to how a zip file reduces the size of files for storage and later restores them to their original form.

**Why Use Autoencoders?**

1. **Dimensionality Reduction**: Autoencoders can learn a lower-dimensional representation of the input data, which can be useful for visualization or as a preprocessing step before applying other machine learning algorithms.
   
2. **Anomaly Detection**: By training an autoencoder on normal data, it can learn to reconstruct that data well. When presented with anomalous data, the reconstruction error will be higher, allowing for anomaly detection.
   
3. **Generative Models**: Autoencoders can be used to generate new data samples by sampling from the learned latent space and decoding it back to the original space.
   
4. **Extracting Features**: Autoencoders can learn useful features from the input data, which can be used for downstream tasks like classification or clustering.

### Architecture

An autoencoder consists of two main components:

1. **Encoder**: The encoder compresses the input data into a lower-dimensional representation (latent space). It consists of one or more layers that reduce the dimensionality of the input data.
   
2. **Decoder**: The decoder reconstructs the original input data from the lower-dimensional representation. It consists of one or more layers that increase the dimensionality of the latent space back to the original input size.

- The encoder and decoder are typically symmetric, meaning that the architecture of the encoder mirrors that of the decoder.
- Has the same architecture as MLP, but the output layer has the same number of neurons as the input layer, and the activation function is usually linear (or identity) to allow for reconstruction of the original input data.

### Undercomplete Autoencoder

An undercomplete autoencoder is a type of autoencoder where the dimensionality of the latent space is smaller than the input space. This forces the model to learn a compressed representation of the input data, which can help in feature extraction and dimensionality reduction.

PCA Analogy: If only the first few principal components are used to represent the data, the PCA model can be thought of as an undercomplete autoencoder. The PCA model learns a linear transformation that projects the data onto a lower-dimensional subspace, similar to how an undercomplete autoencoder learns a compressed representation of the input data.

```python
import numpy as np
from keras.layers import Input, Dense
from keras.models import Model

input_img = Input(shape=(784,))  # Input shape (e.g., for MNIST images)
encoded = layers.Dense(32, activation='relu')(input_img)  # Encoder layer
decoded = layers.Dense(784, activation='sigmoid')(encoded)  # Decoder layer
autoencoder = Model(input_img, decoded)  # Autoencoder model
autoencoder.compile(optimizer='adam', loss='mse')  # Compile the model

# Train the autoencoder
X_train = np.random.rand(1000, 784)  # Example training data

autoencoder.fit(X_train, X_train, epochs=10, batch_size=256, shuffle=True, validation_split=0.2)  # Train the model
```

- In this example, the autoencoder is trained to reconstruct the input data (X_train) by minimizing the mean squared error (MSE) between the input and output. The encoder layer reduces the dimensionality to 32, while the decoder layer reconstructs it back to 784 dimensions.
  
- The model is compiled using the Adam optimizer and trained for 10 epochs with a batch size of 256. The training data is split into 80% for training and 20% for validation.
  
- The autoencoder learns to compress the input data into a lower-dimensional representation and then reconstruct it back to the original input space.

### Stacked Autoencoders

Stacked autoencoders (or Deep autoencoders)are a type of autoencoder architecture where multiple layers of encoders and decoders are stacked on top of each other. Each layer learns a compressed representation of the input data, allowing for deeper feature extraction and more complex representations.

**Why Stacked layers?**
- By stacking multiple layers, the model can learn hierarchical representations of the data, where each layer captures different levels of abstraction. This is similar to how deep neural networks learn features at different levels of granularity.

Pyhton example:
```python
input_img = keras.Input(shape=(784,))  # Input shape (e.g., for MNIST images)
encoded1 = layers.Dense(128, activation='relu')(input_img)  # First encoder layer
encoded2 = layers.Dense(64, activation='relu')(encoded1)  # Second encoder layer
decoded1 = layers.Dense(128, activation='relu')(encoded2)  # First decoder layer
output_img = layers.Dense(784, activation='sigmoid')(decoded1)  # Output layer

autoencoder = Model(input_img, output_img)  # Autoencoder model
autoencoder.compile(optimizer='adam', loss='mse')  # Compile the model

# Train the autoencoder
X_train = np.random.rand(1000, 784)  # Example training data
autoencoder.fit(X_train, X_train, epochs=10, batch_size=256, shuffle=True, validation_split=0.2)  # Train the model
```
- In this example, the autoencoder consists of two encoder layers (128 and 64 units) and two decoder layers (128 and 784 units). The model is trained to reconstruct the input data (X_train) by minimizing the mean squared error (MSE) between the input and output. The training process is similar to that of a single-layer autoencoder.
  
- The stacked autoencoder learns to compress the input data into a lower-dimensional representation (64 units) and then reconstruct it back to the original input space (784 dimensions) through multiple layers.

### Convoluted Autoencoders

- Designed for image data, where the input is typically a 2D image (height x width x channels). They are particularly effective for tasks like image denoising, inpainting, and **unsupervised feature learning.**

  
-  Uses convolutional layers instead of fully connected layers in the encoder and decoder. They are particularly effective for image data, as they can learn spatial hierarchies and local patterns in the input data.
  

**Architecture**:

- **Encoder:** Consists of convolutional layers that downsample the input image, reducing its spatial dimensions (heigh & width) while increasing the number of feature maps (depth). Consists of convolutional layers followed by pooling layers (e.g., max pooling) to reduce the spatial dimensions of the input image.
  
- **Decoder:** Consists of **transposed convolutional layers** (also known as deconvolutional layers) that upsample the encoded representation back to the original image size. The decoder uses transposed convolutional layers (also known as deconvolutional layers) to upsample the encoded representation back to the original image size.
  
- The output layer typically uses a sigmoid activation function to produce pixel values in the range [0, 1].

```python
import numpy as np
from tensorflow import keras
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, Flatten, Dense, Reshape
from keras.models import Model
from keras.datasets import mnist
import matplotlib.pyplot as plt

# Load MNIST dataset
(x_train, _), (x_test, _) = mnist.load_data()
x_train = np.expand_dims(x_train, axis=-1)  # Add channel dimension
x_test = np.expand_dims(x_test, axis=-1)  # Add channel dimension
x_train = x_train.astype("float32") / 255.0  # Normalize pixel values
x_test = x_test.astype("float32") / 255.0  # Normalize pixel values
input_shape = (28, 28, 1)  # Input shape for MNIST images (height, width, channels)
latent_dim = 64  # Dimensionality of the latent space

# Encoder
input_img = Input(shape=input_shape)  # Input layer
x = Conv2D(32, (3, 3), activation="relu", padding="same")(input_img)  # Convolutional layer # 28x28x32
x = MaxPooling2D((2, 2), padding="same")(x)  # Max pooling layer # 14x14x32
x = Conv2D(64, (3, 3), activation="relu", padding="same")(x)  # Convolutional layer # 14x14x64
x = MaxPooling2D((2, 2), padding="same")(x)  # Max pooling layer # 7x7x64
x = Conv2D(128, (3, 3), activation="relu", padding="same")(x)  # Convolutional layer # 7x7x128
x = MaxPooling2D((2, 2), padding="same")(x)  # Max pooling layer # 4x4x128
x = Flatten()(x)  # Flatten the feature maps # 4*4*128=2048
encoded = Dense(latent_dim, activation="relu")(x)  # Latent space representation # 64

# Decoder
x = Dense(np.prod((4, 4, 128)), activation="relu")(encoded)  # Fully connected layer
x = Reshape((4, 4, 128))(x)  # Reshape to feature maps # 4x4x128
x = UpSampling2D((2, 2))(x)  # Upsampling layer # 8x8x128 
x = Conv2DTranspose(64, (3, 3), activation="relu", padding="same")(x)  # Transposed convolutional layer # 8x8x64
x = UpSampling2D((2, 2))(x)  # Upsampling layer # 16x16x64
x = Conv2DTranspose(32, (3, 3), activation="relu", padding="same")(x)  # Transposed convolutional layer # 16x16x32
x = UpSampling2D((2, 2))(x)  # Upsampling layer # 32x32x32
x = Conv2DTranspose(1, (3, 3), activation="sigmoid", padding="same")(x)  # Output layer # 32x32x1
decoded = keras.layers.Cropping2D(cropping=((2, 2), (2, 2)))(x)  # Crop to original size # 28x28x1

# Autoencoder model
autoencoder = Model(input_img, decoded)  # Autoencoder model
autoencoder.compile(optimizer="adam", loss="binary_crossentropy")  # Compile the model
# Train the autoencoder
autoencoder.fit(
    x_train, x_train, epochs=10, batch_size=256, shuffle=True, validation_data=(x_test, x_test)
)  # Train the model
# Evaluate the autoencoder
decoded_imgs = autoencoder.predict(x_test)  # Reconstruct the test images
# Display original and reconstructed images
import matplotlib.pyplot as plt

n = 10  # Number of images to display
plt.figure(figsize=(20, 4))
for i in range(n):
    # Display original images
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28), cmap="gray")
    plt.title("Original")
    plt.axis("off")

    # Display reconstructed images
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap="gray")
    plt.title("Reconstructed")
    plt.axis("off")
plt.show()
```
Code explained:

- Unpacks into `x_train` (60,000 images), `x_test` (10,000 images), and ignores labels (`_`) since the autoencoder is unsupervised.

- `np.expand_dims(array, axis)` adds a dimension at `axis=-1` (last axis). Conv2D layers expect a channel dimension (e.g., 1 for grayscale, 3 for RGB). MNIST images are grayscale, so we add a single channel.

- `x_train.astype("float32") / 255.0` normalizes pixel values to the range [0, 1] for better training stability. Because neural networks work better with normalize inputs and `binary_crossentropy` loss function expects values in [0, 1]. Float32 is standard for GPU computations.

**Encoder**:

- `x = Conv2D(32, (3, 3), activation="relu", padding="same")(input_img)`
  - `Conv2D(filters, kernel_size, activation, padding)` creates a 2D convolutional layer.
  - `filters=32` specifies the number of filters (feature maps) to learn.
  - `kernel_size=(3, 3)` specifies the size of the convolutional kernel (3x3 pixels).
  - `activation="relu"` applies the ReLU activation function to introduce non-linearity.
  - `padding="same"` keeps the output size the same as the input size by adding padding around the input (28 x 28)
  - `input_img` is the input tensor.

- `x = MaxPooling2D((2, 2), padding="same")(x)`
  - Downsamples the feature maps, reducing computational load and capturing dominant features. 
  - `MaxPooling2D(pool_size, padding)` creates a max pooling layer, reducing spacial dimensions by taking the maximum value in each 2x2 region.
  - `pool_size=(2, 2)` specifies the size of the pooling window (2x2 pixels). 
  - `padding="same"` ensures that the output size is the same as the input size by adding padding around the input.
  - `x` is the input tensor from the previous layer.

- `x = Conv2D(64, (3, 3), activation="relu", padding="same")(x)`
  - Applies another convolutional layer to extract more complex features from the downsampled input.
  - `filters=64` specifies the number of filters (feature maps) to learn.

- `x = Flatten()(x)` flattens the feature maps into a 1D vector, preparing it for the fully connected layer. The output shape is (4 * 4 * 128) = 2048.
  
- `encoded = Dense(latent_dim, activation="relu")(x)` creates a fully connected layer with `latent_dim` (64) neurons. This layer learns a compressed representation of the input data in the latent space.

**Decoder**:

- `x = Dense(np.prod((4, 4, 128)), activation="relu")(encoded)`
  - creates a fully connected layer that expands the latent representation back to the flattened shape of the feature maps (4 * 4 * 128 = 2048). This layer learns to reconstruct the feature maps from the latent representation.
  - Maps the latent space back to the size of the encoder’s last feature map.
  
- `x = Reshape((4, 4, 128))(x)`  
  - Reshapes the output of the dense layer back into the shape of the feature maps (4x4x128) for further processing in the decoder.
  - Reshapes the flattened vector back into a 4D tensor for the next layers.

- `x = UpSampling2D((2, 2))(x)` upscales the feature maps by a factor of 2, increasing the spatial dimensions from (4x4) to (8x8).
  
- `x = Conv2DTranspose(64, (3, 3), activation="relu", padding="same")(x)`
  -  `Conv2DTranspose(filters, kernel_size, activation, padding)` performs transposed convolution (deconvolution) to upsample and apply filters.
  -  `filters=64` specifies the number of filters (feature maps) to learn.
  -  `kernel_size=(3, 3)` specifies the size of the transposed convolutional kernel (3x3 pixels).
  -  `activation="relu"` applies the ReLU activation function to introduce non-linearity.
  -  `padding="same"` keeps the output size the same as the input size by adding padding around the input (8x8).
  -  shape: (None, 8, 8, 64), where None is the batch size.
  
- `x = UpSampling2D((2, 2))(x)` upscales the feature maps by a factor of 2, increasing the spatial dimensions from (8x8) to (16x16).
  
- `x = Conv2DTranspose(32, (3, 3), activation="relu", padding="same")(x)`
  - Applies another transposed convolutional layer to upsample and apply filters.
  - `filters=32` specifies the number of filters (feature maps) to learn.
  - shape: (None, 16, 16, 32)


- `x = UpSampling2D((2, 2))(x)` upscales the feature maps by a factor of 2, increasing the spatial dimensions from (16x16) to (32x32).
  
- `x = Conv2DTranspose(1, (3, 3), activation="sigmoid", padding="same")(x)` 
  - Output layer that reconstructs the final image.
  - `filters=1` specifies the number of filters (feature maps) to learn (1 for grayscale images).
  - `kernel_size=(3, 3)` specifies the size of the transposed convolutional kernel (3x3 pixels).
  - `activation="sigmoid"` applies the sigmoid activation function to produce pixel values in the range [0, 1].
  - `padding="same"` keeps the output size the same as the input size by adding padding around the input (32x32).
  - shape: (None, 32, 32, 1)
  
- `autoencoder.fit(x_train, x_train, epochs=10, batch_size=256, shuffle=True, validation_data=(x_test, x_test))`
  -  `x_train, x_train`: Input and target are the same (autoencoder reconstructs inputs).
  - Trains the autoencoder on the training data (x_train) to reconstruct the input data. The model is trained for 10 epochs with a batch size of 256, shuffling the data each epoch.
  - `validation_data=(x_test, x_test)` uses the test data for validation during training.
  - **Note**: Ensure to monitor the loss during training to avoid overfitting.

- `decoded_imgs = autoencoder.predict(x_test)` reconstructs the test images using the trained autoencoder. The output is a set of reconstructed images.

### CAE vs CNN
- **When to not use an autoencoder:**
  - When the data is not suitable for unsupervised learning, such as when labeled data is available for supervised learning tasks.
  - When the model complexity is not justified by the amount of data available, as autoencoders can be prone to overfitting if the dataset is small.
  - When the task requires a specific architecture or model type that is not well-suited for autoencoders, such as certain types of time series analysis or structured data.

In short:

- Use **CAE + classifier** if your dataset is small, noisy, or partially labeled.

- Use **just CNN** if you have lots of clean labeled data and want simplicity.

### Recurrent Autoencoders

Recurrent autoencoders are a type of autoencoder designed to work with **sequential data**, such as time series or natural language. They combine the principles of autoencoders and recurrent neural networks (RNNs) to learn compressed representations of sequences.

**Architecture**:

- **Encoder**: The encoder consists of an RNN (e.g., LSTM or GRU) that processes the input sequence and compresses it into a fixed-size latent representation. The encoder takes the input sequence and produces a hidden state that captures the information from the entire sequence.
  
- **Decoder**: The decoder is also an RNN that takes the latent representation and reconstructs the original sequence. It generates the output sequence step by step, using the latent representation as context.

### Denoising Autoencoders

Denoising autoencoders are a type of autoencoder designed to learn robust representations of data by reconstructing the original input from a corrupted version of it. 

They are particularly **useful for tasks like image denoising**, where the goal is to remove noise from images while preserving important features.

**Architecture**: The architecture is similar to a standard autoencoder, but the input to the encoder is a corrupted version of the original data. The autoencoder learns to reconstruct the original data from this corrupted input.

**Loss Function**: The loss function is typically the mean squared error (MSE) between the original input and the reconstructed output. The model learns to minimize this loss by learning to ignore the noise and focus on the underlying structure of the data.

## Comparison of Autoencoders

| Type             | Use Case                  |  Architecture | Advantages | Disadvantages |
|------------------|---------------------------|----------------|------------|---------------|
| Autoencoder      | General Purpose | Encoder + Decoder (symmetric) | Simple, effective for unsupervised learning | Limited to linear relationships |
| Undercomplete    | Dimensionality Reduction/Feature Extraction | Encoder + Decoder (symmetric) | Forces compression, captures essential features | May lose information |
| Stacked          | Hierarchical Feature Learning/Complex Data | Multi-layered Encoder + Decoder | Learns multiple levels of representation | More complex, requires more data |
| Convolutional    | Image Data |  Convolutional layers | Captures spatial hierarchies, effective for images | Limited to grid-like data |
| Recurrent        | Sequential Data | RNN-based/LSTM Encoder + Decoder | Captures temporal dependencies | Limited to sequential data |
| Denoising        | Noise Reduction | Encoder + Decoder (corrupted input) | Robust to noise, learns useful features | Requires careful corruption process |

## Hyperparameter Optimization (HPO)

**What is a Hyperparameter?**

A hyperparameter is a parameter whose value is set before the learning process begins. Hyperparameters are not learned from the data but are instead specified by the practitioner to control the learning process.

They are settings that govern the training of the model and can significantly impact its performance. Examples of hyperparameters include:
- **Learning rate**: The step size used in the optimization algorithm to update the model's parameters.

- **Batch size**: The number of training samples used in one iteration of the optimization algorithm.

- **Number of epochs**: The number of times the entire training dataset is passed through the model during training.

- **Number of layers and units**: The architecture of the neural network, including the number of hidden layers and the number of units in each layer.

- **Dropout rate**: The fraction of neurons to drop during training to prevent overfitting.

- **Regularization parameters**: Parameters that control the strength of regularization techniques, such as L1 or L2 regularization.

- **Activation functions**: The choice of activation functions used in the model (e.g., ReLU, sigmoid, tanh).

**Why is HPO Important?**

Hyperparameter optimization (HPO) is important because it can significantly impact the performance of machine learning models. 

Properly tuned hyperparameters can lead to better model accuracy, faster convergence, and improved generalization to unseen data. HPO helps in finding the best combination of hyperparameters that yield the best performance on a given task.

Poorly chosen hyperparameters can lead to underfitting or overfitting. 

Manual tuning becomes impractical for complex models with many hyperparameters.

**Validation Set**: A separate portion of data is used to evaluate hyperparameter performance, preventing overfitting to training data.

### Challenges in HPO

**Computational Challenges**

- **Expensive to evaluate**:
  - Training machine learning models can be computationally expensive, especially for deep learning models.
  - Each hyperparameter configuration requires training the model from scratch, which can be time-consuming and resource-intensive.

- **High-dimensional search space**:
  - The search space for hyperparameters can be high-dimensional, especially when dealing with complex models with many hyperparameters.
  - This makes it challenging to explore all possible combinations of hyperparameters effectively.

- **No Direct Gradient**:
  - Hyperparameters are not directly optimized using gradient descent, making it difficult to find optimal values.
  - The optimization process may require more sophisticated techniques than standard gradient-based methods.

- **Overfitting**:
  - Hyperparameter tuning can lead to overfitting if the validation set is not representative of the test set.
  - Care must be taken to ensure that the model generalizes well to unseen data. 

### Table of Hyperprameters (HPs)

| ML Algorithm | Main HPs | Optional HPs |
|--------------|----------|--------------|
| Linear Regression | Regularization (L1, L2) | Learning rate, Batch size |
| Logistic Regression | penalty (L1, L2), c, solver | Learning rate, Batch size |
| KNN | n_neighbors |  |
| SVM | C, kernel, epsilon (for SVR) | gamma, Degree (for polynomial kernel), coef0 |
| XG Boost | n_estimators, learning_rate, max_depth | min_child_weight, gamma, alpha, lambda |
| Bagging | n_estimators, base_estimator | bootstrap, bootstrap_features |
| Deep Learning | Learning rate, Batch size, Number of epochs, no. of hidden layers, optimizer, Activation functions, early stop patience | number of frozen layers (if transfer learning used) |
| CNN | Learning rate, Batch size, Number of epochs, no. of filters, kernel size, pool size | Dropout rate, Regularization (L1, L2), optimizer |
| K-means | n_clusters, | init, n_init, max_iter |
| DBSCAN | eps, min_samples | metric, algorithm |
| Random Forest | n_estimators, max_depth, min_samples_split | min_samples_leaf, max_features |
| LSTM | Number of units, Learning rate, Batch size | Dropout rate, Number of layers |
| PCA | n_components | whiten, svd_solver |

### Parameter Initialization

Parameter initialization is crucial to NN performance

- **Why is it important?**
  - Proper initialization can help the model converge faster and avoid local minima.
  - Poor initialization can lead to slow convergence, vanishing/exploding gradients, and suboptimal solutions.

**Importance of weight initialization**:  

Weight initialization is important because it determines the starting point of the optimization process. Properly initialized weights can help the model learn effectively and converge to a good solution.

**Common weight initialization methods**:

- **Random Initialization**: Weights are initialized randomly, typically from a uniform or normal distribution. This breaks symmetry and allows different neurons to learn different features. 
  - `tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05)` is used for random initialization.
  
- **Xavier/Glorot Initialization**: Weights are initialized from a uniform distribution with a range based on the number of input and output units. This helps maintain the variance of activations across layers. `tf.keras.initializers.GlorotUniform()` is used for Xavier initialization.
  
- **He Initialization**: Similar to Xavier initialization but designed for ReLU activation functions. It scales the weights based on the number of input units, helping to prevent vanishing/exploding gradients. `tf.keras.initializers.HeNormal()` is used for He initialization.


### Learning Rate Strategies

The learning rate determines the step size during optimization:
   - Too high: Model may diverge
   
   - Too low: Extremely slow convergence

A common approach is to run SGD with a fixed step-size for a few epochs and measure error and plot progress, and then adjust the learning rate based on the observed performance.

**Learning Rate Scheduling Techniques**

Learning rate scheduling involves strategies to reduce the learning rate during training to improve convergence and performance. A good learning rate is crucial for effective training:

**Too high**: Model may diverge (Gradient explosion)
**Too low**: Extremely slow convergence (Gradient vanishing)

#### Finding a Good Learning Rate

To find an optimal learning rate:

1. Train the model for a few hundred iterations, exponentially increasing the learning rate from a very small value to a very large value.
   
2. Plot and examine the loss function against the learning rate. 
   - Look for the learning rate where the loss starts to decrease rapidly. This is a good starting point for training.
  
   - Select a learning rate slightly lower than the point where the learning curve starts shooting back up
  
3. Reinitialize the model and train it with the selected learning rate.

### Common Scheduling Techniques

Strategies to reduce learning rate during training called learning schedules.

**Power Scheduling**:

- Reduces the learning rate by a factor of 0.1 every few epochs. This is useful for models that converge quickly and need fine-tuning in later epochs.
  
**Exponential Scheduling**:

- Reduces the learning rate exponentially based on the epoch number. This is useful for models that require a gradual decrease in learning rate over time.

**Piecwise Constant Scheduling**:

- Reduces the learning rate at specific epochs. This is useful for models that require abrupt changes in learning rate at certain points during training.

**Performance-based Scheduling (Most Common)**:

- Reduces the learning rate based on the performance of the model on the validation set. This is useful for models that require dynamic adjustments to the learning rate based on performance.
  - Measure validation error after each epoch and reduce the learning rate if the error does not improve for a certain number of epochs.
  - Reduce the learning rate by a factor of x if the validation error does not improve for 5 epochs.

- `tf.keras.callbacks.ReduceLROnPlateau` is used for performance-based scheduling.

```python
from tensorflow.keras.callbacks import ReduceLROnPlateau

lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    verbose=1
)

model.fit(x_train, y_train, epochs=50, callbacks=[lr_scheduler], validation_data=(x_val, y_val))
```

## HPO Search Strategies

**Trial and Error (Grad Student Descent)**:

- 100% Manually tuning hyperparameters based on intuition and experience.
- Not systematic and can be time-consuming.
- Not recommended for complex models with many hyperparameters.
  
**Grid Search**:

- Most commonly used method
- Exhaustively searches through a predefined set of hyperparameter values.
- Evaluates the cartesian product of all hyperparameter combinations.
- Suitable for small search spaces.
- It is simple to implement but can be computationally expensive, especially for large search spaces.

Grid search treats all values equally, so it may not be efficient for large search spaces. It can also lead to overfitting if the validation set is not representative of the test set.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
model = RandomForestClassifier()

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Perform grid search
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Evaluate the best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy:", accuracy)
```
- In this example, we use the Iris dataset and a Random Forest classifier. We define a hyperparameter grid for `n_estimators`, `max_depth`, and `min_samples_split`. The `GridSearchCV` function performs grid search with 5-fold cross-validation and evaluates the model using accuracy as the scoring metric. Finally, we print the best hyperparameters and test accuracy.

**Random Search**:

- Randomly samples hyperparameter values from a predefined distribution.
- More efficient than grid search for large search spaces.
- Can find good hyperparameters faster than grid search.
- Less exhaustive than grid search, so it may miss the optimal combination.
- `sklearn.model_selection.RandomizedSearchCV` is used for random search.
- **Problem:** May perform unnecessary evaluations as it does not exploit previously well-performing regions.
  
```python
# Define the hyperparameter distribution
param_dist = {
    'n_estimators': randint(50, 200),
    'max_depth': [None, 10, 20],
    'min_samples_split': randint(2, 10)
}
# Perform random search
random_search = RandomizedSearchCV(
    model,
    param_distributions=param_dist,
    n_iter=10,  # Number of random combinations to try
    cv=5,
    scoring='accuracy',
    random_state=42
)
random_search.fit(X_train, y_train)
# Get the best hyperparameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)
# Evaluate the best model
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy:", accuracy)
```

**Gradient-Based Optimization**:

- Traditional technique that moves in the opposite direction of the largest gradient to locate the next point.
- It is often used in conjunction with deep learning models to optimize loss functions.
- This method can converge quickly
- **Problem:** May get stuck in local optima and is not suitable for discrete or categorical hyperparameters.
- `sklearn.model_selection.BayesSearchCV` is used for gradient-based optimization.

## HPO Frameworks

**Sklearn**:

- `sklearn.model_selection.GridSearchCV` and `sklearn.model_selection.RandomizedSearchCV` are used for grid search and random search, respectively.
  
- `sklearn.model_selection.BayesSearchCV` is used for gradient-based optimization.

**TensorFlow**: 

- `tf.keras.callbacks.LearningRateScheduler` is used for learning rate scheduling.
- `tf.keras.callbacks.ReduceLROnPlateau` is used for performance-based learning rate scheduling.
- `Trieste` is a library for Bayesian optimization in TensorFlow.
- `Keras Tuner` is a library for hyperparameter tuning in Keras models. It provides various search algorithms, including random search, hyperband, and Bayesian optimization.

# Lecture 13 

## CNNs

Convolutional Neural Networks (CNNs) are a type of deep learning model specifically designed for processing grid-like data, such as images.

They are widely used in computer vision tasks, including image classification, object detection, and image segmentation.

Traditional machine learning techniques and simple neural networks often struggle when it comes to working with large, high-dimensional images.

 - This is because the input data (e.g., pixels in an image) is vast, and feeding this directly into a fully connected neural network would require an enormous number of parameters, making the process computationally expensive and inefficient.
   - A 32x32 pixel image with 3 color channels (RGB) would have 32 * 32 * 3 = 3072 input features.
   - A fully connected layer with 1000 neurons would require 3072 * 1000 = 3,072,000 parameters just for the first layer.
   - This is not feasible for larger images or deeper networks.

CNNs overcome these challenges by **using partial connections and shared weights**, which significantly reduce the number of parameters and improve computational efficiency.

**CNN VS DNN**

| Feature | CNN | DNN |
|---------|-----|-----|
| Architecture | Composed of convolutional layers, pooling layers, and fully connected layers | Composed of fully connected layers |
| Input Type | Primarily designed for grid-like data (e.g., images) | Can handle various types of data |
| Parameter Sharing | Uses shared weights in convolutional layers | No parameter sharing |
| Local Connectivity | Convolutional layers connect to local regions of the input | Fully connected layers connect to all input features |
| Spatial Hierarchy | Captures spatial hierarchies and local patterns | Does not capture spatial hierarchies |


- CNNs Layers are designed to process grid-like data. They include convolutional layers
that apply filters (kernels) to the input data to detect features like edges, textures, and patterns.

  -**A Filter** is a small matrix (e.g., 3x3 or 5x5) that slides over the input data, performing element-wise multiplication and summation to produce a feature map.

CNNs learn increasingly complex patterns layer by layer:

  - Early layers detect simple patterns (e.g., edges, corners).

  - Middle layers combine these to detect shapes or textures (e.g., eyes, wheels).

  - Deeper layers recognize high-level concepts (e.g., faces, cars, tumors).


**DNNs** are composed of fully connected layers that connect every neuron in one layer to every neuron in the next layer. They do not have the same level of spatial awareness as CNNs and are **less efficient for processing images.**

**CNN Key Concepts**

**What is convolution?**

- Convolution is a mathematical operation that combines two functions to produce a third function. In the context of CNNs, it involves applying a filter (kernel) to an input image to extract features.

- The convolution operation involves sliding the filter (a small matrix (e.g., 3x3 or 5x5)) over the input image and performing element-wise multiplication and summation to produce a feature map (a new representation of the input image that highlights the detected features).

- **Grayscale** consists of a single channel with 0 representing the black areas and 255 the
white regions with the values in between for various shades of Gray

#### Weight Sharing and Partially Connected Layers

**Problem:** In a standard fully connected layer, each neuron is connected to pixel in the input image, leading to a large number of parameters which is computationally expensive and inefficient.

**Solution:** CNNs use weight sharing and partially connected layers to reduce the number of parameters and improve computational efficiency.

**Weight Sharing**: 

- The same filter (kernel) (which is the same set of weights) is applied to different regions of the input image, allowing the model to learn spatial hierarchies and local patterns. This reduces the number of parameters significantly.

- For example, if a filter has a size of (3, 3) and the input image has a size of 32 x 32 pixels with 3 input channels and 32 output channels (where each output channel corresponds to a different feature map), the number of weights for the filter is (3 x 3 x 3 x 32) = 864 weights. If the input image has 32 x 32 pixels and 3 input channels, the number of weights in a fully connected layer would be (32 x 32 x 3) x (32 x 32 x 3) = 3072 x 3072 = 9,437,184 weights.

- This approach allows the network to recognize patterns (e.g., edges) regardless of their location in the image (known as **translation invariance**). This means that if an object appears in different locations in the image, the CNN can still recognize it.


**Partially Connected Layers**: 

Each output pixel in a feature map is computed from a small receptive field (e.g., 3x3 patch) of the input, not the entire input.

This limits connections to local regions, reducing computation and capturing local patterns (e.g., edges) rather than global ones.

**Example:** For a 32x32 input, each output pixel connects to only 9 input pixels (3x3), not all 1024.

## Components of CNNs

### Input Layer

- The input layer of a CNN is the first layer that receives the input data (e.g., an image). It is responsible for passing the input data to the subsequent layers of the network.

- For images the input is a tensor of shape `(batch_size, height, width, channels)`, where `batch_size` is the number of images in a batch, `height` and `width` are the dimensions of the image, and `channels` is the number of color channels (e.g., 1 for grayscale, 3 for RGB).

- In python the input layer is defined using `tf.keras.layers.Input(shape=(height, width, channels))`, where `shape` specifies the dimensions of the input data.

### Convolutional Layer
- The core building block of CNNs, responsible for feature extraction.
  
- Applies a set of learnable filters (kernels) to the input data, producing feature maps that highlight important patterns.

- The filter (a small matrix) is passed over the input image, performing element-wise multiplication and summation to produce a single value in the output feature map, in a process called convolution.

- Colored images have 3 channels (RGB), so the filter must also have 3 channels. The filter is applied to each channel separately, and the results are summed to produce a single value in the output feature map. For example, a `(3, 3, 3)` filter for a colored image and for a greyscale image a `(3, 3, 1)` filter, where **(height, width, channels) are the dimensions of the filter.**

- **Feature Maps**: The output of the convolutional layer is a set of feature maps, each corresponding to a different filter. Each feature map highlights different patterns in the input data (such as edges, textures, or shapes).
  - X number of filters = X number of feature maps

- **Stride**: The number of pixels the filter moves over the input image during convolution. A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means it moves two pixels at a time. Increasing the stride reduces the size of the output feature map.

- **Padding**: The process of adding extra pixels around the input image to control the size of the output feature map. There are two types of padding:
  
  - **Valid Padding**: No padding is added, and the output feature map is smaller than the input image.
  
  - **Same Padding**: Padding is added to ensure that the output feature map has the same dimensions as the input image.

- Conv2D layer is defined using `tf.keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', activation=None)`, where:
  - `filters` is the number of filters (feature maps) to learn.
  - `kernel_size` is the size of the filter (e.g., `(3, 3)`).
  - `strides` is the stride of the convolution (default is `(1, 1)`).
  - `padding` specifies the type of padding (`'valid'` or `'same'`).
  - `activation` specifies the activation function to apply (e.g., `relu'`). This introduces non-linearity to the model, allowing it to learn complex patterns.
    -  **ReLU (Rectified Linear Unit):** Commonly used in CNNs to introduce non-linearity, helping the model learn more complex patterns.
    -  **Softmax:** Used in the output layer for multi-class classification tasks, producing probabilities for each class.

### Pooling Layer

Used to reduce the spatial dimensions (height and width) of the image while retaining important features.

Downsamples feature maps by summarizing regions (e.g., taking the maximum value in a 3x3 window).

This is especially useful in CNNs because it reduces computational complexity and helps prevent overfitting.

**Types**: Max pooling (selects max), average pooling (computes average).

#### Max Pooling (Most Common)

In max pooling, a filter (usually 2x2) slides over the image, and for each region, it selects the maximum value. This reduces the spatial dimensions of the image but retains the most important features.

In python, max pooling is defined using:

`tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')`, where:

- `pool_size` is the size of the pooling window (e.g., `(2, 2)`).
- `strides` is the stride of the pooling operation (default is `(2, 2)`).
- `padding` specifies the type of padding (`'valid'` or `'same'`).

eg. it reduces spacial dimensions by a factor of 2, so a 32x32 image becomes a 16x16 image.

### Fully Connected Layer (Dense Layer)

The Fully Connected Layer (Dense Layer) is where the network makes final decisions. After several convolutions and pooling operations, the network has learned abstract features of the image. These features are passed to a fully connected layer for classification.

After feature extraction through convolution and pooling, the flattened feature map is passed to a fully connected layer. This layer uses learned weights and biases to compute a final output, such as a classification label.

- `tf.keras.layers.Dense(units, activation=None)` is used to create a fully connected layer, where:
  - `units` is the number of neurons in the layer.
  - `activation` specifies the activation function to apply

- **Output:** (batch size, units), e.g., (None, 128) for latent_dim=128.

- **Flattening**: The output of the last pooling layer is flattened into a 1D vector before being passed to the fully connected layer. This is done using `tf.keras.layers.Flatten()`, which reshapes the input tensor into a 1D vector.
  
  - For example, if the output of the last pooling layer is (None, 4, 4, 128), flattening it will result in a shape of (None, 2048) as 4x4x128 = 2048.

## CNN Optimization and Techniques

**Dropout**: 

CNNs are prone to overfitting, meaning they perform well on training data but poorly on unseen data. Dropout is a technique where some neurons are randomly ignored during training, forcing the network to learn more generalized patterns.

A regularization technique that randomly sets a fraction of the input units to 0 during training to prevent overfitting. This forces the network to learn more robust features.

- `tf.keras.layers.Dropout(rate)` is used to create a dropout layer, where:
  - `rate` is the fraction of input units to drop (e.g., `0.5` for 50% dropout).
- **Example**: If the input to the dropout layer is (None, 128), and the dropout rate is 0.5, the output will be (None, 64) as half of the input units are dropped.

**Batch Normalization**: A technique to normalize the inputs of each layer to improve training speed and stability. It helps in reducing internal covariate shift and allows for higher learning rates.
- `tf.keras.layers.BatchNormalization()` is used to create a batch normalization layer.
- It normalizes the input to have a mean of 0 and a standard deviation of 1, which helps in stabilizing the training process.

Python example of a simple CNN model using Keras:

```python
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import numpy as np

# Load and preprocess the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = np.expand_dims(x_train, axis=-1).astype('float32') / 255.0
x_test = np.expand_dims(x_test, axis=-1).astype('float32') / 255.0
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

# Define the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),
    
    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),
    
    Flatten(),
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    
    Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))

# Evaluate the model
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')
```

**Data Augmentation in CNNs**

Data augmentation artificially increases the size of a dataset by applying transformations such as rotation, flipping, scaling, and shifting to existing images.

**Benefits**:

- Helps prevent overfitting by introducing variability in the training data.
  
- Improves the model's ability to generalize to unseen data, making the CNNs robost to different orientations and distortions of the input data.
  
- Mimics real-world scenarios where images may be captured from different angles or under different conditions.

Example: Applying Data Augmentation using Keras
```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Create an instance of ImageDataGenerator for data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,  # Randomly rotate images by up to 20 degrees
    width_shift_range=0.2,  # Randomly shift images horizontally by up to 20% of the width
    height_shift_range=0.2,  # Randomly shift images vertically by up to 20% of the height
    shear_range=0.2,  # Randomly apply shear transformations
    zoom_range=0.2,  # Randomly zoom in on images
    horizontal_flip=True,  # Randomly flip images horizontally
    fill_mode='nearest'  # Fill in missing pixels after transformations
)

# Fit the data generator to the training data
datagen.fit(x_train)

# Train the model using the data generator
model.fit(datagen.flow(x_train, y_train, batch_size=128), epochs=10, validation_data=(x_test, y_test))
```

## Training CNNs

Training a CNN involves two key steps: forward propagation (where the network makes predictions) and backpropagation (where it learns from its mistakes).

### Forward Propagation
- Each layer applies transformations to the input (convolution, activation, pooling).
- Features are extracted progressively.
- Fully connected layers use these features to make predictions
- The output is compared to the true labels using a loss function (e.g., categorical crossentropy for multi-class classification or binary crossentropy for binary classification).
- The loss function quantifies the difference between the predicted output and the true labels.
- The loss is used to update the model's weights during backpropagation.

### Backpropagation
- Gradients are computed using the chain rule
- The gradients are used to update the model's weights using an optimization algorithm (e.g., stochastic gradient descent (SGD), Adam).
- The optimizer adjusts the weights to minimize the loss function.
- The process repeats until the model learns to make accurate predictions.
- The model is trained for a specified number of epochs, where each epoch consists of multiple iterations over the training data (batches).
- The training process involves feeding batches of data through the network, computing the loss, and updating the weights using backpropagation.

## Real World Applications of CNNs

- **Image Classification**: Classifying images into predefined categories (e.g., identifying objects in images).
- **Object Detection**: Locating and classifying multiple objects within an image (e.g., detecting faces in photos).
- **Image Segmentation**: Dividing an image into meaningful segments (e.g., identifying different regions in a medical image).
- **Facial Recognition**: Identifying and verifying individuals based on their facial features.
- **Medical Image Analysis**: Analyzing medical images (e.g., X-rays, MRIs) for disease detection and diagnosis.
- **Self-driving Cars**: Detecting and recognizing objects (e.g., pedestrians, traffic signs) in real-time for autonomous navigation.

Several famous CNN architectures have significantly influenced deep learning:

| Architecture | Year | Description |
|--------------|------|-------------|
| LeNet-5 | 1998 | One of the first CNNs, designed for handwritten digit recognition (MNIST dataset). |
| AlexNet | 2012 | Won the ImageNet competition, introduced ReLU activation and dropout. |
| VGGNet | 2014 | Deep architecture with small (3x3) filters, known for its simplicity and depth. |
| ResNet | 2015 | Introduced residual connections, allowing for very deep networks (up to 152 layers). |
| YOLO | 2016 | Real-time object detection system that predicts bounding boxes and class probabilities directly from full images. |

## Adversarial Examples & Generative Models

**Adversarial examples** are inputs to machine learning models that have been intentionally perturbed (slighty changed`) to cause the model to make incorrect predictions. These perturbations are often imperceptible (hard to notice) to humans but can significantly affect the model's performance.

**Generative models** are a class of machine learning models (such as Generative Adversarial Networks - GANs) that learn to generate new data samples from a given distribution. They can be used to create realistic images, text, or other types of data. Generative models can also be used to create adversarial examples by generating inputs that are specifically designed to fool a model.

These attacks occur across domains—computer vision (e.g., misclassifying images), natural language processing (e.g., altering text sentiment), and cybersecurity (e.g., evading malware detection).

Two types of attacks in DL: White-Box and Black-Box attacks

- **White-Box Attacks**: The attacker has full knowledge of the model architecture, parameters, and training data. This allows them to craft adversarial examples that exploit specific weaknesses in the model.

- **Black-Box Attacks**: The attacker has no knowledge of the model architecture or parameters. They can only query the model to obtain predictions. This makes it more challenging to create adversarial examples, but it is still possible using techniques like transferability (where adversarial examples generated for one model can also fool another model).


### Generating Adversarial Examples

Several methods exist to craft adversarial examples, including:

- **Fast Gradient Sign Method (FGSM):** Uses the sign of the gradient of the loss with respect to the input to create perturbations.
  
- **Projected Gradient Descent (PGD):** An iterative version of FGSM, applying multiple small perturbations.
  
- **Carlini & Wagner (C&W) Attack:** Optimizes perturbations to be minimal yet effective.

#### Defending Against Adversarial Attacks

**Adversarial Training:** Incorporates adversarial examples into the training set to teach the model to recognize them.

**Defensive Distillation:** Uses a distilled model to smooth predictions, making attacks harder.

**Input Preprocessing:** Applies techniques like JPEG compression or feature squeezing to reduce perturbation effects.

## Generative Adversarial Networks (GANs)

Generative models learn to produce data resembling a training distribution, such as images or text, from random noise. Unlike discriminative models, which classify inputs, generative models create new samples.

GANs consist of two competing neural networks:

**Generator:** Maps random noise to synthetic data (e.g., images).

**Discriminator:** Evaluates whether a sample is real (from the training set) or fake (from the generator).

The two networks are trained adversially: the generator improves by trying to “fool” the discriminator, while the discriminator improves by better distinguishing real from fake.

### Training a GAN

Training involves two phases per iteration:

**Discriminator Training:** Uses real images (labeled 1) and fake images (labeled 0) to improve classification.

**Generator Training:** Generates fake images, with the discriminator providing feedback (all labeled as 1).

Challenges include:

- Mode Collapse: The generator produces limited variety.
  
- Vanishing Gradients: Imbalance between generator and discriminator learning rates.

#### Applications of GANs
- Image Generation: Creating realistic images from random noise.
- Deepfake Generation: Creating realistic fake videos or images.
- Data Augmentation: Generating synthetic data for training ML models.
- Style Transfer and Super-Resolution: Enhancing or stylizing images.

# Lecture 14

## Federated Learning (FL)

Federated Learning is a distributed machine learning approach that allows multiple devices or servers to **collaboratively train a model without sharing their raw data.**

Instead of sending data to a central server, each device trains the model locally and only shares the model updates (gradients) with the server. 

This approach enhances privacy, reduces data transfer costs, and enables training on decentralized data sources.

**How Does FL Work?**

1. **Model Distribution:** A central server shares a pre-trained model with remote devices.
2. **Local Training:** Each device trains the model on its private data.
3. **Update Sharing:** Devices summarize and encrypt model updates (e.g., gradients) and send them to the server.
4. **Aggregation:** The server decrypts, averages updates, and integrates them into the global model.
5. **Iteration:** Steps repeat until the model is fully trained.

**Privacy Guarantee:** Devices exchange model gradients, not raw data.

#### Training Process

**Horizontal Learning:** Trains on similar datasets across devices.

**Vertical Learning:** Combines complementary data (e.g., movie and book reviews to predict music preferences).

**Transfer Learning:** Adapts a pre-trained model to a new task (e.g., car detection to cat detection).

## Transfer Learning (TL)

Transfer Learning is a machine learning technique where a model developed for one task is reused as the starting point for a model on a second task. It is particularly useful when the second task has limited labeled data.

Involves fine-tuning a model pre-trained on one task for a new, related task, offering time and cost savings.

**Why Use Transfer Learning?**

- **Limited Data**: When labeled data is scarce for the target task, transfer learning allows leveraging knowledge from a related task with abundant data.

- **Faster Training**: Pre-trained models can significantly reduce training time, as they already have learned useful features.

- **Improved Performance**: Transfer learning often leads to better performance on the target task, as the model starts with a good set of features learned from the source task.

### TL Strategies

1. **Transductive Transfer Learning:**
   
- Transfers knowledge from a specific source to a related target domain.
- **Advantage:** Works with little/no labeled target data.
- **Example:** Sentiment analysis model from product reviews adapted for movie reviews.
  
2. **Inductive Transfer Learning:**
- Same domain, different tasks.
- **Advantage:** Faster training with pre-trained familiarity.
- **Example:** NLP model pre-trained on text, fine-tuned for sentiment analysis.
  
3. **Unsupervised Transfer Learning:**
- Uses unlabeled data in source and target domains.
- **Example:** Identifying motorcycle types from unlabeled vehicle images.

#### TL Steps

1. **Select Pre-trained Model:** Choose a model with prior knowledge (e.g., ImageNet for images).
   
2. **Configure Model:**
   - Freeze Layers: Preserve source knowledge by locking pre-trained layers.
   - Adjust Weights: Start with random weights and refine during training.
   - Modify Layers: Remove the last layer and add new layers for the target task, ensuring compatibility with the new output classes.
  
3. **Train on Target Domain:** Fine-tune with target data.

Python Example: Transfer Learning with TensorFlow

```python
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras import layers, models

# Load pre-trained VGG16 model (without top layer)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze pre-trained layers
base_model.trainable = False

# Add custom layers for new task
model = models.Sequential([
    base_model,
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # Binary classification (e.g., cats vs. dogs)
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Summary
model.summary()
```

- In this example, we load the pre-trained VGG16 model without the top layer and freeze its layers. We then add custom layers for a binary classification task (e.g., cats vs. dogs). Finally, we compile the model and print its summary.
  
- The top layer is removed to adapt the model to a new task, and new layers are added for the specific classification task.
  
- The model can be trained on a new dataset using the `model.fit()` method, similar to training a standard Keras model.

#### Privacy-Accuracy Trade-off

**Issue:** Attackers may attempt to steal user data or hijack an AI model.

**Challenge in FL:** When a data host exchanges their working model with the central server, it improves the model but leaves the data used for training vulnerable to inference attacks.

**Reason for Exchange:** Each exchange enhances the model but increases the risk of privacy breaches.

**Key Concern:** The more rounds of information exchanged, the easier it becomes to infer sensitive information.

**Current Trend:** Research focuses on minimizing and neutralizing privacy threats to ensure secure federated learning.

#### Other Challenges in FL

1. **High Network Bandwidth:** Communication between devices and the central server can be resource-intensive.
2. **Transparency:** Ensuring that training data remains private while maintaining:
   - Testing accuracy.
   - Fairness in predictions.
   - Mitigation of potential biases in the model’s output.
3. **Accountability:** Logging and auditing each stage of the training pipeline to ensure traceability.
4. **Data Control:**
   - Key Questions:
     - What data are used to train the model?
     - How can data be deleted when a host leaves the federation?
   - Rule: If data are deleted, all parties are obligated to retrain the model from scratch to ensure compliance.
5. **Trust Issues:** Establishing trust among participants in the federation to prevent malicious behavior or data misuse.

## Reinforcement Learning (RL)

Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions, allowing it to learn optimal strategies over time.

**Key idea:** An agent learns to perform tasks through trial-and-error interactions with a dynamic environment.

- No static dataset required.
- Agent learns from its own experiences.
- Operates without human supervision, guided by rewards or punishments.
- Focuses on maximizing cumulative rewards over time.
- RL is used in various applications, including robotics, game playing, and autonomous systems.

#### Goal of RL

**Objective:** Maximize the total cumulative reward for the agent.

**Process:** The agent solves problems through its own actions, receiving feedback from the environment.

**Advantages:**
- No need for data collection, preprocessing, or labeling prior to training.
- Can autonomously learn behaviors with the right incentives (positive rewards or negative punishments).

### RL Workflow

- Environment: The external system with which the agent interacts.
- Reward definition: The feedback signal received by the agent after taking an action.
- Create Agent: The learner or decision-maker that interacts with the environment.
- Action: The choices made by the agent to interact with the environment.
- State: The current situation of the agent in the environment.
- Training: The process of learning from interactions with the environment to improve the agent's performance.
- Evaluation: Assessing the agent's performance based on its ability to maximize rewards.
- Deployment: Implementing the trained agent in a real-world scenario or application.

**Types of RL:**
- **Policy-Based RL:**:**
  - Directly learns a policy (a mapping from states to actions).
  - Uses a policy gradient method to optimize the policy.
  - Example: Proximal Policy Optimization (PPO).
- **Value-Based RL:**:
  - Learns a value function (a mapping from states to expected rewards).
  - Uses Q-learning or Deep Q-Networks (DQN) to estimate the value of actions.
- **Model-Based RL:**:
  - Builds a model of the environment to predict future states and rewards.
  - Uses the model to plan actions and improve learning efficiency.
  - Example: AlphaZero, which combines deep learning with Monte Carlo Tree Search (MCTS).

### RL Examples

Example 1: Car Parking

**Goal:** Teach a vehicle (agent) to park in a designated spot.
**Environment:** Includes vehicle dynamics, nearby vehicles, weather, etc.
**Training:**
- Uses sensor data (cameras, GPS, LIDAR) to generate actions (steering, braking, acceleration).
- Trial-and-error process tunes the policy.
**Reward Signal:** Evaluates trial success and guides learning.
**Reference:** MathWorks: Reinforcement Learning

**Challenges:** Sample inefficiency, interpretability, and environment constraints remain hurdles.

## Explainable Artifical Intelligense (XAI)

Explainable Artificial Intelligence (XAI) refers to methods and techniques that make the decisions and predictions of AI models more understandable and interpretable to humans.

**Goal of XAI** is to produce more explainable models while maintaining high
prediction accuracy, enabling humans to understand, trust, and manage AI
systems.

XAI is crucial for ensuring transparency, accountability, and ethical use of AI technologies, especially in high-stakes applications like healthcare, finance, and autonomous systems.

Rising ethical concerns around AI necessitate transparency.

**XAI Components:**
- Prediction accuracy.
- Traceability (narrowed scope of rules/features).
- Decision understanding (building human trust).
- Applications: Healthcare (e.g., diagnostics), Finance (e.g., fraud detection).

**Explainability vs. Interpretability**

- **Explainability**: The ability to explain the model's predictions and decisions in a human-understandable way. (focus on process)
  
- **Interpretability**: The degree to which a human can understand the cause of a decision made by a model. (focus on outcome)

**XAI Challenges**
- **Complexity:** Balancing explanation with model accuracy.
- **Oversimplification Risk:** Simplified explanations may misrepresent intricate systems.
- **Terminology:** Lack of standardized definitions for explainability and interpretability.
- **Development:** Requires integrating XAI into existing AI workflows.

Libraies like `LIME` (Local Interpretable Model-agnostic Explanations) and `SHAP` (SHapley Additive exPlanations) provide tools for generating explanations for model predictions.

# Lecture 15

## Responsible artificial intelligence (RAI)

Responsible AI (RAI) refers to the ethical and accountable development, deployment, and use of artificial intelligence systems. It encompasses principles, practices, and frameworks that ensure AI technologies are designed and used in ways that align with societal values and ethical standards.

**Aim:** To embed ethical principles into AI applications.

**6 Key Principles of RAI:**

- **Fairness:** Ensuring AI systems are unbiased and do not discriminate against individuals or groups
  
- **Transparency:** Making AI systems understandable and explainable to users and stakeholders
  
- **Accountability:** Establishing clear lines of responsibility for AI system outcomes
  
- **Privacy & Security :** Protecting individuals' data and ensuring compliance with data protection regulations. Ensuring AI systems are robust and secure against adversarial attacks
  
- **Sustainability:** Considering the environmental impact of AI systems and promoting responsible resource use.
  
- **Inclusivity:** Ensuring diverse perspectives are considered in AI development and deployment


**RICE** is a framework for evaluating the ethical implications of AI systems. It emphasizes the importance of considering the broader societal impact of AI technologies and ensuring that they are developed and used in ways that align with ethical principles and societal values.

**RICE**
- **R**esponsible
- **I**nclusive
- **C**ompliant
- **E**thical


### AI Alignment

**Goal:** to enable enterprises to tailor AI models to follow their business rules
and policies. Alignment happens during fine-tuning, involving instruction-tuning and a critique
phase.

Importance of AI alignment stems from the increasing impact and risks of Deep Learning (DL)
applications, with misalignment being a significant source of these risks.

Misalignment has two main causes

- **Reward hacking:** The model learns to exploit loopholes in the reward system, leading to unintended consequences.
  
- **Misgeneralization:** The model performs well on training data but fails to generalize to unseen data, leading to poor performance in real-world scenarios.
  
- **Example:** A model trained to maximize clicks on a website may learn to generate misleading headlines that attract clicks but do not provide valuable content, leading to user dissatisfaction and loss of trust.


Formal verification methods like SAT, LP, and SMT are used for robust ML.

These methods help ensure that the model adheres to specified constraints and behaves as expected in various scenarios.

- **SAT (Satisfiability):** A method for determining if a logical formula can be satisfied by assigning values to its variables.
  
- **LP (Linear Programming):** A mathematical optimization technique used to find the best outcome in a linear model.
  
- **SMT (Satisfiability Modulo Theories):** An extension of SAT that incorporates theories like arithmetic and arrays, allowing for more complex reasoning about the model's behavior.
  
- **Example:** Using SAT to verify that a model's predictions are consistent with specified constraints, such as ensuring that a self-driving car does not exceed speed limits.
  
- **Example:** Using LP to optimize the allocation of resources in a supply chain while adhering to constraints like demand and capacity.
  
- **Example:** Using SMT to verify that a model's predictions are consistent with specified constraints, such as ensuring that a self-driving car does not exceed speed limits.