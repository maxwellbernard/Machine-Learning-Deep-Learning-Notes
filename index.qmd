---
title: "ML Class Notes"
author: "Maxwell Bernard"
output-dir: docs
format:
    html:
        toc: true
        toc-depth: 3
        toc-expand: true
        css: styles.css
        code-block-line-numbers: true
        code-block-wrap: true
        code-overflow: wrap
        code-output-overflow: wrap
        theme: default
        code-block-theme: default
        highlight-style: pygments
        self-contained: true
---
# Lecture 2
## ML Flow
1. **Data Collection**: Gather data from various sources, such as databases, APIs, or sensors. This data can be structured (e.g., tables) or unstructured (e.g., text, images).
   
2. **Data Preprocessing**: Clean and preprocess the data to remove noise, handle missing values, normalize or standardize features, and convert categorical variables into numerical representations, feature engineering. This step ensures that the data is in a suitable format for training machine learning models.
   
3. **EDA (Exploratory Data Analysis)**: Analyze the data to understand its distribution, relationships between features, and potential patterns. This step helps identify important features, detect outliers, and gain insights into the data.
   
4. **Model Selection**: Choose an appropriate machine learning algorithm based on the problem type (e.g., classification, regression, clustering) and the characteristics of the data. Common algorithms include linear regression, decision trees, support vector machines, and neural networks.
   
5. **Split Data**: Divide the dataset into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune hyperparameters and evaluate model performance during training, and the test set is used to assess the final model's performance on unseen data.
   
6. **Model Training**: Train the selected model using the training data. This involves optimizing the model's parameters to minimize a loss function, which measures the difference between predicted and actual values.
   
7. **Model Evaluation**: Evaluate the trained model's performance using the validation set. Common evaluation metrics include accuracy, precision, recall, F1-score, and mean squared error, depending on the problem type.
   
8. **Deployment**: Once the model is trained and evaluated, it can be deployed to a production environment where it can make predictions on new, unseen data. This may involve integrating the model into an application or service.

**Instance-based learning**: Involves storing the training data and using it directly for making predictions. The model does not learn a general representation but relies on the training instances to make predictions. 
   - Examples include k-nearest neighbors (KNN) and support vector machines (SVM).

**Model-based learning**: Involves learning a general representation of the data through a model. The model captures the underlying patterns and relationships in the data, allowing it to make predictions on new instances. 
   - Examples include linear regression, decision trees, and neural networks.

## Encoding Categorical Variables

#### One-Hot Encoding
Converts categorical variables into binary vectors, where each category is represented by a separate binary feature. For example, if a categorical variable has three categories (A, B, C), it will be transformed into three binary features: [1, 0, 0] for A, [0, 1, 0] for B, and [0, 0, 1] for C. This method is suitable for nominal categorical variables without any ordinal relationship.

- Can use `pd.get_dummies()` in pandas to perform one-hot encoding.
- Can use `OneHotEncoder` from `sklearn.preprocessing` to perform one-hot encoding. 

**Limitations**: 

- One-hot encoding can lead to **high-dimensional feature spaces**, especially when dealing with categorical variables with many categories. This can result in **increased computational complexity** and potential overfitting.
  
- It **does not capture any ordinal relationships** between categories (if they exist).
  
- It can lead to **sparse matrices**, which may not be efficient for certain machine learning algorithms.
  
- It can **increase the risk of multicollinearity**, where two or more features are highly correlated, which can create bias in Linear models (e.g., linear regression, logistic regression). To solve this drop one of the categories (e.g., the first category) to avoid multicollinearity as no information is lost, as the remaining categories can be used to infer the dropped category.
  
- If high cardinality categorical variables (i.e., categorical variables with many categories) or tree based models (e.g., decision trees, random forests, gradient boosting) are used, one-hot encoding may not be the best choice as it can lead to high-dimensional feature spaces and increased computational complexity. In such cases, label encoding works better as they split the data into different branches based on the categories, which can lead to better performance and reduced computational complexity.

Example of One-Hot Encoding using Scikit-learn::
```python
from sklearn.preprocessing import OneHotEncoder
# Initialize the encoder (drop first category to avoid multicollinearity)
encoder = OneHotEncoder(drop='first', sparse_output=False)
categorical_columns = ['category1', 'category2', 'category3']
encoded_features = encoder.fit_transform(df[categorical_columns])

# Convert the encoded features to a DataFrame
encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_columns))

# Concatenate the encoded features with the original DataFrame
df = pd.concat([df.drop(categorical_columns, axis=1), encoded_df], axis=1)
# axis 1 means concatenate across columns, axis 0 means concatenate across rows
```

Example of One-Hot Encoding using Pandas:
```python
import pandas as pd
df = pd.DataFrame({
    'category1': ['A', 'B', 'C', 'A'],
    'category2': ['X', 'Y', 'X', 'Z']
})
df = pd.get_dummies(df, columns=['category1', 'category2'], drop_first=True)
```
**When Should You NOT Drop a Column?**

If the categorical variable has a meaningful order or hierarchy (ordinal categorical variable), such as "low", "medium", "high", then you should not drop a column. In this case, you can use ordinal encoding instead of one-hot encoding, where each category is assigned a unique integer value based on its order.

While dropping one column is usually the best practice, there are special cases where you might not want to:
  1.	Non-linear models (e.g., Decision Trees, Random Forests, XGBoost, k-NN)
  -	These models do not assume linear relationships between features, so multicollinearity is less of an issue.
  -	You can keep all columns if the model does not suffer from collinearity issues.

#### Frequency Encoding
Frequency encoding is a technique used to convert categorical variables into numerical representations by replacing each category with its frequency or count in the dataset. This method captures the distribution of categories and can be useful for machine learning models that require numerical input.

-	Frequency encoding is generally more efficient and scalable when the categorical variable has high cardinality. 

- When working with linear models frequency encoding can sometimes work better because it avoids multicollinearity and reduces dimensionality.

Frequency Encoding Example 
```python
frequency_encoding = df['category'].value_counts(normalize=True)
df['category_freq'] = df['category'].map(frequency_encoding)
# this will create a new column 'category_freq' in the DataFrame df, which contains the frequency of each category in the 'category' column.
```

## EDA 

An approach for data analysis that employs a variety of techniques (mostly graphical) to

- Summarize the main characteristics of the data.
- Identify patterns, trends, and relationships within the data.
- Detect anomalies or outliers that may require further investigation.
- Generate hypotheses and inform feature engineering for predictive modeling.

**FacetGrid**: A grid of plots that allows for the visualization of multiple variables or categories in a single figure. Each cell in the grid represents a different combination of variables, making it easy to compare distributions and relationships across categories.

**Pairplot**: A grid of scatter plots that shows the pairwise relationships between multiple variables in a dataset. It is useful for visualizing the distribution of individual variables and the relationships between them, helping to identify correlations and patterns.

#### Skewness

Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable. It indicates whether the data is skewed to the left (negative skew) or to the right (positive skew).

  - O: Symmetric distribution (e.g., normal distribution).
  
  - **Positive Skew**: indicates tail is on the right (longer tail on the right side), meaning most values are concentrated on the left side of the distribution.

  - **Negative Skew**: indicates tail is on the left (longer tail on the left side), meaning most values are concentrated on the right side of the distribution.

**How to handle skewness**:

- **Log Transformation**: Apply a logarithmic transformation to the data to **reduce positive skewness.** This is particularly effective for data with exponential growth patterns.

- **Square Root Transformation**: Apply a square root transformation to the data to reduce positive skewness. This is **useful for count data or data with moderate skewness.**

- **Box-Cox Transformation**: apply to stabilize variance and make the data more normally distributed. This transformation is flexible and can handle both positive and negative skewness.

- **Yeo-Johnson Transformation**: Similar to Box-Cox, but can handle zero and negative values. It is useful for data that cannot be transformed using Box-Cox.

NOTE: If you're using **SVMs or tree-based models**, you may not need to worry about skewness as these models are generally **robust to skewed data as they do not assume a particular data distribution.** However, if you're using linear models, it's important to address skewness to improve model performance and interpretability.

## Scaling/Normalization
Scaling or normalization is the process of transforming features to a similar scale, which is important for many machine learning algorithms that are sensitive to the scale of the input data.

Models that are sensitive to the scale of the input data include:

- **Linear Regression**: Sensitive to the scale of features, as it assumes a linear relationship between the input features and the target variable. Scaling helps improve convergence during optimization.
  
- **Logistic Regression**: Similar to linear regression, logistic regression is sensitive to the scale of features, as it uses a linear combination of features to predict probabilities. Scaling helps improve convergence during optimization.
  
- **Support Vector Machines (SVMs)**: SVMs are sensitive to the scale of features, as they **rely on the distance between data points** to find the optimal hyperplane. Scaling helps improve the performance of SVMs.
  
- **K-Nearest Neighbors (KNN)**: KNN is sensitive to the scale of features, as it **relies on the distance between data points** to determine the nearest neighbors. Scaling helps improve the performance of KNN.

NOTE: Models that rely on gradient descent (e.g neural networks) often converge faster when features are scaled, as **features on similar scales can lead to more efficient optimization**.

#### Min-Max Scaling
Min-Max scaling transforms features to a fixed range, typically [0, 1]. It is useful when the data has a known minimum and maximum value. 

This method is **sensitive to outliers** because it uses the minimum and maximum values of the feature to scale.

Use when the features **do not follow a Gaussian (normal) distribution**.

Apply Min-Max scaling using `MinMaxScaler` from `sklearn.preprocessing`:

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
```
#### Standardization (Z-score Normalization)
Standardization transforms features to have a mean of 0 and a standard deviation of 1. It is **useful when the data follows a Gaussian (normal) distribution.**

This method is **less sensitive to outliers** compared to Min-Max scaling, as it uses the mean and standard deviation of the feature to scale.

Use when the model assumes that the data is normally distributed, such as in linear regression, logistic regression, SVMs and Neural Networks.

Use when the model is sensitive to the variance of features, such as distance-based models (KNN, SVMs)

Apply standardization using `StandardScaler` from `sklearn.preprocessing`:

```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

If your data is sparse (a significant portion of the feature values are zero) consider using L1 Normalization (also known as **Lasso normalization**) or **L2 normalization** (also known as **Ridge normalization**) instead of standardization, as these methods are more suitable for sparse data.
  - Sparse data is often used in text analysis, such as in bag-of-words, These vectors often contain many zeros because most words don't appear in a given document.

## Scikit Pipelines
Scikit-learn pipelines are a way to streamline the process of building machine learning models by chaining together multiple steps, such as preprocessing, feature selection, and model training, into a single object. This allows for easier management of the workflow and ensures that all steps are applied consistently during both training and prediction.

Pandas is simple and good for small datasets however lacks reproducibility and scalability, while Scikit-learn pipelines are more robust and scalable, making them suitable for larger datasets and more complex workflows.

A pipeline ensures that the same transformations are **applied consistently** during training and testing, making your code **cleaner, more reproducible, and less error-prone.**

It integrates well with GridSearchCV and RandomizedSearchCV for hyperparameter tuning, as the pipeline treats the pre-processing and modelling steps as a single unit.

Created by passing a list of tuples:

- Where each tuple consists of a name (string) for the step and the corresponding transformer or estimator, eg `('scaler', StandardScaler())`
  
- The `ColumnTransformer` allows you to apply different transformations to different subsets of the features (columns) in your dataset.
  
- Use `FunctionTransformer` if you need a custom function applied across the dataset.

Eample of a Scikit-learn pipeline:
```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier  # Example model
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score # for evaluation

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ColumnTransformer with transformations applied to 'sepal length' and other columns
preprocessor = ColumnTransformer(
    transformers=[
        # Apply scaling to 'sepal length'
        ("scaled_num", StandardScaler(), ["sepal length"]),
        # Impute missing values for other numerical columns without scaling
        (
            "num_other",
            SimpleImputer(strategy="mean"),
            ["sepal width", "petal length", "petal width"],
        ),
        # Categorical transformations (e.g., one-hot encoding)
        (
            "encoded_cat",
            Pipeline(
                [
                    ("imputer", SimpleImputer(strategy="most_frequent")),
                    ("encoder", OneHotEncoder(drop="first")),
                ]
            ),
            ["Category"],
        ),
    ],
    remainder="passthrough",  # Keep all other columns that are not explicitly transformed
)

# Create a pipeline with preprocessing and model
model_pipeline = Pipeline(
    [
        ("preprocessor", preprocessor),  # First, apply the preprocessing steps
        (
            "model",
            RandomForestClassifier(),
        ),  # Then, apply the model (example: RandomForestClassifier)
    ]
)

# Fit the pipeline on the training data
model_pipeline.fit(X_train, y_train)  # Training data

# Predict with the pipeline (both preprocessing and model)
predictions = model_pipeline.predict(X_test)  # Test data for prediction

# Evaluate the model (example: using accuracy)
accuracy = accuracy_score(y_test, predictions)
```

NOTE:

- `fit_transform()` applies the transformations and returns the transformed data
- `fit()` applies the transformations and returns the transformer object
- With a 'Pipeline', transformations and model fitting can be done in a single step, making it easier to manage the workflow and ensuring that all steps are applied consistently during both training and prediction. So you can use `model_pipeline.fit(X_train, y_train)` to fit the entire pipeline, which includes both preprocessing and model training.

## Use of Validation Set
The validation set is a subset of the dataset that is used to **evaluate the performance** of a machine learning model **during the training process.** It is separate from the training set and the test set.

The validation set helps you ensure the model’s performance is robust and not biased toward the training set.

It serves several purposes:

- **Hyperparameter Tuning**: The validation set is used to tune the hyperparameters of the model, such as learning rate, regularization strength, and number of hidden layers. By evaluating the model's performance on the validation set, you can select the best hyperparameters that lead to better generalization on unseen data.

- **Model Selection**: The validation set helps in selecting the best model architecture or algorithm among multiple candidates. By comparing the performance of different models on the validation set, you can choose the one that performs best.

- **Early Stopping**: The validation set can be used to implement early stopping during training. If the model's performance on the validation set starts to degrade while the training performance continues to improve, it indicates that the model is overfitting. Early stopping allows you to halt training at the point where the model performs best on the validation set.

- **Performance Monitoring**: The validation set provides a way to monitor the model's performance during training. By evaluating the model on the validation set at regular intervals, you can track its progress and make adjustments if necessary.

## Cross-Validation
Cross-validation is a technique used to assess the performance of a machine learning model by splitting the dataset into multiple subsets (folds) and **training the model on different combinations of these subsets.**

It helps to ensure that the model generalizes well to unseen data and reduces the risk of overfitting.

**K-Fold Cross-Validation**:

- The dataset is divided into 'k' equally sized folds (subsets).
- The model is trained on 'k-1' folds and validated on the remaining fold. 
- This process is repeated 'k' times, with each fold serving as the validation set once. 
- The final performance metric is the **average of the metrics obtained from each fold.**

**Stratified K-Fold Cross-Validation**: 

- Similar to K-Fold, but ensures that **each fold has a similar distribution of target classes.**
- This is particularly useful for imbalanced datasets, where some classes have significantly more samples than others.

Python example of **K-Fold Cross-Validation** using Scikit-learn:
```python
from sklearn.model_selection import KFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier
# Initialize the model
model = RandomForestClassifier()
# Initialize K-Fold cross-validation with 5 folds
kf = KFold(n_splits=5, shuffle=True, random_state=42)
# Perform cross-validation
scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')
# Print the average accuracy across all folds
print(f'Average accuracy: {scores.mean()}')
```

Python example of **Stratified K-Fold Cross-Validation** using Scikit-learn:
```python
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier
# Initialize the model
model = RandomForestClassifier()
# Initialize Stratified K-Fold cross-validation with 5 folds
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
# Shuffle=True ensures that the data is shuffled before splitting into folds, which helps to ensure that the folds are representative of the overall dataset.

# Perform cross-validation
scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')
# Print the average accuracy across all folds
print(f'Average accuracy: {scores.mean()}')
```
## Evaluating Classification Models
Evaluating classification models involves assessing their performance using various metrics that quantify how well the model predicts the target classes. Common evaluation metrics include:

- **Accuracy**: The proportion of correctly predicted instances out of the total instances. It is calculated as:
  $$
  \text{Accuracy} = \frac{\text{True Positives} + \text{True Negatives}}{\text{Total Instances}}
  $$
  - True Positives (TP): The number of instances correctly predicted as positive.
  - True Negatives (TN): The number of instances correctly predicted as negative.
  - Total Instances: The total number of instances in the dataset.
  
  - It is **suitable for balanced datasets** but can be misleading for imbalanced datasets.

- **Precision**: The proportion of true positive predictions out of **all positive predictions** made by the model. It is calculated as:
  $$\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}$$
  - It is useful when the cost of false positives is high, such as in spam detection.
  
  - It is useful when the cost of false positives is high, such as in medical diagnoses.
  
  - A false positive in tumor detection is when the model predicts a tumor is present when it is not, leading to unnecessary stress and medical procedures for the patient.

- **Recall (Sensitivity)**: The proportion of true positive predictions out of **all actual positive** instances. It is calculated as:
  $$
  \text{Recall} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}
  $$
  - It is useful when the cost of false negatives is high, such as in fraud detection.
  - It is also known as **sensitivity** or **true positive rate**.
  - A false negative in tumor detection is when the model predicts a tumor is not present when it is, leading to missed diagnoses and potentially life-threatening consequences for the patient.

- **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two metrics. It is calculated as:
  $$
  \text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
  $$
  - It is useful when you want to balance precision and recall, especially in imbalanced datasets. (eg there are many more negative instances than positive instances).
  
  - For tumor detection, F1 score is more important than accuracy, as it balances the trade-off between precision and recall, ensuring that both false positives and false negatives are minimized.

  - An F1 Score **close to 1 means that the model is performing well**, both in terms of precision and recall.
  
  - An F1 Score close to **0 means that the model is performing poorly**, either in terms of precision or recall, or both.

  - Apply using `classification_report` from `sklearn.metrics`
    - It provides a detailed report of precision, recall, F1 score, and support (the number of true instances for each class) for each class in the dataset.
    - **Macro averaged F1 score** is the average of the F1 scores for each class, **treating all classes equally**.
    - **Weighted averaged F1 score** is the average of the F1 scores for each class, weighted by the number of true instances for each class. This is **useful when dealing with imbalanced datasets**, as it **gives more importance to classes with more instances**.


- **Confusion Matrix**: A table that summarizes the performance of a classification model by showing the counts of true positive, true negative, false positive, and false negative predictions. It provides a detailed view of the model's performance across different classes.

  - Apply using `confusion_matrix` from `sklearn.metrics`

**AU-ROC Curve (Receiver Operating Characteristic Curve)**:

The AU-ROC curve is a graphical representation of the model's performance across different classification thresholds. It plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various threshold values.

A widely used metric for evaluating the performance of binary classification models (especially with imbalanced datasets).

The area under the ROC curve (AUC-ROC) quantifies the model's ability to distinguish between classes, with a value of 1 indicating perfect classification and a value of 0.5 indicating random guessing.
  - Apply using `roc_curve` and `auc` from `sklearn.metrics`

**Perfect Classifier**: A perfect classifier achieves an AUC-ROC of 1, meaning it can perfectly distinguish between positive and negative instances at all classification thresholds. (Curve hugs top left corner of the plot)

**Random Classifier**: A random classifier achieves an AUC-ROC of 0.5, meaning it has no discriminative power and performs no better than random guessing. (Curve is a diagonal line from bottom left to top right corner of the plot)

- A ROC-AUC: 0.9423 means there is a 94.23% chance the model correctly ranks a positive sample above a negative one (excellent performance)

**Strengths of AU-ROC**:

- It is **robust to class imbalance**, as it evaluates the model's performance across all classification thresholds, rather than relying on a single threshold.

**Weaknesses of AU-ROC**:

- It may not provide a clear understanding of the model's performance at specific classification thresholds, which can be important in certain applications (e.g., medical diagnoses, fraud detection).
  
- **Ignores Precision**: 
  - The AU-ROC curve does not consider precision, which can be important in applications where false positives have significant consequences (e.g., medical diagnoses, fraud detection).

  - A model with high recall but low precision may achieve a high AUC-ROC, but it may not be suitable for applications where false positives are costly.

  - In this case, its better to use Precision-Recall Curve (PRC) instead of AU-ROC curve.

## Evaluating Regression Models
Evaluating regression models involves assessing their performance using various metrics that quantify how well the model predicts continuous target values. Common evaluation metrics include:

**Mean Squared Error (MSE)**: The average of the squared differences between predicted and actual values. It is calculated as:
$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$
  - Where $y_i$ is the actual value, $\hat{y}_i$ is the predicted value, and $n$ is the number of instances.

  - It penalizes larger errors more heavily, making it **sensitive to outliers**.

**Root Mean Squared Error (RMSE)**: The square root of the mean squared error. It is calculated as:
$$
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$
  - It provides a measure of the average magnitude of the errors in the predictions, with lower values indicating better performance.
  
  - RMSE is in the same units as the target variable, making it easier to interpret compared to MSE.

  - If RMSE is small relative to the range of the target variable, it indicates that the model is performing well.(e.g., RMSE is 0.5 for a target variable with a range of 0 to 10, indicating good performance)

  - Is **sensitive to outliers**, as it squares the errors, which can disproportionately affect the overall error metric.

**Mean Absolute Error (MAE)**: The average of the absolute differences between predicted and actual values. It is calculated as:
$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$
  - Where $y_i$ is the actual value, $\hat{y}_i$ is the predicted value, and $n$ is the number of instances.
  
  - It provides a straightforward measure of prediction accuracy, with lower values indicating better performance.
  
  - If MAE is small relative to the target range (e.g., MAE = 8 when prices range from 100 to 1000), the model is reasonably accurate

  - It is **less sensitive to outliers** compared to MSE and RMSE, as it does not square the errors.

**R-squared (Coefficient of Determination)**: A statistical measure that represents the proportion of variance in the target variable that can be explained by the independent variables in the model. It is calculated as:
$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$
  - Where $\bar{y}$ is the mean of the actual values.

  - R-squared values range from 0 to 1, with higher values indicating better model performance. A value of 1 indicates that the model perfectly explains the variance in the target variable, while a value of 0 indicates that the model does not explain any variance.

  - It is useful for comparing the performance of different regression models on the same dataset.
  
  - **Adjusted R-squared**: A modified version of R-squared that adjusts for the number of predictors in the model. It is preferred when comparing models with different numbers of predictors, as it penalizes the addition of irrelevant predictors that do not improve the model's performance. It answers the question: "Is my model explaining variance efficiently given the number of predictors?" 

# Lecture 3

## How to Choose a Model

Machine learning problems can be categorized into:

-	**Supervised** (when target labels are available)
  -	Regression (predicting continuous values)
  -	Classification (predicting discrete classes)

-	**Unsupervised** (when target labels are not available)
  -	Clustering (grouping similar instances)
  -	Dimensionality reduction (reducing the number of features)

-	**Reinforcement learning** (when an agent learns to make decisions by interacting with an environment)
  -	Sequential decision-making (learning to take actions based on the current state of the environment)

Regression Models:

- Linear Regression
- Logistic Regression (for binary classification)
- Support Vector Regression (SVR)
- Decision Trees
- Random Forest Regression
- Gradient Boosting Regression
- Neural Networks (for regression tasks)
- k-Nearest Neighbors (k-NN)
- Lasso Regression (L1 regularization)
- Ridge Regression (L2 regularization)
- Elastic Net Regression (combination of L1 and L2 regularization)
- XGBoost Regression
- LightGBM Regression
- CatBoost Regression
- Long Short-Term Memory (LSTM) networks for time series forecasting
- Gated Recurrent Units (GRU) for time series forecasting
- Recurrent Neural Networks (RNNs) for time series forecasting
- Convolutional Neural Networks (CNNs) for time series forecasting
- Multivariate Adaptive Regression Splines (MARS)

Classification Models:

- Logistic Regression (for binary classification)
- Support Vector Machines (SVM)
- Decision Trees
- Random Forest Classifier
- Gradient Boosting Classifier
- Neural Networks (for classification tasks)
- k-Nearest Neighbors (k-NN)
- Naive Bayes Classifier
- AdaBoost Classifier
- XGBoost Classifier
- LightGBM Classifier
- CatBoost Classifier
- Linear Discriminant Analysis (LDA)
- Quadratic Discriminant Analysis (QDA)
- Support Vector Classifier (SVC)
- Multi-Layer Perceptron (MLP) Classifier
- Convolutional Neural Networks (CNNs) for image classification
- Recurrent Neural Networks (RNNs) for sequence classification
- Long Short-Term Memory (LSTM) networks for sequence classification
  
Each of which has numerous candidate algorithms. Some key reasons why multiple algorithms exist include:

-	**Different assumptions**: Algorithms make different assumptions about the data, such as linearity, independence of features, or distribution of errors. These assumptions can affect the model's performance and suitability for a specific problem.

-	**Scalability needs**: Some algorithms are designed for small datasets (e.g., k-NN), while others scale to large datasets (e.g., gradient boosting, deep learning).

-	**Interpretability**: Some algorithms (e.g., linear regression, decision trees) are more interpretable and easier to understand, while others (e.g., deep learning) are more complex and harder to interpret.

-	**Performance**: Different algorithms may perform better or worse depending on the characteristics of the data, such as noise, outliers, or feature interactions.

-	**Computational efficiency**: Some algorithms are computationally more efficient than others, making them more suitable for real-time applications or large datasets.

When tackling a machine learning problem, there is no universal best choice. The performance of an algorithm depends on:

-	**Data dimensionality**: High-dimensional data may require feature selection or dimensionality reduction.

-	**Size of the dataset**: Some algorithms struggle with small data (e.g., deep learning), while others perform well (e.g., logistic regression, k-NN).

-	**Data distribution**: The distribution of the data can affect the performance of different algorithms, such as linearity, normality, or independence of features.

-	**Computational complexity**: Algorithms like Support Vector Machines (SVMs) can be computationally expensive for large datasets, whereas decision trees scale well.

Examples:

-	**Linear models** (e.g., linear regression, logistic regression) are suitable for problems with linear relationships between features and target variables.

-	**Decision trees** are suitable for problems with non-linear relationships and can handle categorical features, but may overfit without pruning.

-	**Support Vector Machines (SVMs)** are suitable for high-dimensional data and can handle non-linear relationships using kernel functions, but may struggle with large datasets.

-	**Neural networks** are suitable for complex problems with large datasets and can learn non-linear relationships, but require careful tuning of hyperparameters and may be computationally expensive.

Since no single algorithm is universally best, model selection is often an **iterative process** involving **experimentation and evaluation.**

A commonly accepted principle in data science is that a **simple model trained on a large dataset often outperforms a complex model trained on a small dataset.** This is because:

1.	**More data reduces variance:** Small datasets introduce high variance and overfitting, whereas large datasets allow algorithms to generalize better.

2.	**Feature engineering improves with data:** Having more samples helps discover important patterns that even simple models can capture.

3.	**Deep learning is data-hungry:** Neural networks require millions of examples to achieve high accuracy.

#### Algorithm Choice & Data Volume:

The effectiveness of an algorithm is highly dependent on the amount of data available:

- **Small Datasets**: (< 10,000 samples)
  - Simple models (e.g., linear regression, logistic regression) often perform well.
  - Complex models (e.g., deep learning) may overfit due to limited data.

- **Medium Datasets**: (10,000 - 1 million samples)
  - Simple models can still be effective, but more complex models (e.g., decision trees, random forests) can capture non-linear relationships.
  - Ensemble methods (e.g., bagging, boosting) can improve performance by combining multiple models.

- **Large Datasets**: (> 1 million samples)
  - Complex models (e.g., CNNs, gradient boosting) can be trained effectively.

#### Scaling Considerations

- **Memory Constraints**: Large datasets may not fit into memory, requiring techniques like mini-batch training or distributed computing.
  
- **Training Time**: Complex models may take longer to train on large datasets, requiring efficient optimization algorithms and hardware acceleration (e.g., GPUs).
  
- **k-NN** performs well in low dimensions but is computationally expensive with large datasets.
  
- **SVMs** can be effective for high-dimensional data but may struggle with large datasets due to their computational complexity.
  
- **Gradient Boosting** is effective for large datasets but may require careful tuning of hyperparameters to avoid overfitting.
  
- **Neural Networks** are powerful for large datasets but require significant computational resources and careful tuning of hyperparameters.
  
## Benefits on Clustering
Clustering is an unsupervised learning technique that groups similar instances together based on their features. It is widely used in various applications, including customer segmentation, image compression, and anomaly detection.

Clustering algorithms can be broadly categorized into:

- **Partitioning Methods**: These algorithms divide the dataset into a fixed number of clusters, such as **K-Means**. They are efficient for large datasets and work well when clusters are spherical and evenly sized.
  - K-Means assigns each data point to a cluster and uses the cluster centre as a representative feature.

- **Density-Based Methods**: These algorithms group instances based on the density of data points, such as **DBSCAN**. They are effective for identifying clusters of varying shapes and sizes, and can handle noise in the data.
  
Clustering helps in Dimensionality Reduction by identifying patterns and relationships in the data, allowing for the creation of lower-dimensional representations that retain important information. Instead of using all features, clustering can help identify representative features or groups of features that capture the underlying structure of the data, by **replacing the original features with cluster centroids** or representative instances.

Points that **do not belong strongly to any cluster** (i.e., have high distance from all cluster centroids) can be flagged as **outliers or anomalies.**

Outlier detection is essential in various domains such as fraud detection, medical diagnosis, and cybersecurity. Since **outliers do not conform to the general pattern of the data**, they tend to have **low affinity to all clusters.**

k-Means Distance-Based Outlier Detection:

- In k-Means, outliers can be identified by calculating the distance of each data point to its assigned cluster centroid.
  
- If the distance exceeds a certain threshold (e.g., a multiple of the average distance to the centroid), the point can be flagged as an outlier.


## Semi Supervised Learning
Semi-supervised learning is a machine learning approach that combines both labeled and unlabeled data to improve model performance. It is particularly useful when obtaining labeled data is expensive or time-consuming, while unlabeled data is abundant.

In real-world applications, labelled data is often **scarce and expensive to obtain**, whereas unlabelled data is abundant. Semi-supervised learning **leverages both labelled and unlabeled data** to improve model performance.

1.	**Clustering Groups Similar Data Together:**

   - Clustering algorithms, such as K-Means or DBSCAN, can be applied to the unlabeled data to group similar instances together.
  
   - These clusters can provide valuable insights into the underlying structure of the data and help identify patterns.
  
   - For example, in customer segmentation, clustering can group customers with similar purchasing behaviors, allowing for targeted marketing strategies.

2.  **Using Clusters to Generate Pseudo-Labels:**

   - Once clusters are formed, pseudo-labels can be assigned to the unlabeled data based on the cluster assignments.
  
   - For instance, if a cluster contains mostly positive instances, the pseudo-label for that cluster can be set to positive.
  
   - This process allows the model to learn from the structure of the data without requiring explicit labels for every instance.

- **Step 1:** Perform clustering on the entire dataset.
- **Step 2:** Assign known labels to the closest cluster(s).
- **Step 3:** Propagate labels to unlabeled data points in the same cluster.

**Common Clustering Methods for Semi-Supervised Learning:**

- **k-Means:** Assign labels to cluster centroids and extend to all points in the cluster.
  
- **Hierarchical Clustering:** Identify meaningful clusters and label based on hierarchy.
  
- **Self-Training with Clustering:** Use initial labelled data to train a model, predict labels for unlabeled data, and refine.
  
- **Use Case:** Imagine a medical dataset where only 5% of patients are labelled with a disease. Using clustering, we can group similar patients and propagate labels to make better predictions.

## Clustering Models
Clustering models are **unsupervised learning** algorithms that group similar data points together based on their features. They are widely used in various applications, such as customer segmentation, image compression, and anomaly detection.

Clustering is an unsupervised learning technique, meaning we **do not have predefined labels to compare against**. This makes **evaluating clustering performance inherently more challenging** than supervised learning methods like classification or regression.

We cannot directly compute an error rate like we do in classification (**we lack an objective measure of correctness.**)

Performance depends on **hyperparameters** like:

 - the number of clusters
 - distance metrics
 - initialization.
 - convergence criteria.

### K-Means
K-Means is a popular clustering algorithm that partitions the dataset into a fixed number of clusters (k) based on the similarity of data points. It is widely used due to its simplicity and efficiency.

The K-Means algorithm follows these steps:

1.	**Initialize** k cluster centroids **randomly**.
2.	**Assign each data point to the nearest centroid** (using Euclidean distance).
3.	**Update centroids** by computing the mean of all points in each cluster.
4.	**Repeat steps 2 and 3 until convergence** (when centroids no longer change significantly).

**Computationally efficient:** Each iteration requires only simple operations (distance calculation, mean computation).

**Scales well to large datasets:** Time complexity is approximately **O(nkT)**,

 - **n** is data size
 - **k** is clusters
 - **T** is iterations. 
 - Often reaches a stable solution in a few iterations.


**Limitations of K-Means**:

- **Choosing k**: The number of clusters (k) must be specified beforehand, which can be challenging without domain knowledge.

- **Sensitivity to initialization**: The algorithm can converge to different solutions based on the initial placement of centroids. To mitigate this, multiple runs with different initializations can be performed, and the best solution can be selected based on the lowest inertia (sum of squared distances from points to their assigned centroids).

- **Assumes spherical clusters**: K-Means assumes that clusters are spherical and evenly sized, which may not hold true for all datasets. It may struggle with clusters of varying shapes, sizes, or densities.

- **Sensitive to outliers**: Outliers can disproportionately affect the position of centroids, leading to suboptimal clustering results. To address this, preprocessing steps like outlier removal or robust scaling can be applied.

- If centroids are poorly placed initially, K-Means might get stuck in a local minimum where small improvements are no longer possible.

**Limitation Solutions:**

- **Choosing k**: Use methods like the **Elbow Method** or **Silhouette Score** to determine the optimal number of clusters.
  
- **Sensitivity to initialization**: Use the **K-Means++ initialization** method, which selects initial centroids more strategically to improve convergence.
  
- Consider using other clustering algorithms like **DBSCAN** or **Gaussian Mixture Models (GMM)** that can handle non-spherical clusters.
  
`k'Means++` is an improved version of K-Means that addresses the sensitivity to initialization by selecting initial centroids more strategically. It helps to avoid poor local minima and improves convergence.
```python
from sklearn.cluster import KMeans
# Initialize K-Means with k=3 and K-Means++ initialization
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)
# Fit the model to the data
kmeans.fit(X)
# Get the cluster labels for each data point
labels = kmeans.labels_
# Get the cluster centroids
centroids = kmeans.cluster_centers_
```
#### Mini-Batch K-Means
Mini-Batch K-Means is a variant of K-Means that **processes data in small batches** instead of the entire dataset at once. This approach **reduces memory usage** and **speeds up convergence**, making it suitable for large datasets.

It updates centroids incrementally rather than recalculating them using the entire dataset. This allows it to run significantly faster on large datasets with only a small loss in accuracy.

- Less memory usage → Works well for big data.
- Faster convergence → Runs in constant time per iteration, making it scalable.
- Good approximation → Provides clusters similar to full K-Means but faster.
  
```python
from sklearn.cluster import MiniBatchKMeans
# Initialize Mini-Batch K-Means with k=3 and batch size of 100
minibatch_kmeans = MiniBatchKMeans(n_clusters=3, batch_size=100, random_state=42)
minibatch_kmeans.fit(X)
```
### Silhouette/Elbow Method
The Silhouette and Elbow methods are techniques used to evaluate the quality of clustering results and **help determine the optimal number of clusters (k)** in K-Means clustering.


**Silhouette Score**:

The Silhouette Score measures how similar an object is to its own cluster compared to other clusters. It ranges from -1 to 1.

To find the optimal number of clusters k, compute the Silhouette Score for different values of k and pick the one with the highest score

- If **close to +1** → The point is **well-clustered** (tight within its own cluster, far from others).
- If SSS is close to 0 → The point is on a cluster boundary (could belong to multiple clusters).
- If SSS is negative → The point is likely misclassified (closer to another cluster than its own).

```python
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

silhouette_scores = []
for k in range(2, 11):  # Test k from 2 to 10
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)  # Fit the model to the data
    score = silhouette_score(X, kmeans.labels_)  # Calculate Silhouette Score
    silhouette_scores.append(score)  # Store the score

# Plot Silhouette Scores
plt.plot(range(2, 11), silhouette_scores, marker='o')
```

**Elbow Method**:
The Elbow Method is a heuristic used to determine the optimal number of clusters by plotting the **inertia** (sum of squared distances from points to their assigned centroids) against the number of clusters (k). The "elbow" point in the plot indicates the optimal k.

- As k increases, inertia decreases because more clusters lead to better fit.
  
- The goal is to **find the point where adding more clusters yields diminishing returns** (the "elbow").

Implementing the Elbow Method in Python:
```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertia = []
for k in range(1, 11):  # Test k from 1 to 10
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)  # Fit the model to the data
    inertia.append(kmeans.inertia_)  # Store the inertia value
  
# Plot Elbow Curve
plt.plot(range(1, 11), inertia, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.show()
```

The Elbow Method is particularly **useful when the clusters are well-separated and have distinct shapes**. However, it may not work well for datasets with overlapping clusters or varying densities.


Use the Elbow Method when:

1.	You have large datasets (faster to compute/computationally efficient).
2.	You expect well-separated clusters.
3.	You want a quick, heuristic approach.

Use the Silhouette Score when:

1.	You need an objective measure (no manual elbow detection).
2.	Clusters may overlap or be irregularly shaped.
3.	You're comparing different clustering algorithms.

### DBSCAN
Density-Based Spatial Clustering of Applications with Noise. 

Is an **unsupervised clustering algorithm** that groups together points that are closely packed together while marking points that lie alone in low-density regions as outliers. It is particularly effective for datasets with clusters of varying shapes and sizes.

Is an density-based clustering algorithm that **does not require the number of clusters to be specified**. Instead, it identifies clusters based on the density of data points in the feature space.


How it works:

1.	**Define parameters**: DBSCAN requires two parameters:
     - **Epsilon (eps)**: The maximum distance between two points to be considered part of the same cluster.
     - **MinPts**: The minimum number of points required to form a dense region (core point).
  
2.	**Identify core points**: A point is a core point if it has at least MinPts points within its eps-neighborhood.

3.	**Form clusters**: DBSCAN starts with an unvisited point, marks it as a core point, and expands the cluster by adding all points within its eps-neighborhood. It continues to expand the cluster until no more points can be added.

4.	**Handle noise**: Points that are not core points and do not belong to any cluster are considered noise or outliers.

How To Select Parameters:

1. Find optimal eps using KNN distance plot (Elbow Method)
2. Set MinPts to a value based on domain knowledge or rule of thumb (e.g., 2 * number of features).

-	If your dataset has low noise → A lower minPts (3 or 4) can work.
-	If your dataset is noisy or high-dimensional → A higher minPts (10–20) is better.
-	For large datasets → You might need larger minPts (50+ for thousands of points).

Elbow Method for eps:
```python
X = np.random.rand(100, 2)  # Example data

minPits = 5  # Set a reasonable value for MinPts
nbrs = NearestNeighbors(n_neighbors=minPts).fit(X)
distances, indices = nbrs.kneighbors(X)

# Sort distances to find the optimal eps
sorted_distances = np.sort(distances[:, minPts - 1])
plt.plot(sorted_distances)
plt.xlabel('Points sorted by distance')
plt.ylabel('Distance to {}-th nearest neighbor'.format(minPts))
plt.title('Elbow Method for DBSCAN eps')
plt.show()
```

### Hierarchical

Hierarchical clustering is a **sequential clustering method** that builds a tree-like structure (dendrogram) using a distance matrix, without needing to predefine the number of clusters.

Types:

1.	Agglomerative (Bottom-Up): Each data point starts as its own cluster; merges continue until one cluster or stopping criteria is met.

2.	Divisive (Top-Down): All data points start in one cluster; splits continue until individual points remain.

**Advantages:**

- No need to specify number of clusters.
- Effective for small datasets.
- Can detect clusters of arbitrary shapes.

**Disadvantages:**

- Computationally intensive for large/high-dimensional data.
- Sensitive to outliers.

**Dendrogram:** A visual tree that illustrates the merging/splitting process and helps identify the optimal number of clusters.

**Linkage criteria** determine how the **distance between clusters is calculated** during hierarchical clustering:

- **Single Linkage:** Uses the **shortest distance** between any two points in the clusters.
- **Complete Linkage:** Uses the **longest distance** between any two points in the clusters.
- **Average Linkage:** Uses the **average** of all pairwise distances between points in the two clusters.
- **Ward’s Linkage:** Merges clusters to **minimize the total within-cluster variance** (sum of squared distances).

# Lecture 4 

## Over/Underfitting

**Overfitting** occurs when a model learns the noise in the training data rather than the underlying patterns, leading to **poor generalization to new data**. It typically happens when the model is too complex relative to the amount of training data available.

**Underfitting** occurs when a model is too simple to capture the underlying patterns in the data, leading to **poor performance on both the training and test data**. It typically happens when the model is not complex enough relative to the amount of training data available.

**Loss Function:** Measures the difference between predicted and actual values.

**Optimization Algorithm:** Typically, models are trained using gradient descent (GD) to minimize the loss.

## Linear Regression

Linear regression is a simple and commonly used algorithm for **predicting a continuous target variable** based on one or more input features. 

The goal is to find the best-fitting linear relationship between the input features and the target variable.

It assumes a linear relationship between:
- **Dependent variable (Y)**: The value you want to predict.
- **Independent variables (X)**: The input features used for prediction.

The linear regression model can be represented as:
  $$ y = W^T X + \epsilon $$
  where:
  - $y$ is the predicted output (target variable).
  - $W$ is the weight vector (model parameters).
  - $X$ is the input feature vector.
  - $\epsilon$ is the error term (noise).

- **Loss Function:** The loss function for linear regression is typically the **mean squared error (MSE)**:
  $$ L(W) = \frac{1}{n} \sum_{i=1}^{n} (y_i - W^T x_i)^2 $$
  where:
  - $n$ is the number of samples.
  - $y_i$ is the actual target value for sample $i$.
  - $x_i$ is the input feature vector for sample $i$.


- **Optimization Algorithm:** The optimization algorithm used to minimize the loss function is usually gradient descent (GD). The update rule for the weights is:
  $$ W \leftarrow W - \eta \nabla L(W) $$
  where:
  - $\eta$ is the learning rate.
  - $\nabla L(W)$ is the gradient of the loss function with respect to the weights.


Key Assumptions of Linear Regression:

- **Normality:** For each value of the independent variable, the distribution of the dependent variable must be normal.

- **Linearity:** The relationship between independent variables (X) and the dependent variable (Y) must be linear (If data is nonlinear, transformations (log, polynomial features) or nonlinear models might be needed.)

- **Independence:** All observations should be independent of each other.

- **No Multicollinearity (For Multiple Regression):** Independent variables should not be highly correlated with each other. (Independent variables should not be highly correlated with each other.)


Disadvantages:

- Performs poorly when there are non-linear relationships. 
- Sensitive to outliers

## KNN 
Is a non-parametric (No strict assumptions on data distribution), instance-based **supervised learning algorithm**

Makes predictions based on the majority class of the K nearest data points in the feature space.

It is a **distance-based algorithm,** typically using **Euclidean distance** to measure similarity between data points.

Commonly used for simple recommendation systems, pattern recognition, data mining, financial market predictions, intrusion detection

Computing KNN (Algorithm Steps)

1.	Select a value for k (number of neighbours)
2.	Compute distance between the new data point and all points in the dataset
3.	Select the k closest points based on distance
4.	Assign the most common class among neighbours

-	Note: The model **does not learn during training**, it memorizes data and computes predictions when queried

Distance Metrics used in KNN

-	**Euclidean** Distance (default): Limited to real-valued vectors. It calculates the **straight-line distance**

-	**Manhattan** Distance: Measures **absolute value** between two points.

-	**Minkowski** Distance: Generalized form of Euclidean and Manhattan distance metrics

Defining k

-	Different k-values can lead to overfitting or underfitting
    - **Lower values** of k can have **high variance**, but **low bias**
    - Larger values of k may lead to high bias and lower variance
  
-	Choice of k will largely depend on input data

-	Data with more outliers or noise will likely perform better with higher values of k

-	Must have an odd number of k to avoid ties in classification

-	Cross-validation can help choose optimal k for your dataset:
    - Cross-validation helps systematically evaluate different values of K and find the one that performs best on unseen data.
    - Can use `cross_val_score` from `sklearn.model_selection`


Applications of KNN

-	**Data pre-processing:** Datasets frequently have missing values, but KNN can estimate for those values [Missing data imputation]. 

-	**Recommendation Engines:** Using clickstream data from websites, KNN can provide automatic recommendations to users on additional content. 

-	**Finance:** Using KNN on credit data can help banks assess risk of a loan. 

-	**Healthcare:** Predicting risk of heart attacks and prostate cancer. 

Advantages

-	Simple and easy to implement
-	No assumptions on data (Non-parametric, **works well with nonlinear distributions**)
-	Few hyperparameters
-	Works with multi-class data (can classify into multiple categories)

Disadvantages 

-	Scaling problem (Slow for large datasets since it computes distances at query time)
-	Curse of dimensionality (performance drops as dimensions increase)
-	Sensitive to irrelevant features (features should be normalized)
-	Prone to overfitting (due to non-optimal k selection)

## Logistic Regression

Based on log-odds or **logit function**, which models the probability of the outcome being a particular class

The logistic regression equation is:
$$logit(p) = \log\left(\frac{p}{1-p}\right) = W^T X + b$$
where:
- $p$ is the probability of the positive class.
- $W$ is the weight vector (model parameters).
- $X$ is the input feature vector.
- $b$ is the bias term (intercept).
- The logit function transforms the probability into a linear combination of the input features.
- The output of the logit function is then passed through the **sigmoid function** to obtain the predicted probability:
$$p = \sigma(W^T X + b) = \frac{1}{1 + e^{-(W^T X + b)}}$$

- The sigmoid function maps the output of the logit function to a probability between 0 and 1.

- The loss function for logistic regression is typically the **binary cross-entropy loss**:
$$L(W, b) = -\frac{1}{n} \sum_{i=1}^{n} \left[y_i \log(p_i) + (1 - y_i) \log(1 - p_i)\right]$$
where:
- $n$ is the number of samples.
- $y_i$ is the actual target value for sample $i$ (0 or 1).
- $p_i$ is the predicted probability for sample $i$.  

- The optimization algorithm used to minimize the loss function is usually **gradient descent (GD)**. The update rule for the weights and bias is:
$$W \leftarrow W - \eta \nabla L(W, b)$$
$$b \leftarrow b - \eta \frac{\partial L(W, b)}{\partial b}$$ 
where:
- $\eta$ is the learning rate.
- $\nabla L(W, b)$ is the gradient of the loss function with respect to the weights.
- $\frac{\partial L(W, b)}{\partial b}$ is the gradient of the loss function with respect to the bias.
- The gradients are computed using backpropagation, which applies the chain rule to compute the gradients efficiently.

**Multinomial logistic regression:** Dependent variable has three or more possible outcomes but no specified order.

**Ordinal Logistic Regression** is used when the dependent variable has ordered categories (e.g., low, medium, high). While the classes are categorical, they have an inherent order.

- Eg Predicting customer satisfaction on a scale (1 = Poor, 2 = Fair, 3 = Good, 4 = Excellent).

Logistic regression with **many features** where some might not contribute to the model (**Lasso** can help select important features).

When the model might be **overfitting** the training data (**Ridge regularization** can help generalize better).

# Lecture 5 

## PCA
Principal Component Analysis (PCA) is an **unsupervised non-parametric dimensionality reduction** technique.

It is a technique that **transforms high-dimensional** data into a **lower-dimensional space** while **preserving as much variance as possible**.

It is widely used for feature extraction, visualization, and noise reduction.

Why use PCA?

**Reduces dimensionality**: helps with overfitting and computation efficiency

**Removes noise**: PCA can help filter out noise from the data by focusing on the most significant components

Steps of PCA

1. **Standardize the data**: Center the data by subtracting the mean and scaling to unit variance.
   $$ X' = \frac{X - \mu}{\sigma} $$
   where $X$ is the original data, $\mu$ is the mean, and $\sigma$ is the standard deviation.
   
   - This step ensures that all features contribute equally to the PCA transformation, especially when they are on different scales. Can use `StandardScaler` from `sklearn.preprocessing` to standardize the data.

2. **Compute the covariance matrix**: Calculate the covariance matrix of the standardized data
   - The covariance matrix captures the relationships between different features in the data. Can use `np.cov` to compute the covariance matrix.

3. **Compute the eigenvalues and eigenvectors**: Calculate the eigenvalues and eigenvectors of the covariance matrix
   - **Eigenvalues** represent the amount of **variance explained by each principal component**
   - **Eigenvectors** represent the **direction** of the principal components.
   - Can use `np.linalg.eig` to compute eigenvalues and eigenvectors.

4. **Select the top k eigenvectors**: Choose the top k eigenvectors *corresponding to the largest eigenvalues*
   - These eigenvectors form the new feature space, where k is the desired number of dimensions.
   - Can use `np.argsort` to sort eigenvalues and select the top k eigenvectors.

5. **Project the data**: Project the original data onto the new feature space using the selected eigenvectors
   - Can use the dot product to project the data: $X_{proj} = X' W_{top}$, where $W_{top}$ contains the top k eigenvectors.

Use explained variance ratio — typically retain enough to explain 90–95% of variance.

Example in Python retaining k components that explain 90% variance:

```python
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # X is the original data

# Initialize PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)
# Calculate cumulative explained variance ratio
explained_variance = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)

# Find the number of components that explain 90% variance
var_threshold = 0.90
k = np.argmax(cumulative_variance >= var_threshold) + 1  # +1 because index starts at 0
print(f"Number of components to retain 90% variance: {k}")
```
When to Use PCA?

- KNN, K-Means, SVM, GaussianNB (**distance-sensitive**)
  
- When you have high-dimensional data (e.g., images, text) and want to reduce dimensionality while preserving variance.
  
- Logistic Regression, Linear Regression (when features are highly correlated, PCA can help **reduce multicollinearity**).

- Neural Networks (to reduce input dimensionality and improve training efficiency).

When Not to Use PCA?

- Tree-based models (handle high dimensions well)
    - Models such as **Decision Trees, Random Forests, and Gradient Boosting** do not require PCA since they can handle high-dimensional data without dimensionality reduction.
  
- Models with Sparse Data
    -   Lasso Regression: Already performs feature selection by shrinking coefficients.
    -   **Text-Based Models** (e.g., NLP with TF-IDF, BERT, Word2Vec): PCA may discard useful information from text embeddings.

Types of PCA

- **Randomized PCA** – faster, less memory efficient (whole training set needs to be stored in memory)

- **Incremental PCA** – processes data in batches, useful for large datasets that do not fit in memory
  
- **Kernel PCA** – applies kernel trick to capture **non-linear relationships** in data. Useful when the data is not linearly separable in its original space but can be transformed into a higher-dimensional space where it becomes linearly separable.

## Singular Value Decomposition (SVD)

Definition: A **matrix factorization techniqu** that transforms correlated variables into a set of uncorrelated variables.

Applications:
- Dimensionality Reduction
  
- Latent Semantic Analysis (LSA): Reduces dimensions of term-document matrices to uncover latent relationships in text.
  
- Image Compression: Retains only the largest singular values, significantly reducing storage while preserving image quality.
  
- Limitation: Computationally expensive for large matrices.

## Decision Trees
Decision Trees are a type of **supervised learning algorithm** used for both classification and regression tasks. 

They work by recursively **splitting the data into subsets based on feature values**, creating a tree-like structure where:

- **Node** represents a feature
- **Branch** represents a decision rule 
- **Leaf Node** represents an outcome (class label or predicted value).

Metrics for Best Split:

- **Gini Impurity**: 
    - Measures the **impurity of a node**.
    - Lower Gini impurity indicates a better split.
    - Works well for **classification tasks on categorical data**.
    - Does NOT work well with continuous data.
    - impurity means: a measure of how mixed the classes are in a node.

- **Cross Entropy** (Information Gain):
    - Measures the **amount of information gained** from splitting the data based on a feature.
    - Measures the difference between two probability distributions: the true distribution (actual labels) and the predicted distribution (model outputs).
    - Lower cross-entropy indicates a better fit between the predicted and true distributions.
    - Works well for **classification tasks**.
    - Can be used with both categorical and continuous data.

**Recursive Process:** Splits continue until stopping criteria are met.

**Stopping criteria** can include:
  - Maximum tree depth
  - Minimum number of samples per leaf node
  - Minimum impurity decrease
  
Advantages of Decision Trees:

- Easy to interpret
- No need for feature scaling or normalization (unlike KNN or SVM)
- Captures non-linear relationships 

Disadvantages of Decision Trees:

- Prone to overfitting (especially with deep trees)
- Sensitive to noise in the data
- Can be biased towards features with more levels (categorical variables with many unique values)


Hyperparameters of Decision Trees:

- **max_depth**: Maximum depth of the tree (controls overfitting).
- **min_samples_split**: Minimum number of samples required to split an internal node (controls overfitting).
- **min_samples_leaf**: Minimum number of samples required to be at a leaf node (controls overfitting).
- **max_features**: Maximum number of features to consider when looking for the best split (controls overfitting).
- **criterion**: The function to measure the quality of a split (e.g., "gini" for Gini impurity, "entropy" for information gain).
- **splitter**: The strategy used to choose the split at each node (e.g., "best" or "random").

## Ensemble Learning
Ensemble learning is a machine learning technique that **combines multiple models** to improve overall performance. It leverages the strengths of individual models while mitigating their weaknesses.

Work best when the predictors are as independent from each other. Diverse models correct each other’s errors.

Goal: increase robustness and generalization of predictions.

Types of Ensemble Learning:

**Bagging (Bootstrap Aggregating)**:

  - Training **multiple instances of the same model** on **different subsets** (using bootstrap sampling with replacement) of the data and combining their predictions to produce a result.
  - Reduces variance and helps prevent overfitting.
  - Example: Random Forests, which use decision trees as base learners.
  - For **regression**, the final prediction is the **average** of all predictions from individual models.
  - For **classification**, the final prediction is the **majority vote** from all predictions from individual models.

**Boosting**:

  - Training multiple instances of the same model **sequentially**, where each new model focuses on **correcting the errors made by the previous models**.
  - Misclassified points get more weight.
  - Final prediction is a weighted sum or vote.
  - Example: AdaBoost, Gradient Boosting, XGBoost, LightGBM.
  - Advantage: Reduces bias and improves accuracy.
  - Disadvantage: Risk of overfitting with too many iterations and computationally expensive due to sequential learning.

## Gradient Boosting
Gradient Boosting is a powerful **ensemble learning technique** that builds models **sequentially**, where each new model attempts to correct the errors made by the previous models. It is particularly effective for both regression and classification tasks.

It combines gardient descent with boosting to create a strong predictive model.

Steps:

1. Fit inital model
2. Compute residuals (errors) of the initial model
3. Fit a new model to the residuals
4. Update the initial model by adding the new model's predictions scaled by a learning rate
5. Repeat steps 2-4 for a specified number of iterations or until convergence.

**Gradient**: A vector that shows the **direction** and **steepness** of the **slope of a loss function** with respect to the model’s parameters (e.g. weights)
  -	We don’t manually compute gradients with libraries like Scikit-learn (it’s hidden under the hood)

**Learning Rate**: A hyperparameter that controls **how much the model is updated at each iteration.**
   - A small number (e.g., 0.1) that scales how big a step you take when updating the model based on the gradient. 
   - Too low → slow convergence
   - Too high → overshooting the optimal solution.

**XGBoost:** A highly optimized and efficient implementation of gradient boosting that performs well in competitions and real-world tasks.
   - Log Loss (Cross-Entropy Loss): Penalizes confident wrong predictions; often used in boosting algorithms like XGBoost.

## Random Forests
Random Forests is an **ensemble learning** method that combines **multiple decision trees** to improve predictive performance and reduce overfitting.

It is widely used for both classification and regression tasks.

It works by training multiple decision trees on different subsets **(bagging)** of the data and averaging their predictions (for regression) or taking a majority vote (for classification).

Process:

1. **Bootstrap Sampling**: Randomly sample the training data with replacement to create multiple subsets (bootstrapped datasets).
   
2. **Feature Randomness**: For each tree, randomly select a subset of features to consider for splitting at each node. This helps reduce correlation between trees and improves generalization.
   
3. **Train trees**: Train a decision tree on each bootstrapped dataset using the selected features.
   
4. **Aggregate Predictions**: For regression, average the predictions of all trees; for classification, take the majority vote.

Advantages of Random Forests:

- **Robustness**: Less prone to overfitting compared to individual decision trees due to averaging.
  
- **Feature Importance**: Provides insights into feature importance, helping identify which features contribute most to the model's predictions. 
  
- **Handles Missing Values**: Can handle missing values in the dataset without requiring imputation.
  
- **Scalability**: Can handle large datasets and high-dimensional feature spaces efficiently.

Disadvantages of Random Forests:

- **Interpretability**: Less interpretable than individual decision trees, making it harder to understand the model's decisions.
  
- **Computationally Intensive**: Training multiple trees can be computationally expensive, especially for large datasets.


# SVM
Support Vector Machines (SVM) is a powerful **supervised learning algorithm** used for both classification (primarily) and regression tasks.

It is particularly effective for high-dimensional data and can handle non-linear relationships using kernel functions(such as radial basis function (RBF))

SVM works by finding the **optimal hyperplane** that separates different classes in the feature space.

**Maximizes the margin** (distance between the boundary and the nearest data points from each class) to improve generalization.

Common Kernels:

1. **Linear** – for linearly separable data.
2. **Polynomial** – adds polynomial combinations of features.
3. **Radial Basis Function (RBF)** – default in scikit-learn; good for complex, non-linear data.

**Parameters:**
- **C**: Regularization parameter that **controls the trade-off** between maximizing the margin and minimizing classification errors.
  - Smaller C allows for a wider margin but may misclassify some points
  - Larger C focuses on classifying all points correctly but may lead to overfitting.
- **gamma**: Kernel coefficient for RBF kernel; controls the influence of individual training samples.
  - Small gamma means a large radius of influence
  - Large gamma means a small radius of influence.

Advantages of SVM:

- **Effective in high-dimensional** spaces(eg text data or images).
- **Scales well** to moderately large datasets.
- **Robust to overfitting**, especially in high-dimensional spaces.
- **Can handle non-linear relationships** using kernel functions.


Limitations of SVM:
- **Memory-intensive**: Requires more memory for large datasets.
- **Sensitive to feature scaling**: Requires normalization or standardization of features.
- **Choice of kernel and hyperparameters** can significantly affect performance.


## Outlier Detection

1. Graphical Methods:
   - **Box Plots**: Visualize the distribution of data and identify outliers as points outside the whiskers.
     - Outliers are typically defined as points that fall outside 1.5 times the interquartile range (IQR) from the first and third quartiles. Eg
     - Q1 - 1.5 * IQR
     - Q3 + 1.5 * IQR
   - **Scatter Plots**: Plot data points to visually identify outliers based on their position relative to other points.
   - **Histograms**: Show the distribution of data and highlight points that deviate significantly from the main distribution.

2. Clustering Methods:
   - **K-Means Clustering**: Identify outliers as points that are far from their assigned cluster centroids.
   - **DBSCAN**: Detects outliers as points that do not belong to any dense region (noise points).
   - **Hierarchical Clustering**: Identify outliers as points that do not form clusters or are isolated from other points.
   - These **use distance metrics** to determine how far points are from their clusters or neighbors.

3. Isolation Forest:
   - An **ensemble-based method** that isolates anomalies by randomly partitioning the data.
   - It builds an ensemble of isolation trees, where anomalies are expected to be isolated faster than normal points.
   - It is particularly effective for high-dimensional data and can handle large datasets efficiently.
   - does **not require** distance metrics or assumptions about the data distribution.

## Recommender Systems

1. **Content-Based Filtering**
   - Uses item features (e.g., genre, price) and user preferences
   - Independent of other users’ behavior
   - A supervised learning problem (predict rating $y_i$ from features $x_i$)

2. **Collaborative Filtering**
   - Recommends based on user-item interactions
   - Assumes similar users like similar items

   a. **User-Based CF**
      - Finds users with similar tastes
      - Recommends what similar users liked

   b. **Item-Based CF**
      - Finds similar items
      - Recommends based on a user’s past liked items

**Types of Collaborative Filtering:**
- Neighborhood-based: Use similarity metrics (e.g., cosine) to find neighbors
- Latent Factor models: Project users/items into latent space to reveal patterns, such as SVD.
- 

**Matrix Factorization (MF):** (such as SVD)
- Decomposes the user-item matrix into:
  - User matrix (preferences)
  - Item matrix (characteristics)
- Product of the matrices ≈ original matrix
- Fills in missing entries (predictions)

## Class Imbalance Handling

Class imbalance occurs when the number of instances in each class is not evenly distributed, leading to biased models that favor the majority class.

To handle class imbalance, several techniques can be employed:

#### SMOTE (Synthetic Minority Over-sampling Technique)

Generates synthetic samples for the minority class by interpolating between existing minority class samples.

SMOTE works by selecting a minority class sample and finding its k-nearest neighbors. 

It then creates synthetic samples by interpolating between the selected sample and its neighbors.

#### ADASYN (Adaptive Synthetic Sampling)

An extension of SMOTE that focuses on generating synthetic samples in **regions where the minority class is underrepresented.**

ADASYN adaptively generates more synthetic samples in regions where the minority class is sparse, helping to balance the class distribution.

Focuses model attention where it struggles most (class boundaries)

**Disadvantages:** Assumes space between minority samples belongs to the minority class—may not hold in non-linearly separable data


# Lecture 8

## Gradient Descent (GD)
GD is an optimization alorthim used to minimize the loss function by adjusting the model weights.

How it works:
1. **Initialize weights**: Start with random weights or zeros.
2. **Compute gradient**: Calculate the gradient of the loss function with respect to the weights. It computes the gradient (slope) of the loss with respect to all the data points.
3. **Update weights**: Adjust the weights in the opposite direction of the gradient by a small step size (learning rate).
4. **Repeat**: Repeat steps 2-3 until convergence (i.e., when the change in loss is below a threshold or a maximum number of iterations is reached).
5. **Convergence**: Stop when the loss function converges or reaches a predefined threshold.

Applicable to any differentiable loss function (e.g., in linear regression, logistic regression, neural networks).

The **learning rate** (a hyperparameter between 0 and 1) determines how large each step is. It controls how quickly the model learns.

## SGD
Is a variant of GD that updates the weights using only a **single data point** (or a small batch) randomly chosen  at each iteration instead of the entire dataset.

**Faster convergence**: Can converge faster than GD, especially for large datasets.

**Noisy updates**: The updates can be noisy and may lead to oscillations (less stable) in the loss function.

**Sensitive to learning rate**: Requires careful tuning of the learning rate to avoid divergence.

#### Mini-Batch SGD
Is a compromise between GD and SGD, where the weights are updated using a **small batch of data points** instead of the entire dataset or a single data point. eg. 32 or 64 samples at a time.

This approach combines the benefits of both GD and SGD, providing a balance between convergence speed and stability.

## Regularization (L0 / L1 / L2)

Regularization adds a **penalty term to the loss function** to discourage overly complex models and prevent overfitting.

#### L0 Regularization

**Penalizes the number of non-zero weights** in the model. 

It **encourages sparsity** in the model by minimizing the number of features used.

Not differentiable—**rarely used** in practice.

#### L1 Regularization (Lasso)

L1 regularization adds a penalty equal to the **absolute value of the weights** to the loss function.

It encourages sparsity in the model by driving some weights to zero, effectively selecting a simpler model.

Solution is **not unique**, and it can lead to multiple solutions with different weights.

Good when you suspect that only a few features are important for the model.

#### L2 Regularization (Ridge)

L2 regularization adds a penalty equal to the **square of the weights** to the loss function.

It discourages large weights and helps prevent overfitting by **shrinking the weights towards zero**

Useful when **all features matter**, but you want to avoid overfitting (e.g., multicollinearity).

Solution is **unique** and leads to a smoother model and data should be **normalized/scaled before** applying L2 regularization.

## Early Stopping

A training strategy that stops training when validation performance stops improving.
Prevents overfitting by halting before the model starts learning noise.

``` python
from keras.callbacks import EarlyStopping
early_stop = EarlyStopping(
  monitor='val_loss', # Monitor validation loss
  patience=5, # Stops training after 5 epochs of no improvement
  restore_best_weights=True # roll back to the best weights
)

# Fit the model with early stopping
model.fit(
  X_train,
  y_train,
  epochs=50, 
  validation_split=0.2, # 20% of data for validation
  batch_size=32, # Batch size for training
  shuffle=True, # Shuffle the training data
  verbose=1, # Print training progress
  callbacks=[early_stop]
)
```


## Implementation Steps

1. **Monitor Validation Performance**: Track the model's performance on a validation set during training.
2. **Define Patience**: Set a patience parameter that determines how many epochs to wait for an improvement before stopping.
3. **Stop Training**: If no improvement is observed after the defined patience, stop training and restore the best model weights.

## Dropout

A regularization technique for neural networks to prevent overfitting.

During training, randomly (drops) **set a fraction of the input units to zero** at each update, which helps prevent the network from becoming too dependent on specific neurons.

This forces the network to learn more robust features that are not reliant on any single neuron.

When to adjust:
  - Overfitting: Increase dropout.
  - Underfitting: Decrease dropout.

Dropout slows convergence but often leads to **better generalization**. It complements early stopping.

## Validation Loss
Validation loss is the loss calculated on a separate validation dataset that is not used for training.

It is used to monitor the model's performance during training and helps in detecting overfitting.

When validation loss increases while training loss decreases → overfitting.

Validation loss serves as a **proxy for test performance** since the test set should remain untouched during training.

## BatchNorm

Batch Normalization (BatchNorm) is a technique used to **normalize the inputs of each layer** in a neural network.

Normalizes the inputs to a layer across a mini-batch (mean = 0, variance = 1), then applies a learned scaling and shift.

Helps stabilize and speed up training by **reducing internal covariate shift** (the change in distribution of inputs to layers during training).

Often used after linear/convolutional layers and before activation functions. It can be applied to both fully connected and convolutional layers.

If BN is the first layer, no need to manually standardize input data.

Works best in CNNs and feedforward networks. Not ideal for RNNs — use alternatives like Gradient Clipping instead.

Benefits:

- **Faster convergence**: Reduces the number of training epochs required.
- **Improved generalization**: Helps prevent overfitting by acting as a form of regularization.
- **Higher learning rates**: Allows for larger learning rates, which can lead to faster training.
- **Reduced sensitivity to initialization**: Makes the model less sensitive to weight initialization.
- **Helps prevent vanishing/exploding gradients**: Keeps the gradients in a reasonable range, especially in deep networks.


If you add a BN layer as the very first layer of your NN model, it will normalize the input data before passing it to the first hidden layer, you do not need to standardize the input data before training, as the BN layer will take care of that. However, if you add a BN layer after the first hidden layer, you should standardize the input data before training, as the BN layer will not normalize the input data before passing it to the first hidden layer.

## Vanishing Gradients Problem

Occurs when gradients become **too small during backpropagation** through deep neural networks, leading to slow or stalled learning.

Early layers learn very slowly due to tiny gradients.

This problem is particularly prevalent in deep networks with activation functions like **sigmoid or tanh**, which **squash the input into a small range**, causing **gradients to diminish** as they propagate back through the layers.

Solutions include:

- **BatchNorm:** Stabilizes input distributions.
- **ReLU activation:** Doesn’t squash gradients.
- **Gradient Clipping:** Prevents exploding gradients by capping the gradients during backpropagation.

## Backpropagation

Backpropagation is the algorithm used to **compute the gradients of the loss function with respect to the model parameters** (weights and biases) in a neural network.

It is a key component of training neural networks using gradient descent (or its variants like SGD).

**Goal:** Minimize the loss function by adjusting the model parameters (weights and biases) based on the computed gradients, to reduce the error between the predicted output and the actual target values.


The backpropagation algorithm consists of two main steps:

1. **Forward Pass**: Compute the output of the network by passing the input through each layer, applying the activation functions, and calculating the loss.
   - You pass input data through the neural network.
	 - Each layer processes the data using weights and activation functions.
	 - At the end, you get a prediction and you compute the loss (error) by comparing the prediction to the actual target values, using a loss function (e.g., mean squared error for regression, cross-entropy for classification).
   - The forward pass computes the output of the network and the loss function.
   - The loss function quantifies how well the model is performing by measuring the difference between the predicted output and the actual target values.
  

2. **Backward Pass**: Compute the gradients of the loss with respect to each parameter (weight and bias) by applying the chain rule of calculus.
   - Start at the output layer and propagate the gradients backward through the network.
   - For each layer, compute the gradient of the loss with respect to the weights and biases using the chain rule.
   - Update the weights and biases using the computed gradients and a learning rate (the learning rate controls how big of a step you take in the direction of the gradient).
   - The backward pass computes the gradients needed for the optimization step.
   - The gradients are used to update the model parameters (weights and biases) in the **direction that minimizes the loss function.**


# Lecture 9

## Notation for Neural Networks

- **Observed Data**:
  - The input data matrix $X \in \mathbb{R}^{n \times d}$, where $n$ is the number of samples (rows) and $d$ is the number of features (columns). 
  - The output label vector for supervised learning $y \in \mathbb{R}^{n \times c}$, where $c$ is the number of classes.
  - The output label vector for unsupervised learning $y \in \mathbb{R}^{n}$, where $n$ is the number of samples.
  - Each y in the output label vector corresponds to a sample in the input data matrix $X$.

- **Latent Feature**:
  - The latent feature matrix $Z \in \mathbb{R}^{n \times k}$, where $k$ is the number of latent features. 
  - The latent feature matrix is used to represent the input data in a lower-dimensional space. They are hidden variables that are not directly observed but are inferred from the observed data.

- **Model Parameters**:
  - The model parameters are represented by $\theta$, which can include weights, biases, and other hyperparameters. 
  - The model parameters are learned from the training data using optimization algorithms such as gradient descent.
  - The model weights are represented by $W \in \mathbb{R}^{d \times k}$, where $d$ is the number of features and $k$ is the number of latent features. 
  - The model weights are used to transform the input data into the latent feature space.
  - V is the weight matrix for the output layer, which maps the latent features to the output labels, it is represented by $V \in \mathbb{R}^{k \times c}$, where $c$ is the number of classes. Why is it a one for one mapping? Because the output labels are one-hot encoded, meaning that each sample belongs to only one class.

#### Supervised Learning Process

- **Standard supervised learning process:**
  - Learn parameters 'w' based on original features $x_i$ and target labels $y_i$.
- **Change of Basis:**
  - Learn parameters 'w' based on change of basis features $z_i$ and target labels $y_i$. So instead of learning the parameters based on the original features, we learn the parameters based on the latent features, $z_i$, which we represent as v, the latent feature matrix.
- **Latent Features:**
  - The latent features are learned from the original features using a transformation weight matrix $W$, so that $z_i = W^T x_i$.
  - W maps the original input features to the latent feature space (a change of basis).
  - The transformation weight matrix $W$ is learned from the training data using optimization algorithms such as gradient descent. The optimization process aims to minimize the loss function, ensuring that the predicted labels align closely with the actual labels.
  - Learn W based on original features $x_i$ (unsupervised) and target labels $y_i$ (supervised).
- **Neural Networks:**
  - Join the two processes together to learn the parameters 'W' and  'v' simultaneously.
  - $W$ is the transformation weight matrix that maps the original input features to the latent feature space
  - $v$ is the weight matrix for the output layer that maps the latent features to the output labels.

#### Linearity and Non-Linearity
- **Linear latent-factor model with linear regression:**
  - $y_i = W^T x_i + \epsilon_i$, where $\epsilon_i$ is the error term.
  - The model is linear in both the input features and the latent features.
  - Use features from latent-factor model: $z_i = W x_i$.
  - Make predictions using linear model: $y_i = v^T z_i + \epsilon_i$. $v^T$ is the weight matrix for the output layer that maps the latent features to the output labels. It is transposed as it is a row vector and $z_i$ is a column vector, which means that the two can be multiplied together using dot product.

- **Linear latent-factor model with logistic regression:**
  - $y_i = \sigma(W^T x_i + \epsilon_i)$, where $\sigma$ is the sigmoid function and $\epsilon_i$ is the error term.
  - The model is linear in both the input features and the latent features.
  - Use features from latent-factor model: $z_i = W x_i$.
  - Make predictions using logistic model: $y_i = \sigma(v^T z_i + \epsilon_i)$. $v^T$ is the weight matrix for the output layer that maps the latent features to the output labels. It is transposed as it is a row vector and $z_i$ is a column vector, which means that the two can be multiplied together using dot product.
  
- **Non-linear latent-factor model:**
  -  Transform $z_i$ using a **non-linear activation function $h$** to get $h(z_i)$.
  -  Common choicce of non-linear function is the **sigmoid function**, **tanh function**, or **ReLU function**. And it is applied element-wise to the latent features.
  -  The resulting equation is:
     - $y_i = h(W^T x_i) + \epsilon_i$, where $\epsilon_i$ is the error term.
  -  The model is non-linear in both the input features and the latent features.
  -  Use features from latent-factor model: $z_i = W x_i$. 

- **Training a neural network:**
  - Apply stochastic gradient descent (SGD) for optimization, to compute gradient of the loss function with respect to the model parameters (W, the weights for feature learning and v, the weights for output layer (prediction)).  
  - The loss function is typically the mean squared error (MSE) for regression tasks or cross-entropy loss for classification tasks.
  - The optimization process aims to minimize the loss function, ensuring that the predicted labels align closely with the actual labels.
  - It updates them iteratively based on the gradients computed from the loss function, to minimize the error between the predicted and actual labels. 

- **Neural network with 3 hidden layers:**
  - $y_i = h_3(h_2(h_1(W^T x_i))) + \epsilon_i$, where $\epsilon_i$ is the error term.
  - The model is non-linear in both the input features and the latent features.
  - Use features from latent-factor model: $z_i = W x_i$. 
  - The resulting equation is:
     - $y_i = h_3(h_2(h_1(z_i))) + \epsilon_i$, where $\epsilon_i$ is the error term.
  - For 'm' hidden layers, the equation is:
     - $y_i = h_m(h_{m-1}(h_{m-2}(...h_1(z_i)...))) + \epsilon_i$, where $\epsilon_i$ is the error term.
     - It shows that the network is a composition of multiple non-linear functions, which allows it to learn complex relationships between the input features and the output labels. It is essentially a function composed of many sub-functions, each of which is a non-linear transformation of the previous one.

## Activation Functions
Activation functions define how the **weighted sum of inputs** is **transformed** into the **output** of a neuron. They introduce non-linearity, which enables neural networks to model complex patterns.

All layers use the same activation function, for simplicity and stability across the network.

Output layer uses a different activation function from the hidden layers. For example:

  - Sigmoid function for binary classification
  - **Softmax** function for **multi-class** classification
  - Linear function for regression tasks

**Universal Approximation Theorem**:
A deep neural network with non-linear activation functions can approximate any continuous function on compact subsets of $\mathbb{R}^n$, given sufficient neurons.

#### Common Activation Functions

| Function      | Output Range        |  Properties |
|-----------|----------|--------------------|
| Sigmoid     | (0, 1)           | S-shaped curve, smooth gradient. Good for binary classification. Can cause vanishing gradient problem |
| Tanh        | (-1, 1)          | Scaled version of sigmoid, zero-centered. Helps with convergence. Can still suffer from vanishing gradient |
| ReLU        | [0, ∞)           | Outputs input if positive, else 0. Computationally efficient. Helps avoid vanishing gradients |
| Softmax    | (0, 1)           | Converts logits to probability distribution. Used in multi-class classification. Sensitive to outliers |


## Bias
Bias is the error from incorrect model assumptions.

Bias is one of the three main sources of error in a model:

1. 	Bias – due to overly simplistic assumptions.
2. 	Variance – due to sensitivity to small data fluctuations.
3. 	Irreducible error – due to noise in the data itself.

- **High Bias** = model too simple, leading to **underfitting** of the training data.
  - eg fitting a straight line to data thats not linear.
  - leads to high training error and high test error.
- **Low Bias** = model too complex, leading to **overfitting** of the training data.
  - eg fitting a complex curve to data that is linear.
  - leads to low training error but high test error.

**Bias-Variance Tradeoff**: 
Goal: Find a balance between underfitting (bias) and overfitting (variance)

Best model: Low bias + low variance, generalizes well to unseen data.

**Bias Term in Neural Nets**

A bias term is usually a constant value (often 1) added to the weighted sum.

It **allows the activation function to shift left/right**, helping the model learn better patterns.

Without bias, the model is constrained to pass through the origin.

## TLUs
Threshold Logic Unit (TLU) is a simple model of a neuron that:

  - Computes a **weighted sum of its inputs**
  - Applies a threshold function to produce a binary output. The TLU can be represented mathematically as: $y = \begin{cases} 1 & \text{if } w^T x + b > 0 \\ 0 & \text{otherwise} \end{cases}$
    - $w$ is the weight vector
    - $x$ is the input vector
    - $b$ is the bias term.

TLUs form the basis of perceptrons and MLPs.

It is like a light switch, where the input is the current and the output is the light. The TLU turns on (outputs 1) when the weighted sum of inputs exceeds a certain threshold (the current is above a certain level), and turns off (outputs 0) when the weighted sum of inputs is below the threshold (the current is below a certain level).

Issues with TLUs:

- Cannot learn non-linear functions (e.g., XOR problem) 
- TLUs are not differentiable at the threshold, which makes it hard to apply gradient descent for training.
- TLUs are limited to binary outputs (0 or 1), which restricts their applicability to binary classification tasks.
- TLUs do not have a bias term, which limits their ability to fit the data well.
- No hidden layers, which limits their ability to learn complex patterns in the data.

## Perceptron
Simplest ANN (Artificial Neural Network) model, consisting of a **single layer of TLUs.**

Linear classifier that learns weights and bias using the Perceptron learning rule.

Uses the Heaviside step function as its activation:
- output is 1 if weighted sunm of inputs > threshold, else 0.

A bias input (1) is typically added to allow shifting the activation threshold.

**Training the Perceptron:**
1. Input one training sample at a time.
2. Compute the predicted output using the current weights and bias.
3. Compare the predicted output with the actual output.
4. Update the weights and bias based on the error.
5. Repeat steps 1-4 for a fixed number of epochs or until convergence.

**Limitations:**
Can only learn linearly separable functions, meaning that it can only classify input data that can be separated by a straight line (or hyperplane) in the feature space. 

**Cannot learn non-linear functions**, such as XOR, which requires a more complex model with multiple layers of TLUs or non-linear activation functions.

**Sensitive** to the choice of **learning rate** and the **initial weights**, which can affect the convergence of the algorithm and the final model performance.

Perceptrons do not output a class probability, but rather a binary classification (0 or 1). This means that they are **not directly suitable for multi-class classification.**


## Multi-Layer Perceptron (MLP)

A MLP is a type of artificial neural network (ANN) with:
   - **Input layer:** Receives input features.
   - **One or more hidden layers:** Each layer consists of multiple neurons (or TLUs) that apply non-linear activation functions to learn complex patterns.
   - **Output layer:** Produces the final output, which can be a single value (for regression) or a probability distribution over classes (for classification).

Each neuron in a layer is connected to **all neurons in the previous layer**, and applies:
 - a weighted sum of inputs
 - non-linear activation function (e.g., ReLU, sigmoid, tanh) to introduce non-linearity.

Every input/hidden layer includes a **bias neuron with a constant value of 1**, which allows the model to fit the data better by s**hifting the activation function to the left or right**, enabling it to learn more complex relationships between the input features and the output labels.


## Feedforward Neural Network

A feedforward neural network is a type of MLP where:

   - The **information flows in one direction**, from the input layer to the output layer, without any feedback loops.

Each neuron in a layer receives inputs from the previous layer, applies a weighted sum and an activation function, and passes the output to the next layer. The feedforward neural network is trained using supervised learning algorithms, such as backpropagation, to learn the optimal weights and biases that minimize the classification error.

# Lecture 10

## RNNs (Recurrent Neural Networks)

RNNs are a type of neural network designed to **process sequential data**, such as time series or natural language. 

They have a feedback loop that allows them to **maintain a hidden state**, which captures information from previous time steps.

This hidden state acts as **memory**, enabling the model to capture **temporal dependencies** across time steps.

**Hidden state:** updated at every time step using both the current input and the previous hidden state.

**Outputs:** Typically passed through a fully connected (dense) layer and then a softmax function (for classification tasks).

#### RNN Training: BPTT

Standard backpropagation algorithm is not suitable for training RNNs because in RNNs, the output at each time step depends on the hidden state from the previous time step, creating a temporal dependency.

Backpropagation Through Time (BPTT) is an adaptation of the standard backpropagation algorithm tailored for training RNNs.

How it works:
1. The RNN is unrolled over time — each time step is treated like a separate layer in a deep feedforward network.
2. Forward pass: Compute activations across all time steps.
3. Backward pass: Compute gradients for each time step, moving from the last time step back to the first.
4. Gradient accumulation: Gradients from all time steps are summed and used to update the shared weights (since weights are reused at each time step).


**Limitations**:

  - **Computationally expensive**
  - **Vanishing/exploding gradients:** Gradients may shrink or explode as they are propagated backward
  - **Memory-intensive:** All hidden states and intermediate values must be stored during the forward pass to compute gradients during the backward pass.

Solutions to these limitations include:

**LSTM (Long Short-Term Memory)** networks and **GRU (Gated Recurrent Unit)** networks, which are specialized RNN architectures designed to mitigate the vanishing gradient problem and capture long-term dependencies more effectively. These models use **gating mechanisms** to control the flow of information and are more stable when training on long sequences.



## DL Frameworks
Deep learning frameworks are software libraries that simplify the development, training, and deployment of deep neural networks, including architectures like CNNs,RNNs and MLPs.

They provide high-level APIs, pre-built layers, automatic differentiation for gradient computation, and hardware acceleration (e.g., GPU support) to streamline tasks like model design, optimization, and evaluation.

Some popular deep learning frameworks include:

  - **TensorFlow**: Developed by Google, TensorFlow is an open-source deep learning framework that provides a flexible and comprehensive ecosystem for building and deploying machine learning models. It supports both high-level APIs (like Keras) and low-level operations for custom model development.
  - **PyTorch**: Developed by Facebook's AI Research lab, PyTorch is an open-source deep learning framework that emphasizes dynamic computation graphs, making it easy to build and modify neural networks on-the-fly. It is widely used in research and production environments.
  - **Keras**: Keras is a high-level neural networks API written in Python that runs on top of TensorFlow. It provides a user-friendly interface for building and training deep learning models, making it accessible to beginners and experienced practitioners alike.
  - **MXNet**: An open-source deep learning framework developed by Apache, MXNet is designed for efficiency and flexibility. It supports both symbolic and imperative programming, allowing users to choose the best approach for their specific use case.
  - **Caffe**: Developed by the Berkeley Vision and Learning Center (BVLC), Caffe is a deep learning framework focused on speed and modularity. It is particularly well-suited for image classification tasks and has a strong community support.

#### Non-Distributed vs. Distributed ML

**Non-Distributed ML**: Non-distributed ML involves training a machine learning model on a single machine (e.g., a laptop, server, or GPU) using its local computational resources (CPU/GPU, memory, storage). The entire dataset and model computations, including forward passes, gradient calculations, and weight updates, occur on this single machine. Common frameworks like Scikit-learn, TensorFlow, or PyTorch can be used in non-distributed settings for small to medium-sized datasets and models, such as linear regression, small neural networks, or tree-based models.

**Distributed ML**: Distributed ML involves training a machine learning model across multiple machines or nodes, leveraging their combined computational resources to handle larger datasets and more complex models. This approach is essential for scaling up training processes, especially in deep learning, where models can be very large and require significant computational power. Distributed ML frameworks like TensorFlow and PyTorch provide tools for parallelizing computations, synchronizing gradients, and managing data across multiple devices or clusters. Distributed ML can be implemented using data parallelism (splitting the dataset across machines) or model parallelism (splitting the model across machines).

#### TensorFlow
TensorFlow is an open-source deep learning framework developed by Google that provides a flexible and comprehensive ecosystem for building and deploying machine learning models. It supports both high-level APIs (like Keras) and low-level operations for custom model development.

TensorFlow is designed to be efficient and scalable, allowing users to train models on a single machine or across multiple devices (CPUs, GPUs, TPUs) and even in distributed environments. It provides tools for automatic differentiation, optimization, and model evaluation, making it suitable for a wide range of applications, including computer vision, natural language processing, and reinforcement learning.

Its core is very similar to NumPy, but it uses tensors (multi-dimensional arrays) instead of arrays. Tensors are the fundamental data structure in TensorFlow and can be manipulated using various operations, similar to NumPy arrays.

Core features include:

  - **Tensors**: Multi-dimensional arrays that represent data in TensorFlow. Tensors can have different ranks (dimensions) and shapes, allowing for flexible representation of data.
  - **Graphs**: TensorFlow uses a computational graph to represent the flow of data and operations. Nodes in the graph represent operations (e.g., addition, multiplication), while edges represent the tensors flowing between them. This allows for efficient execution and optimization of computations.
  - **Sessions**: In TensorFlow 1.x, a session is used to execute the computational graph. In TensorFlow 2.x, eager execution is enabled by default, allowing for immediate evaluation of operations without the need for sessions.
  - **Keras API**: TensorFlow includes a high-level Keras API for building and training deep learning models. Keras provides a user-friendly interface for defining layers, models, and training processes, making it accessible to beginners and experienced practitioners alike.

Python example:
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model using Keras Sequential API
model = Sequential([
    Dense(units=10, activation='relu', input_shape=(20,)),  # Hidden layer: ReLU activation
    Dense(units=1, activation='sigmoid')                    # Output layer: Sigmoid for binary classification
])

# Compile the model
model.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=['accuracy'])

# Notation:
# - X: Input features, X ∈ ℝ^{n × d} (n=1000 samples, d=20 features)
# - y: Labels, y ∈ {0, 1}^n
# - W_1: Weight matrix for hidden layer, W_1 ∈ ℝ^{10 × 20}
# - b_1: Bias for hidden layer, b_1 ∈ ℝ^{10}
# - W_2: Weight matrix for output layer, W_2 ∈ ℝ^{1 × 10}
# - b_2: Bias for output layer, b_2 ∈ ℝ^{1}
# - z_1 = W_1 X + b_1: Pre-activation for hidden layer
# - a_1 = ReLU(z_1): Hidden layer activation
# - z_2 = W_2 a_1 + b_2: Pre-activation for output layer
# - ŷ = sigmoid(z_2): Predicted probability, ŷ ∈ [0, 1]
# - L: Binary cross-entropy loss, L = -1/n ∑ [y_i log(ŷ_i) + (1-y_i) log(1-ŷ_i)]

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)

# Evaluate on test set
y_pred = (model.predict(X_test) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

# Model summary
model.summary()
```

# Lecture 11

## LTSM
LSTM (Long Short-Term Memory) networks are a type of recurrent neural network (RNN) designed to **handle long-term dependencies** better than traditional RNNs, particularly the **vanishing gradient problem.**

They achieve this through a unique architecture that includes memory cells and gating mechanisms, allowing them to maintain and control information over long sequences.

Components of LSTM:

  - **Cell State**: The memory of the LSTM cell, which carries information across time steps.
  - **Forget Gate**: Determines what information to discard from the cell state.
  - **Input Gate**: Decides what new information to add to the cell state (c)
  - **Output Gate**: Controls what information to output from the cell state to the next layer.

**Peephole Connections (LSTM Variant)**: 

Peephole connections are a variant of LSTM architecture that allows the **gates to access the cell state directly.**

This enables the gates to make more informed decisions about what information to keep or discard, improving the model's performance on tasks with long-term dependencies.

In code, `keras.layers.LSTM` and `keras.layers.LSTMCell` are used to build these models. The LSTMCell provides more control, while LSTM is simpler to use for stacked layers.
  - `keras.layers.LSTM`: A high-level layer that combines the LSTM cell and recurrent logic.
  - `keras.layers.LSTMCell`: A low-level cell that can be used to build custom RNN architectures.

Using `LSTM layer` (simpler and more common):
```python
model = keras.models.Sequential(
    [
        keras.layers.LSTM(20, return_sequences=True, input_shape=(None, 1)),
        keras.layers.LSTM(20, return_sequences=True),
        keras.layers.TimeDistributed(keras.layers.Dense(10)),
    ]
)
```
Explanation of code above:

- `keras.models.Sequential`: A linear stack of layers to build the model.
- `keras.layers.LSTM(20, return_sequences=True, input_shape=(None, 1))`: Adds an LSTM layer with 20 units (neurons). `return_sequences=True` means it returns the full sequence of outputs (hidden states, $h_t$) for each time step, and `input_shape=(None, 1)` specifies the input shape (None for variable time steps and 1 feature per time step (e.g., univariate time series).
- `keras.layers.LSTM(20, return_sequences=True)`: Adds another LSTM layer with 20 units, also returning the full sequence of outputs.
- `keras.layers.TimeDistributed(keras.layers.Dense(10))`: Wraps a Dense layer (fully connected layer) to apply it to each time step independently. The Dense layer has 10 units, meaning it outputs a vector of size 10 for each time step.
- The model can be trained using the same methods as other Keras models, such as `model.compile()`, `model.fit()`, and `model.evaluate()`.

Using `LSTM Celllayer`
```python
model = keras.models.Sequential(
    [
        keras.layers.LSTMCell(20, input_shape=(None, 1)),
        keras.layers.LSTMCell(20),
        keras.layers.TimeDistributed(keras.layers.Dense(10)),
    ]
)
```

## GRU & Bi-LSTM
**GRU (Gated Recurrent Unit)** is a simplified version of LSTM that **combines the forget and input gates into a single update gate.** Has no output gate and merges the cell state and hidden state into a single state vector.

It has fewer parameters than LSTM, making it **computationally more efficient** while still effectively capturing long-term dependencies in sequential data.
  
Implemented in `keras.layers.GRU` and `keras.layers.GRUCell`.
  
**Bi-LSTM (Bidirectional LSTM)** is an extension of LSTM that processes the input sequence in **both forward and backward directions.** This allows the model to capture context from both past and future time steps, improving performance on tasks **where the entire sequence context is important**, such as **natural language processing** and speech recognition.

**Comparison Table**:

| Feature              | LSTM                          | GRU                          | Bi-LSTM                      |
|---------------|-------------------------------|------------------------------|------------------------------|
| Architecture  | Three gates (input, output, forget) | Two gates (Update, reset)  | Two LSTMs layers (forward and backward) |
| Advantages     | Better for long-term dependencies and complex sequences | Faster and less memory-intensive | Captures context from both directions |
| Disadvantages | Slower and more memory-intensive | May not capture long-term dependencies as well | More complex and requires more resources |

# Lecture 12

## Autoencoders
Autoencoders are a type of neural network used for **unsupervised learning**, primarily for dimensionality reduction, feature extraction, and data denoising.
  
Autoencoders can be thought of as a way to **compress and then decompress data**, similar to how a zip file reduces the size of files for storage and later restores them to their original form.

**Why Use Autoencoders?**

1. **Dimensionality Reduction**: Autoencoders can learn a lower-dimensional representation of the input data, which can be useful for visualization or as a preprocessing step before applying other machine learning algorithms.
   
2. **Anomaly Detection**: By training an autoencoder on normal data, it can learn to reconstruct that data well. When presented with anomalous data, the reconstruction error will be higher, allowing for anomaly detection.
   
3. **Generative Models**: Autoencoders can be used to generate new data samples by sampling from the learned latent space and decoding it back to the original space.
   
4. **Extracting Features**: Autoencoders can learn useful features from the input data, which can be used for downstream tasks like classification or clustering.

#### Architecture

An autoencoder consists of two main components:

1. **Encoder**: The encoder **compresses the input data** into a lower-dimensional representation (latent space). It consists of one or more layers that reduce the dimensionality of the input data.
   
2. **Decoder**: The decoder **reconstructs the original input** data from the lower-dimensional representation. It consists of one or more layers that increase the dimensionality of the latent space back to the original input size.

The encoder and decoder are typically symmetric, meaning that the architecture of the encoder mirrors that of the decoder.

Autoencoders have the same architecture as MLP, but the **output layer has the same number of neurons as the input layer**, and the activation function is usually linear (or identity) to allow for reconstruction of the original input data.

#### Undercomplete Autoencoder

An undercomplete autoencoder is a type of autoencoder where the dimensionality of the latent space is smaller than the input space. This forces the model to learn a compressed representation of the input data, which can help in feature extraction and dimensionality reduction.

PCA Analogy: If only the first few principal components are used to represent the data, the PCA model can be thought of as an undercomplete autoencoder. The PCA model learns a linear transformation that projects the data onto a lower-dimensional subspace, similar to how an undercomplete autoencoder learns a compressed representation of the input data.

```python
import numpy as np
from keras.layers import Input, Dense
from keras.models import Model

input_img = Input(shape=(784,))  # Input shape (e.g., for MNIST images)
encoded = layers.Dense(32, activation='relu')(input_img)  # Encoder layer
decoded = layers.Dense(784, activation='sigmoid')(encoded)  # Decoder layer
autoencoder = Model(input_img, decoded)  # Autoencoder model
autoencoder.compile(optimizer='adam', loss='mse')  # Compile the model

# Train the autoencoder
X_train = np.random.rand(1000, 784)  # Example training data

autoencoder.fit(X_train, X_train, epochs=10, batch_size=256, shuffle=True, validation_split=0.2)  # Train the model
```

In this example, the autoencoder is trained to reconstruct the input data (X_train) by minimizing the mean squared error (MSE) between the input and output. The encoder layer reduces the dimensionality to 32, while the decoder layer reconstructs it back to 784 dimensions.
  
The model is compiled using the Adam optimizer and trained for 10 epochs with a batch size of 256. The training data is split into 80% for training and 20% for validation.
  
The autoencoder learns to compress the input data into a lower-dimensional representation and then reconstruct it back to the original input space.

#### Stacked Autoencoders

Stacked autoencoders (or Deep autoencoders) are a type of autoencoder architecture where multiple layers of encoders and decoders are stacked on top of each other. Each layer learns a compressed representation of the input data, allowing for deeper feature extraction and more complex representations.

**Why Stacked layers?**

By stacking multiple layers, the model can learn hierarchical representations of the data, where each layer captures different levels of abstraction. This is similar to how deep neural networks learn features at different levels of granularity.

Python example:
```python
input_img = keras.Input(shape=(784,))  # Input shape (e.g., for MNIST images)
encoded1 = layers.Dense(128, activation='relu')(input_img)  # First encoder layer
encoded2 = layers.Dense(64, activation='relu')(encoded1)  # Second encoder layer
decoded1 = layers.Dense(128, activation='relu')(encoded2)  # First decoder layer
output_img = layers.Dense(784, activation='sigmoid')(decoded1)  # Output layer

autoencoder = Model(input_img, output_img)  # Autoencoder model
autoencoder.compile(optimizer='adam', loss='mse')  # Compile the model

# Train the autoencoder
X_train = np.random.rand(1000, 784)  # Example training data
autoencoder.fit(X_train, X_train, epochs=10, batch_size=256, shuffle=True, validation_split=0.2)  # Train the model
```
In this example, the autoencoder consists of two encoder layers (128 and 64 units) and two decoder layers (128 and 784 units). The model is trained to reconstruct the input data (X_train) by minimizing the mean squared error (MSE) between the input and output. The training process is similar to that of a single-layer autoencoder.
  
The stacked autoencoder learns to compress the input data into a lower-dimensional representation (64 units) and then reconstruct it back to the original input space (784 dimensions) through multiple layers.

#### Convolutional Autoencoders

Designed for image data, where the input is typically a 2D image (height x width x channels). They are particularly effective for tasks like image denoising, inpainting, and **unsupervised feature learning.**

Uses convolutional layers instead of fully connected layers in the encoder and decoder. They are particularly effective for image data, as they can learn spatial hierarchies and local patterns in the input data.

**Architecture**:

**Encoder:** Consists of convolutional layers that downsample the input image, reducing its spatial dimensions (height & width) while increasing the number of feature maps (depth). Consists of convolutional layers followed by pooling layers (e.g., max pooling) to reduce the spatial dimensions of the input image.

**Decoder:** Consists of **transposed convolutional layers** (also known as deconvolutional layers) that upsample the encoded representation back to the original image size. The decoder uses transposed convolutional layers (also known as deconvolutional layers) to upsample the encoded representation back to the original image size.
  
The output layer typically uses a sigmoid activation function to produce pixel values in the range [0, 1].

```python
import numpy as np
from tensorflow import keras
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, Flatten, Dense, Reshape
from keras.models import Model
from keras.datasets import mnist
import matplotlib.pyplot as plt

# Load MNIST dataset
(x_train, _), (x_test, _) = mnist.load_data()
x_train = np.expand_dims(x_train, axis=-1)  # Add channel dimension
x_test = np.expand_dims(x_test, axis=-1)  # Add channel dimension
x_train = x_train.astype("float32") / 255.0  # Normalize pixel values
x_test = x_test.astype("float32") / 255.0  # Normalize pixel values
input_shape = (28, 28, 1)  # Input shape for MNIST images (height, width, channels)
latent_dim = 64  # Dimensionality of the latent space

# Encoder
input_img = Input(shape=input_shape)  # Input layer
x = Conv2D(32, (3, 3), activation="relu", padding="same")(input_img)  # Convolutional layer # 28x28x32
x = MaxPooling2D((2, 2), padding="same")(x)  # Max pooling layer # 14x14x32
x = Conv2D(64, (3, 3), activation="relu", padding="same")(x)  # Convolutional layer # 14x14x64
x = MaxPooling2D((2, 2), padding="same")(x)  # Max pooling layer # 7x7x64
x = Conv2D(128, (3, 3), activation="relu", padding="same")(x)  # Convolutional layer # 7x7x128
x = MaxPooling2D((2, 2), padding="same")(x)  # Max pooling layer # 4x4x128
x = Flatten()(x)  # Flatten the feature maps # 4*4*128=2048
encoded = Dense(latent_dim, activation="relu")(x)  # Latent space representation # 64

# Decoder
x = Dense(np.prod((4, 4, 128)), activation="relu")(encoded)  # Fully connected layer
x = Reshape((4, 4, 128))(x)  # Reshape to feature maps # 4x4x128
x = UpSampling2D((2, 2))(x)  # Upsampling layer # 8x8x128 
x = Conv2DTranspose(64, (3, 3), activation="relu", padding="same")(x)  # Transposed convolutional layer # 8x8x64
x = UpSampling2D((2, 2))(x)  # Upsampling layer # 16x16x64
x = Conv2DTranspose(32, (3, 3), activation="relu", padding="same")(x)  # Transposed convolutional layer # 16x16x32
x = UpSampling2D((2, 2))(x)  # Upsampling layer # 32x32x32
x = Conv2DTranspose(1, (3, 3), activation="sigmoid", padding="same")(x)  # Output layer # 32x32x1
decoded = keras.layers.Cropping2D(cropping=((2, 2), (2, 2)))(x)  # Crop to original size # 28x28x1

# Autoencoder model
autoencoder = Model(input_img, decoded)  # Autoencoder model
autoencoder.compile(optimizer="adam", loss="binary_crossentropy")  # Compile the model
# Train the autoencoder
autoencoder.fit(
    x_train, x_train, epochs=10, batch_size=256, shuffle=True, validation_data=(x_test, x_test)
)  # Train the model
# Evaluate the autoencoder
decoded_imgs = autoencoder.predict(x_test)  # Reconstruct the test images
# Display original and reconstructed images
import matplotlib.pyplot as plt

n = 10  # Number of images to display
plt.figure(figsize=(20, 4))
for i in range(n):
    # Display original images
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28), cmap="gray")
    plt.title("Original")
    plt.axis("off")

    # Display reconstructed images
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28), cmap="gray")
    plt.title("Reconstructed")
    plt.axis("off")
plt.show()
```
Code explained:

Unpacks into `x_train` (60,000 images), `x_test` (10,000 images), and ignores labels (`_`) since the autoencoder is unsupervised.

`np.expand_dims(array, axis)` adds a dimension at `axis=-1` (last axis). Conv2D layers expect a channel dimension (e.g., 1 for grayscale, 3 for RGB). MNIST images are grayscale, so we add a single channel.

`x_train.astype("float32") / 255.0` normalizes pixel values to the range [0, 1] for better training stability. Because neural networks work better with normalize inputs and `binary_crossentropy` loss function expects values in [0, 1]. Float32 is standard for GPU computations.

**Encoder**:

- `x = Conv2D(32, (3, 3), activation="relu", padding="same")(input_img)`
  - `Conv2D(filters, kernel_size, activation, padding)` creates a 2D convolutional layer.
  - `filters=32` specifies the number of filters (feature maps) to learn.
  - `kernel_size=(3, 3)` specifies the size of the convolutional kernel (3x3 pixels).
  - `activation="relu"` applies the ReLU activation function to introduce non-linearity.
  - `padding="same"` keeps the output size the same as the input size by adding padding around the input (28 x 28)
  - `input_img` is the input tensor.

- `x = MaxPooling2D((2, 2), padding="same")(x)`
  - Downsamples the feature maps, reducing computational load and capturing dominant features. 
  - `MaxPooling2D(pool_size, padding)` creates a max pooling layer, reducing spacial dimensions by taking the maximum value in each 2x2 region.
  - `pool_size=(2, 2)` specifies the size of the pooling window (2x2 pixels). 
  - `padding="same"` ensures that the output size is the same as the input size by adding padding around the input.
  - `x` is the input tensor from the previous layer.

- `x = Conv2D(64, (3, 3), activation="relu", padding="same")(x)`
  - Applies another convolutional layer to extract more complex features from the downsampled input.
  - `filters=64` specifies the number of filters (feature maps) to learn.

- `x = Flatten()(x)` flattens the feature maps into a 1D vector, preparing it for the fully connected layer. The output shape is (4 * 4 * 128) = 2048.
  
- `encoded = Dense(latent_dim, activation="relu")(x)` creates a fully connected layer with `latent_dim` (64) neurons. This layer learns a compressed representation of the input data in the latent space.

**Decoder**:

- `x = Dense(np.prod((4, 4, 128)), activation="relu")(encoded)`
  - creates a fully connected layer that expands the latent representation back to the flattened shape of the feature maps (4 * 4 * 128 = 2048). This layer learns to reconstruct the feature maps from the latent representation.
  - Maps the latent space back to the size of the encoder’s last feature map.
  
- `x = Reshape((4, 4, 128))(x)`  
  - Reshapes the output of the dense layer back into the shape of the feature maps (4x4x128) for further processing in the decoder.
  - Reshapes the flattened vector back into a 4D tensor for the next layers.

- `x = UpSampling2D((2, 2))(x)` upscales the feature maps by a factor of 2, increasing the spatial dimensions from (4x4) to (8x8).
  
- `x = Conv2DTranspose(64, (3, 3), activation="relu", padding="same")(x)`
  -  `Conv2DTranspose(filters, kernel_size, activation, padding)` performs transposed convolution (deconvolution) to upsample and apply filters.
  -  `filters=64` specifies the number of filters (feature maps) to learn.
  -  `kernel_size=(3, 3)` specifies the size of the transposed convolutional kernel (3x3 pixels).
  -  `activation="relu"` applies the ReLU activation function to introduce non-linearity.
  -  `padding="same"` keeps the output size the same as the input size by adding padding around the input (8x8).
  -  shape: (None, 8, 8, 64), where None is the batch size.
  
- `x = UpSampling2D((2, 2))(x)` upscales the feature maps by a factor of 2, increasing the spatial dimensions from (8x8) to (16x16).
  
- `x = Conv2DTranspose(32, (3, 3), activation="relu", padding="same")(x)`
  - Applies another transposed convolutional layer to upsample and apply filters.
  - `filters=32` specifies the number of filters (feature maps) to learn.
  - shape: (None, 16, 16, 32)


- `x = UpSampling2D((2, 2))(x)` upscales the feature maps by a factor of 2, increasing the spatial dimensions from (16x16) to (32x32).
  
- `x = Conv2DTranspose(1, (3, 3), activation="sigmoid", padding="same")(x)` 
  - Output layer that reconstructs the final image.
  - `filters=1` specifies the number of filters (feature maps) to learn (1 for grayscale images).
  - `kernel_size=(3, 3)` specifies the size of the transposed convolutional kernel (3x3 pixels).
  - `activation="sigmoid"` applies the sigmoid activation function to produce pixel values in the range [0, 1].
  - `padding="same"` keeps the output size the same as the input size by adding padding around the input (32x32).
  - shape: (None, 32, 32, 1)
  
- `autoencoder.fit(x_train, x_train, epochs=10, batch_size=256, shuffle=True, validation_data=(x_test, x_test))`
  -  `x_train, x_train`: Input and target are the same (autoencoder reconstructs inputs).
  - Trains the autoencoder on the training data (x_train) to reconstruct the input data. The model is trained for 10 epochs with a batch size of 256, shuffling the data each epoch.
  - `validation_data=(x_test, x_test)` uses the test data for validation during training.
  - **Note**: Ensure to monitor the loss during training to avoid overfitting.

- `decoded_imgs = autoencoder.predict(x_test)` reconstructs the test images using the trained autoencoder. The output is a set of reconstructed images.

## CAE vs CNN
**When to not use an autoencoder:**

  - When the data is not suitable for unsupervised learning, such as when labeled data is available for supervised learning tasks.
  - When the model complexity is not justified by the amount of data available, as autoencoders can be prone to overfitting if the dataset is small.
  - When the task requires a specific architecture or model type that is not well-suited for autoencoders, such as certain types of time series analysis or structured data.

In short:

- Use **CAE + classifier** if your dataset is small, noisy, or partially labeled.

- Use **just CNN** if you have lots of clean labeled data and want simplicity.

#### Recurrent Autoencoders

Recurrent autoencoders are a type of autoencoder designed to work with **sequential data**, such as time series or natural language. They combine the principles of autoencoders and recurrent neural networks (RNNs) to learn compressed representations of sequences.

**Architecture**:

- **Encoder**: The encoder consists of an RNN (e.g., LSTM or GRU) that processes the input sequence and compresses it into a fixed-size latent representation. The encoder takes the input sequence and produces a hidden state that captures the information from the entire sequence.
  
- **Decoder**: The decoder is also an RNN that takes the latent representation and reconstructs the original sequence. It generates the output sequence step by step, using the latent representation as context.

#### Denoising Autoencoders

Denoising autoencoders are a type of autoencoder designed to learn robust representations of data by reconstructing the original input from a **corrupted version of it.**

They are particularly **useful for tasks like image denoising**, where the goal is to remove noise from images while preserving important features.

**Architecture**: The architecture is similar to a standard autoencoder, but the input to the encoder is a corrupted version of the original data. The autoencoder learns to reconstruct the original data from this corrupted input.

**Loss Function**: The loss function is typically the mean squared error (MSE) between the original input and the reconstructed output. The model learns to minimize this loss by learning to ignore the noise and focus on the underlying structure of the data.

## Comparison of Autoencoders

| Type             | Use Case                  |  Architecture | Advantages | Disadvantages |
|------------------|---------------------------|----------------|------------|---------------|
| Autoencoder      | General Purpose | Encoder + Decoder (symmetric) | Simple, effective for unsupervised learning | Limited to linear relationships |
| Undercomplete    | Dimensionality Reduction/Feature Extraction | Encoder + Decoder (symmetric) | Forces compression, captures essential features | May lose information |
| Stacked          | Hierarchical Feature Learning/Complex Data | Multi-layered Encoder + Decoder | Learns multiple levels of representation | More complex, requires more data |
| Convolutional    | Image Data |  Convolutional layers | Captures spatial hierarchies, effective for images | Limited to grid-like data |
| Recurrent        | Sequential Data | RNN-based/LSTM Encoder + Decoder | Captures temporal dependencies | Limited to sequential data |
| Denoising        | Noise Reduction | Encoder + Decoder (corrupted input) | Robust to noise, learns useful features | Requires careful corruption process |

## Hyperparameter Optimization (HPO)

**What is a Hyperparameter?**

A hyperparameter is a parameter whose value is set before the learning process begins. Hyperparameters are not learned from the data but are instead specified by the practitioner to control the learning process.

They are settings that govern the training of the model and can significantly impact its performance. Examples of hyperparameters include:

- **Learning rate**: The step size used in the optimization algorithm to update the model's parameters.

- **Batch size**: The number of training samples used in one iteration of the optimization algorithm.

- **Number of epochs**: The number of times the entire training dataset is passed through the model during training.

- **Number of layers and units**: The architecture of the neural network, including the number of hidden layers and the number of units in each layer.

- **Dropout rate**: The fraction of neurons to drop during training to prevent overfitting.

- **Regularization parameters**: Parameters that control the strength of regularization techniques, such as L1 or L2 regularization.

- **Activation functions**: The choice of activation functions used in the model (e.g., ReLU, sigmoid, tanh).

**Why is HPO Important?**

Hyperparameter optimization (HPO) is important because it can significantly impact the performance of machine learning models. 

Properly tuned hyperparameters can lead to better model accuracy, faster convergence, and improved generalization to unseen data. HPO helps in finding the best combination of hyperparameters that yield the best performance on a given task.

Poorly chosen hyperparameters can lead to underfitting or overfitting. 

Manual tuning becomes impractical for complex models with many hyperparameters.

**Validation Set**: A separate portion of data is used to evaluate hyperparameter performance, preventing overfitting to training data.

#### Challenges in HPO

**Computational Challenges**

- **Expensive to evaluate**:
  - Training machine learning models can be computationally expensive, especially for deep learning models.
  - Each hyperparameter configuration requires training the model from scratch, which can be time-consuming and resource-intensive.

- **High-dimensional search space**:
  - The search space for hyperparameters can be high-dimensional, especially when dealing with complex models with many hyperparameters.
  - This makes it challenging to explore all possible combinations of hyperparameters effectively.

- **No Direct Gradient**:
  - Hyperparameters are not directly optimized using gradient descent, making it difficult to find optimal values.
  - The optimization process may require more sophisticated techniques than standard gradient-based methods.

- **Overfitting**:
  - Hyperparameter tuning can lead to overfitting if the validation set is not representative of the test set.
  - Care must be taken to ensure that the model generalizes well to unseen data. 

#### Table of Hyperprameters (HPs)

| ML Algorithm | Main HPs | Optional HPs |
|--------------|----------|--------------|
| Linear Regression | Regularization (L1, L2) | Learning rate, Batch size |
| Logistic Regression | penalty (L1, L2), c, solver | Learning rate, Batch size |
| KNN | n_neighbors |  |
| SVM | C, kernel, epsilon (for SVR) | gamma, Degree (for polynomial kernel), coef0 |
| XG Boost | n_estimators, learning_rate, max_depth | min_child_weight, gamma, alpha, lambda |
| Bagging | n_estimators, base_estimator | bootstrap, bootstrap_features |
| Deep Learning | Learning rate, Batch size, Number of epochs, no. of hidden layers, optimizer, Activation functions, early stop patience | number of frozen layers (if transfer learning used) |
| CNN | Learning rate, Batch size, Number of epochs, no. of filters, kernel size, pool size | Dropout rate, Regularization (L1, L2), optimizer |
| K-means | n_clusters, | init, n_init, max_iter |
| DBSCAN | eps, min_samples | metric, algorithm |
| Random Forest | n_estimators, max_depth, min_samples_split | min_samples_leaf, max_features |
| LSTM | Number of units, Learning rate, Batch size | Dropout rate, Number of layers |
| PCA | n_components | whiten, svd_solver |

#### Parameter Initialization

Parameter initialization is crucial to NN performance

- **Why is it important?**
  - Proper initialization can help the model converge faster and avoid local minima.
  - Poor initialization can lead to slow convergence, vanishing/exploding gradients, and suboptimal solutions.

**Importance of weight initialization**:  

Weight initialization is important because it determines the starting point of the optimization process. Properly initialized weights can help the model learn effectively and converge to a good solution.

**Common weight initialization methods**:

- **Random Initialization**: Weights are initialized randomly, typically from a uniform or normal distribution. This breaks symmetry and allows different neurons to learn different features. 
  - `tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05)` is used for random initialization.
  
- **Xavier/Glorot Initialization**: Weights are initialized from a uniform distribution with a range based on the number of input and output units. This helps maintain the variance of activations across layers. `tf.keras.initializers.GlorotUniform()` is used for Xavier initialization.
  
- **He Initialization**: Similar to Xavier initialization but designed for ReLU activation functions. It scales the weights based on the number of input units, helping to prevent vanishing/exploding gradients. `tf.keras.initializers.HeNormal()` is used for He initialization.


#### Learning Rate Strategies

The learning rate determines the step size during optimization:
   - Too high: Model may diverge
   
   - Too low: Extremely slow convergence

A common approach is to run SGD with a fixed step-size for a few epochs and measure error and plot progress, and then adjust the learning rate based on the observed performance.

**Learning Rate Scheduling Techniques**

Learning rate scheduling involves strategies to reduce the learning rate during training to improve convergence and performance. A good learning rate is crucial for effective training:

**Too high**: Model may diverge (Gradient explosion)
**Too low**: Extremely slow convergence (Gradient vanishing)

#### Finding a Good Learning Rate

To find an optimal learning rate:

1. Train the model for a few hundred iterations, exponentially increasing the learning rate from a very small value to a very large value.
   
2. Plot and examine the loss function against the learning rate. 
   - Look for the learning rate where the loss starts to decrease rapidly. This is a good starting point for training.
  
   - Select a learning rate slightly lower than the point where the learning curve starts shooting back up
  
3. Reinitialize the model and train it with the selected learning rate.

#### Common Scheduling Techniques

Strategies to reduce learning rate during training called learning schedules.

**Power Scheduling**:

- Reduces the learning rate by a factor of 0.1 every few epochs. This is useful for models that converge quickly and need fine-tuning in later epochs.
  
**Exponential Scheduling**:

- Reduces the learning rate exponentially based on the epoch number. This is useful for models that require a gradual decrease in learning rate over time.

**Piecwise Constant Scheduling**:

- Reduces the learning rate at specific epochs. This is useful for models that require abrupt changes in learning rate at certain points during training.

**Performance-based Scheduling (Most Common)**:

Reduces the learning rate based on the performance of the model on the validation set. This is useful for models that require dynamic adjustments to the learning rate based on performance.
  - Measure validation error after each epoch and reduce the learning rate if the error does not improve for a certain number of epochs.
  - Reduce the learning rate by a factor of x if the validation error does not improve for 5 epochs.

- `tf.keras.callbacks.ReduceLROnPlateau` is used for performance-based scheduling.

```python
from tensorflow.keras.callbacks import ReduceLROnPlateau

lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    verbose=1
)

model.fit(x_train, y_train, epochs=50, callbacks=[lr_scheduler], validation_data=(x_val, y_val))
```

#### HPO Search Strategies

**Trial and Error (Grad Student Descent)**:

- 100% Manually tuning hyperparameters based on intuition and experience.
- Not systematic and can be time-consuming.
- Not recommended for complex models with many hyperparameters.
  
**Grid Search**:

- Most commonly used method
- Exhaustively searches through a predefined set of hyperparameter values.
- Evaluates the cartesian product of all hyperparameter combinations.
- Suitable for small search spaces.
- It is simple to implement but can be computationally expensive, especially for large search spaces.

Grid search treats all values equally, so it may not be efficient for large search spaces. It can also lead to overfitting if the validation set is not representative of the test set.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
model = RandomForestClassifier()

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Perform grid search
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Evaluate the best model
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy:", accuracy)
```
- In this example, we use the Iris dataset and a Random Forest classifier. We define a hyperparameter grid for `n_estimators`, `max_depth`, and `min_samples_split`. The `GridSearchCV` function performs grid search with 5-fold cross-validation and evaluates the model using accuracy as the scoring metric. Finally, we print the best hyperparameters and test accuracy.

**Random Search**:

- Randomly samples hyperparameter values from a predefined distribution.
- More efficient than grid search for large search spaces.
- Can find good hyperparameters faster than grid search.
- Less exhaustive than grid search, so it may miss the optimal combination.
- `sklearn.model_selection.RandomizedSearchCV` is used for random search.
- **Problem:** May perform unnecessary evaluations as it does not exploit previously well-performing regions.
  
```python
# Define the hyperparameter distribution
param_dist = {
    'n_estimators': randint(50, 200),
    'max_depth': [None, 10, 20],
    'min_samples_split': randint(2, 10)
}
# Perform random search
random_search = RandomizedSearchCV(
    model,
    param_distributions=param_dist,
    n_iter=10,  # Number of random combinations to try
    cv=5,
    scoring='accuracy',
    random_state=42
)
random_search.fit(X_train, y_train)
# Get the best hyperparameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)
# Evaluate the best model
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy:", accuracy)
```

**Gradient-Based Optimization**:

- Traditional technique that moves in the opposite direction of the largest gradient to locate the next point.
- It is often used in conjunction with deep learning models to optimize loss functions.
- This method can converge quickly
- **Problem:** May get stuck in local optima and is not suitable for discrete or categorical hyperparameters.
- `sklearn.model_selection.BayesSearchCV` is used for gradient-based optimization.

#### HPO Frameworks

**Sklearn**:

- `sklearn.model_selection.GridSearchCV` and `sklearn.model_selection.RandomizedSearchCV` are used for grid search and random search, respectively.
  
- `sklearn.model_selection.BayesSearchCV` is used for gradient-based optimization.

**TensorFlow**: 

- `tf.keras.callbacks.LearningRateScheduler` is used for learning rate scheduling.
- `tf.keras.callbacks.ReduceLROnPlateau` is used for performance-based learning rate scheduling.
- `Trieste` is a library for Bayesian optimization in TensorFlow.
- `Keras Tuner` is a library for hyperparameter tuning in Keras models. It provides various search algorithms, including random search, hyperband, and Bayesian optimization.

# Lecture 13 

## CNNs

Convolutional Neural Networks (CNNs) are a type of deep learning model specifically designed for processing grid-like data, such as images.

It’s efficient at recognizing spatial patterns using **fewer parameters than traditional fully connected neural networks.**

They are widely used in computer vision tasks, including image classification, object detection, and image segmentation.

Traditional machine learning techniques and simple neural networks often struggle when it comes to working with large, high-dimensional images. This is because the input data (e.g., pixels in an image) is vast, and feeding this directly into a fully connected neural network would require an enormous number of parameters, making the process computationally expensive and inefficient.
   - A 32x32 pixel image with 3 color channels (RGB) would have 32 * 32 * 3 = 3072 input features.
   - A fully connected layer with 1000 neurons would require 3072 * 1000 = 3,072,000 parameters just for the first layer.
   - This is not feasible for larger images or deeper networks.

CNNs overcome these challenges by **using partial connections and shared weights**, which significantly reduce the number of parameters and improve computational efficiency.

**CNN VS DNN**

| Feature | CNN | DNN |
|---------|-----|-----|
| Architecture | Composed of convolutional layers, pooling layers, and fully connected layers | Composed of fully connected layers |
| Input Type | Primarily designed for grid-like data (e.g., images) | Can handle various types of data |
| Parameter Sharing | Uses shared weights in convolutional layers | No parameter sharing |
| Local Connectivity | Convolutional layers connect to local regions of the input | Fully connected layers connect to all input features |
| Spatial Hierarchy | Captures spatial hierarchies and local patterns | Does not capture spatial hierarchies |


**Convolution:** when a filter (which is a small matrix (e.g., 3x3 or 5x5)) slides over the input data, performing **element-wise multiplication** and summation to **produce a feature map** (a new representation of the input image that highlights the detected features). The same filter is applied across the entire input, allowing the model to learn spatial hierarchies and local patterns.

CNNs learn increasingly complex patterns layer by layer:

  - Early layers detect simple patterns (e.g., edges, corners).

  - Middle layers combine these to detect shapes or textures (e.g., eyes, wheels).

  - Deeper layers recognize high-level concepts (e.g., faces, cars, tumors).

**Weight Sharing**: 

The same filter (kernel) (which is the same set of weights) is applied to different regions of the input image, allowing the model to learn spatial hierarchies and local patterns. This reduces the number of parameters significantly.

For example, if a filter has a size of (3, 3) and the input image has a size of 32 x 32 pixels with 3 input channels and 32 output channels (where each output channel corresponds to a different feature map), the number of weights for the filter is (3 x 3 x 3 x 32) = 864 weights. If the input image has 32 x 32 pixels and 3 input channels, the number of weights in a fully connected layer would be (32 x 32 x 3) x (32 x 32 x 3) = 3072 x 3072 = 9,437,184 weights.

This approach allows the network to recognize patterns (e.g., edges) regardless of their location in the image (known as **translation invariance**). This means that if an object appears in different locations in the image, the CNN can still recognize it.


**Partially Connected Layers**: 

Each output pixel in a feature map is computed from a small receptive field (e.g., 3x3 patch) of the input, not the entire input.

This limits connections to local regions, reducing computation and capturing local patterns (e.g., edges) rather than global ones.

**Example:** For a 32x32 input, each output pixel connects to only 9 input pixels (3x3), not all 1024.

## Components of CNNs

#### Input Layer

Takes image input: shape = (height, width, channels)

For images the input is a tensor of shape `(batch_size, height, width, channels)`, where:
  -  `batch_size` is the number of images in a batch, 
  -  `height` and `width` are the dimensions of the image
  -  `channels` is the number of color channels (e.g., 1 for grayscale, 3 for RGB).

In python the input layer is defined using `tf.keras.layers.Input(shape=(height, width, channels))`, where `shape` specifies the dimensions of the input data.

#### Convolutional Layer
Applies filters (kernels) to the input data, producing feature maps to extract features

The filter (a small matrix) is passed over the input image, performing element-wise multiplication and summation to produce a single value in the output feature map, in a process called convolution.
  - **Feature Maps**: The output of the convolutional layer is a set of feature maps, each corresponding to a different filter. Each feature map highlights different patterns in the input data (such as edges, textures, or shapes).
  - X number of filters = X number of feature maps
  - **Stride**: The number of pixels the filter moves over the input image during convolution. A stride of 1 means the filter moves one pixel at a time. Increasing the stride reduces the size of the output feature map.
  - **Padding**: The process of adding extra pixels around the input image to control the size of the output feature map. There are two types of padding:
    - **Valid Padding**: No padding is added, and the output feature map is smaller than the input image.
    - **Same Padding**: Padding is added to ensure that the output feature map has the same dimensions as the input image.

Conv2D layer is defined using `tf.keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', activation=None)`, where:
  - `filters` is the number of filters (feature maps) to learn.
  - `kernel_size` is the size of the filter (e.g., `(3, 3)`).
  - `strides` is the stride of the convolution (default is `(1, 1)`).
  - `padding` specifies the type of padding (`'valid'` or `'same'`).
  - `activation` specifies the activation function to apply (e.g., `relu'`). This introduces non-linearity to the model, allowing it to learn complex patterns.
    -  **ReLU (Rectified Linear Unit):** Commonly used in CNNs to introduce non-linearity, helping the model learn more complex patterns.
    -  **Softmax:** Used in the output layer for multi-class classification tasks, producing probabilities for each class.

#### Pooling Layer

Used to reduce the spatial dimensions (height and width) of the image while retaining important features.

**Downsamples feature maps** by summarizing regions (e.g., taking the maximum value in a 3x3 window).

This is especially useful in CNNs because it reduces computational complexity and helps prevent overfitting.

**Types**: Max pooling (selects max), average pooling (computes average).

##### Max Pooling (Most Common)

In max pooling, a filter (usually 2x2) slides over the image, and for each region, it selects the maximum value. This reduces the spatial dimensions of the image but retains the most important features.

In python, max pooling is defined using:

`tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')`, where:

- `pool_size` is the size of the pooling window (e.g., `(2, 2)`).
- `strides` is the stride of the pooling operation (default is `(2, 2)`).
- `padding` specifies the type of padding (`'valid'` or `'same'`).

eg. it reduces spacial dimensions by a factor of 2, so a 32x32 image becomes a 16x16 image.

##### Average Pooling
In average pooling, a filter (usually 2x2) slides over the image, and for each region, it computes the average value. This also reduces the spatial dimensions of the image but retains more information than max pooling.

When to use Max pooling vs Average pooling:

- **Max Pooling**: Preferred when the goal is to retain the most prominent features in the image, such as edges or textures. It is commonly used in CNNs for image classification tasks.
- **Average Pooling**: Preferred when the goal is to retain more information about the overall structure of the image, such as in tasks like semantic segmentation or when the input data is noisy.

#### Flattening Layer
Converts 2D feature maps into 1D vector to prepare for fully connected layers.

The flattening layer reshapes the output of the last pooling layer into a 1D vector, which can then be passed to fully connected layers for classification or regression tasks.

This is done using `tf.keras.layers.Flatten()`, which reshapes the input tensor into a 1D vector.

For example, if the output of the last pooling layer is (None, 4, 4, 128), flattening it will result in a shape of (None, 2048) as 4x4x128 = 2048.

#### Fully Connected Layer (Dense Layer)

The Fully Connected Layer (Dense Layer) is where the network makes final decisions.

 After several convolutions and pooling operations, the network has learned abstract features of the image. These features are passed to a fully connected layer for classification.

After feature extraction through convolution and pooling, the flattened feature map is passed to a fully connected layer. This layer uses learned weights and biases to compute a final output, such as a classification label.

- `tf.keras.layers.Dense(units, activation=None)` is used to create a fully connected layer, where:
  - `units` is the number of neurons in the layer.
  - `activation` specifies the activation function to apply

- **Output:** (batch size, units), e.g., (None, 128) for latent_dim=128.

## CNN Optimization and Techniques

**Dropout**: 
A regularization technique that randomly sets a fraction of the input units to 0 during training to prevent overfitting. This forces the network to learn more robust features.

- `tf.keras.layers.Dropout(rate)` is used to create a dropout layer, where:
  - `rate` is the fraction of input units to drop (e.g., `0.5` for 50% dropout).
- **Example**: If the input to the dropout layer is (None, 128), and the dropout rate is 0.5, the output will be (None, 64) as half of the input units are dropped.

**Batch Normalization**: A technique to normalize the inputs of each layer to improve training speed and stability. It helps in reducing internal covariate shift and allows for higher learning rates.
- `tf.keras.layers.BatchNormalization()` is used to create a batch normalization layer.
- It normalizes the input to have a mean of 0 and a standard deviation of 1, which helps in stabilizing the training process.

Python example of a simple CNN model using Keras:

```python
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import numpy as np

# Load and preprocess the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = np.expand_dims(x_train, axis=-1).astype('float32') / 255.0
x_test = np.expand_dims(x_test, axis=-1).astype('float32') / 255.0
y_train = to_categorical(y_train, num_classes=10)
y_test = to_categorical(y_test, num_classes=10)

# Define the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),
    
    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),
    
    Flatten(),
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    
    Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))

# Evaluate the model
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')
```

**Data Augmentation in CNNs**

Data augmentation artificially increases the size of a dataset by applying transformations such as rotation, flipping, scaling, and shifting to existing images.

**Benefits**:

- Helps prevent overfitting by introducing variability in the training data.
  
- Improves the model's ability to generalize to unseen data, making the CNNs robost to different orientations and distortions of the input data.
  
- Mimics real-world scenarios where images may be captured from different angles or under different conditions.

Example: Applying Data Augmentation using Keras
```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Create an instance of ImageDataGenerator for data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,  # Randomly rotate images by up to 20 degrees
    width_shift_range=0.2,  # Randomly shift images horizontally by up to 20% of the width
    height_shift_range=0.2,  # Randomly shift images vertically by up to 20% of the height
    shear_range=0.2,  # Randomly apply shear transformations
    zoom_range=0.2,  # Randomly zoom in on images
    horizontal_flip=True,  # Randomly flip images horizontally
    fill_mode='nearest'  # Fill in missing pixels after transformations
)

# Fit the data generator to the training data
datagen.fit(x_train)

# Train the model using the data generator
model.fit(datagen.flow(x_train, y_train, batch_size=128), epochs=10, validation_data=(x_test, y_test))
```

## Training CNNs

Training a CNN involves two key steps: forward propagation (where the network makes predictions) and backpropagation (where it learns from its mistakes).

#### Forward Propagation
- Each layer applies transformations to the input (convolution, activation, pooling).
- Features are extracted progressively.
- Fully connected layers use these features to make predictions
- The output is compared to the true labels using a loss function (e.g., **categorical crossentropy for multi-class classification** or **binary crossentropy for binary classification**).
- The loss function quantifies the difference between the predicted output and the true labels.
- The loss is used to update the model's weights during backpropagation.

#### Backpropagation
- Gradients are computed using the chain rule
- The gradients are used to update the model's weights using an optimization algorithm (e.g., stochastic gradient descent (SGD), Adam).
- The optimizer adjusts the weights to minimize the loss function.
- The process repeats until the model learns to make accurate predictions.
- The model is trained for a specified number of epochs, where each epoch consists of multiple iterations over the training data (batches).
- The training process involves feeding batches of data through the network, computing the loss, and updating the weights using backpropagation.

**Adam Optimizer**: 
A type of SGD that adapts the learning rate for each parameter based on the first and second moments of the gradients. It combines the benefits of AdaGrad and RMSProp, making it effective for training deep neural networks.

Uses adaptive learning rates + momentum (tracks past gradients).


#### Real World Applications of CNNs

- **Image Classification**: Classifying images into predefined categories (e.g., identifying objects in images).
- **Object Detection**: Locating and classifying multiple objects within an image (e.g., detecting faces in photos).
- **Image Segmentation**: Dividing an image into meaningful segments (e.g., identifying different regions in a medical image).
- **Facial Recognition**: Identifying and verifying individuals based on their facial features.
- **Medical Image Analysis**: Analyzing medical images (e.g., X-rays, MRIs) for disease detection and diagnosis.
- **Self-driving Cars**: Detecting and recognizing objects (e.g., pedestrians, traffic signs) in real-time for autonomous navigation.

Several famous CNN architectures have significantly influenced deep learning:

| Architecture | Year | Description |
|--------------|------|-------------|
| LeNet-5 | 1998 | One of the first CNNs, designed for handwritten digit recognition (MNIST dataset). |
| AlexNet | 2012 | Won the ImageNet competition, introduced ReLU activation and dropout. |
| VGGNet | 2014 | Deep architecture with small (3x3) filters, known for its simplicity and depth. |
| ResNet | 2015 | Introduced residual connections, allowing for very deep networks (up to 152 layers). |
| YOLO | 2016 | Real-time object detection system that predicts bounding boxes and class probabilities directly from full images. |

## Adversarial Examples & Generative Models

**Adversarial examples** are inputs to machine learning models that have been intentionally perturbed (slighty changed`) to cause the model to make incorrect predictions. These perturbations are often imperceptible (hard to notice) to humans but can significantly affect the model's performance.

**Generative models** are a class of machine learning models (such as Generative Adversarial Networks - GANs) that learn to generate new data samples from a given distribution. They can be used to create realistic images, text, or other types of data. Generative models can also be used to create adversarial examples by generating inputs that are specifically designed to fool a model.

These attacks occur across domains—computer vision (e.g., misclassifying images), natural language processing (e.g., altering text sentiment), and cybersecurity (e.g., evading malware detection).

Two types of attacks in DL: White-Box and Black-Box attacks

- **White-Box Attacks**: The attacker has full knowledge of the model architecture, parameters, and training data. This allows them to craft adversarial examples that exploit specific weaknesses in the model.

- **Black-Box Attacks**: The attacker has no knowledge of the model architecture or parameters. They can only query the model to obtain predictions. This makes it more challenging to create adversarial examples, but it is still possible using techniques like transferability (where adversarial examples generated for one model can also fool another model).


#### Generating Adversarial Examples

Several methods exist to craft adversarial examples, including:

- **Fast Gradient Sign Method (FGSM):** Uses the sign of the gradient of the loss with respect to the input to create perturbations.
  
- **Projected Gradient Descent (PGD):** An iterative version of FGSM, applying multiple small perturbations.
  
- **Carlini & Wagner (C&W) Attack:** Optimizes perturbations to be minimal yet effective.

#### Defending Against Adversarial Attacks

**Adversarial Training:** Incorporates adversarial examples into the training set to teach the model to recognize them.

**Defensive Distillation:** Uses a distilled model to smooth predictions, making attacks harder.

**Input Preprocessing:** Applies techniques like JPEG compression or feature squeezing to reduce perturbation effects.

## Generative Adversarial Networks (GANs)

Generative models learn to produce data resembling a training distribution, such as images or text, from random noise. Unlike discriminative models, which classify inputs, generative models create new samples.

GANs consist of two competing neural networks:

**Generator:** Maps random noise to synthetic data (e.g., images).

**Discriminator:** Evaluates whether a sample is real (from the training set) or fake (from the generator).

The two networks are trained adversially: the generator improves by trying to “fool” the discriminator, while the discriminator improves by better distinguishing real from fake.

#### Training a GAN

Training involves two phases per iteration:

**Discriminator Training:** Uses real images (labeled 1) and fake images (labeled 0) to improve classification.

**Generator Training:** Generates fake images, with the discriminator providing feedback (all labeled as 1).

Challenges include:

- Mode Collapse: The generator produces limited variety.
  
- Vanishing Gradients: Imbalance between generator and discriminator learning rates.

#### Applications of GANs
- Image Generation: Creating realistic images from random noise.
- Deepfake Generation: Creating realistic fake videos or images.
- Data Augmentation: Generating synthetic data for training ML models.
- Style Transfer and Super-Resolution: Enhancing or stylizing images.

# Lecture 14

## Federated Learning (FL)

Federated Learning is a distributed machine learning approach that allows multiple devices or servers to **collaboratively train a model without sharing their raw data.**

Instead of sending data to a central server, each device trains the model locally and only **shares the model updates (gradients)** with the server. 

This approach **enhances privacy**, **reduces data transfer costs**, and **enables training on decentralized data sources**.

**How Does FL Work?**

1. **Model Distribution:** A central server shares a pre-trained model with remote devices.
2. **Local Training:** Each device trains the model on its private data.
3. **Update Sharing:** Devices summarize and encrypt model updates (e.g., gradients) and send them to the server.
4. **Aggregation:** The server decrypts, averages updates, and integrates them into the global model.
5. **Iteration:** Steps repeat until the model is fully trained.

**Privacy Guarantee:** Devices exchange model gradients, not raw data.

#### Training Process

**Horizontal Learning:** Trains on similar datasets across devices.

**Vertical Learning:** Combines complementary data (e.g., movie and book reviews to predict music preferences).

**Transfer Learning:** Adapts a pre-trained model to a new task (e.g., car detection to cat detection).

## Transfer Learning (TL)

Transfer Learning is a machine learning technique where a model developed for one task is reused as the starting point for a model on a second task. It is particularly useful when the second task has limited labeled data.

Involves **fine-tuning a model pre-trained on one task for a new, related task**, offering time and cost savings.

**Why Use Transfer Learning?**

- **Limited Data**: When labeled data is scarce for the target task, transfer learning allows leveraging knowledge from a related task with abundant data.

- **Faster Training**: Pre-trained models can significantly reduce training time, as they already have learned useful features.

- **Improved Performance**: Transfer learning often leads to better performance on the target task, as the model starts with a good set of features learned from the source task.

#### TL Strategies

1. **Transductive Transfer Learning:**
   
- Transfers knowledge from a specific source to a related target domain.
- **Advantage:** Works with little/no labeled target data.
- **Example:** Sentiment analysis model from product reviews adapted for movie reviews.
  
2. **Inductive Transfer Learning:**
- Same domain, different tasks.
- **Advantage:** Faster training with pre-trained familiarity.
- **Example:** NLP model pre-trained on text, fine-tuned for sentiment analysis.
  
3. **Unsupervised Transfer Learning:**
- Uses unlabeled data in source and target domains.
- **Example:** Identifying motorcycle types from unlabeled vehicle images.

#### TL Steps

1. **Select Pre-trained Model:** Choose a model with prior knowledge (e.g., ImageNet for images).
   
2. **Configure Model:**
   - Freeze Layers: Preserve source knowledge by locking pre-trained layers.
   - Adjust Weights: Start with random weights and refine during training.
   - Modify Layers: Remove the last layer and add new layers for the target task, ensuring compatibility with the new output classes.
  
3. **Train on Target Domain:** Fine-tune with target data.

Python Example: Transfer Learning with TensorFlow

```python
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras import layers, models

# Load pre-trained VGG16 model (without top layer)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze pre-trained layers
base_model.trainable = False

# Add custom layers for new task
model = models.Sequential([
    base_model,
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # Binary classification (e.g., cats vs. dogs)
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Summary
model.summary()
```

- In this example, we load the pre-trained VGG16 model without the top layer and freeze its layers. We then add custom layers for a binary classification task (e.g., cats vs. dogs). Finally, we compile the model and print its summary.
  
- The top layer is removed to adapt the model to a new task, and new layers are added for the specific classification task.
  
- The model can be trained on a new dataset using the `model.fit()` method, similar to training a standard Keras model.

#### Privacy-Accuracy Trade-off

**Issue:** Attackers may attempt to steal user data or hijack an AI model.

**Challenge in FL:** When a data host exchanges their working model with the central server, it improves the model but leaves the data used for training vulnerable to inference attacks.

**Reason for Exchange:** Each exchange enhances the model but increases the risk of privacy breaches.

**Key Concern:** The more rounds of information exchanged, the easier it becomes to infer sensitive information.

**Current Trend:** Research focuses on minimizing and neutralizing privacy threats to ensure secure federated learning.

#### Other Challenges in FL

1. **High Network Bandwidth:** Communication between devices and the central server can be resource-intensive.
2. **Transparency:** Ensuring that training data remains private while maintaining:
   - Testing accuracy.
   - Fairness in predictions.
   - Mitigation of potential biases in the model’s output.
3. **Accountability:** Logging and auditing each stage of the training pipeline to ensure traceability.
4. **Data Control:**
   - Key Questions:
     - What data are used to train the model?
     - How can data be deleted when a host leaves the federation?
   - Rule: If data are deleted, all parties are obligated to retrain the model from scratch to ensure compliance.
5. **Trust Issues:** Establishing trust among participants in the federation to prevent malicious behavior or data misuse.

## Reinforcement Learning (RL)

Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions, allowing it to learn optimal strategies over time.

**Key idea:** An agent learns to perform tasks through trial-and-error interactions with a dynamic environment.

- No static dataset required.
- Agent learns from its own experiences.
- Operates without human supervision, guided by rewards or punishments.
- Focuses on maximizing cumulative rewards over time.
- RL is used in various applications, including robotics, game playing, and autonomous systems.

#### Goal of RL

**Objective:** Maximize the total cumulative reward for the agent.

**Process:** The agent solves problems through its own actions, receiving feedback from the environment.

**Advantages:**
- No need for data collection, preprocessing, or labeling prior to training.
- Can autonomously learn behaviors with the right incentives (positive rewards or negative punishments).

#### RL Workflow

- Environment: The external system with which the agent interacts.
- Reward definition: The feedback signal received by the agent after taking an action.
- Create Agent: The learner or decision-maker that interacts with the environment.
- Action: The choices made by the agent to interact with the environment.
- State: The current situation of the agent in the environment.
- Training: The process of learning from interactions with the environment to improve the agent's performance.
- Evaluation: Assessing the agent's performance based on its ability to maximize rewards.
- Deployment: Implementing the trained agent in a real-world scenario or application.

**Types of RL:**
- **Policy-Based RL:**:**
  - Directly learns a policy (a mapping from states to actions).
  - Uses a policy gradient method to optimize the policy.
  - Example: Proximal Policy Optimization (PPO).
- **Value-Based RL:**:
  - Learns a value function (a mapping from states to expected rewards).
  - Uses Q-learning or Deep Q-Networks (DQN) to estimate the value of actions.
- **Model-Based RL:**:
  - Builds a model of the environment to predict future states and rewards.
  - Uses the model to plan actions and improve learning efficiency.
  - Example: AlphaZero, which combines deep learning with Monte Carlo Tree Search (MCTS).

#### RL Examples

Example 1: Car Parking

**Goal:** Teach a vehicle (agent) to park in a designated spot.
**Environment:** Includes vehicle dynamics, nearby vehicles, weather, etc.
**Training:**
- Uses sensor data (cameras, GPS, LIDAR) to generate actions (steering, braking, acceleration).
- Trial-and-error process tunes the policy.
**Reward Signal:** Evaluates trial success and guides learning.
**Reference:** MathWorks: Reinforcement Learning

**Challenges:** Sample inefficiency, interpretability, and environment constraints remain hurdles.

## Explainable Artificial Intelligence (XAI)

Explainable Artificial Intelligence (XAI) refers to methods and techniques that make the decisions and predictions of AI models more **understandable and interpretable to humans**.

**Goal of XAI** is to produce more explainable models while maintaining high
prediction accuracy, enabling humans to understand, trust, and manage AI
systems.

XAI is crucial for ensuring transparency, accountability, and ethical use of AI technologies, especially in high-stakes applications like healthcare, finance, and autonomous systems.

Rising ethical concerns around AI necessitate transparency.

**XAI Components:**
- Prediction accuracy.
- Traceability (narrowed scope of rules/features).
- Decision understanding (building human trust).
- Applications: Healthcare (e.g., diagnostics), Finance (e.g., fraud detection).

**Explainability vs. Interpretability**

- **Explainability**: The ability to explain the model's predictions and decisions in a human-understandable way. (**focus on process**)

- **Interpretability**: The degree to which a human can understand the cause of a decision made by a model. (**focus on outcome**)

**XAI Challenges**
- **Complexity:** Balancing explanation with model accuracy.
- **Oversimplification Risk:** Simplified explanations may misrepresent intricate systems.
- **Terminology:** Lack of standardized definitions for explainability and interpretability.
- **Development:** Requires integrating XAI into existing AI workflows.

Libraries like `LIME` (Local Interpretable Model-agnostic Explanations) and `SHAP` (SHapley Additive exPlanations) provide tools for generating explanations for model predictions.

# Lecture 15

## Responsible artificial intelligence (RAI)

Responsible AI (RAI) refers to the ethical and accountable development, deployment, and use of artificial intelligence systems. It encompasses principles, practices, and frameworks that ensure AI technologies are designed and used in ways that align with societal values and ethical standards.

**Aim:** To embed ethical principles into AI applications.

**6 Key Principles of RAI:**

- **Fairness:** Ensuring AI systems are unbiased and do not discriminate against individuals or groups
  
- **Transparency:** Making AI systems understandable and explainable to users and stakeholders
  
- **Accountability:** Establishing clear lines of responsibility for AI system outcomes
  
- **Privacy & Security :** Protecting individuals' data and ensuring compliance with data protection regulations. Ensuring AI systems are robust and secure against adversarial attacks
  
- **Sustainability:** Considering the environmental impact of AI systems and promoting responsible resource use.
  
- **Inclusivity:** Ensuring diverse perspectives are considered in AI development and deployment


**RICE** is a framework for **evaluating the ethical implications of AI systems**. It emphasizes the importance of considering the broader societal impact of AI technologies and ensuring that they are developed and used in ways that align with ethical principles and societal values.

**RICE**
- **R**esponsible
- **I**nclusive
- **C**ompliant
- **E**thical


## AI Alignment

**Goal:** to enable enterprises to tailor AI models to follow their business rules
and policies. Alignment happens during fine-tuning, involving instruction-tuning and a critique
phase.

Importance of AI alignment stems from the increasing impact and risks of Deep Learning (DL)
applications, with misalignment being a significant source of these risks.

Misalignment has two main causes

- **Reward hacking:** The model learns to exploit loopholes in the reward system, leading to unintended consequences.
  
- **Misgeneralization:** The model performs well on training data but fails to generalize to unseen data, leading to poor performance in real-world scenarios.
  
- **Example:** A model trained to maximize clicks on a website may learn to generate misleading headlines that attract clicks but do not provide valuable content, leading to user dissatisfaction and loss of trust.


Formal verification methods like SAT, LP, and SMT are used for robust ML.

These methods help ensure that the model adheres to specified constraints and behaves as expected in various scenarios.

- **SAT (Satisfiability):** A method for determining if a logical formula can be satisfied by assigning values to its variables.
  
- **LP (Linear Programming):** A mathematical optimization technique used to find the best outcome in a linear model.
  
- **SMT (Satisfiability Modulo Theories):** An extension of SAT that incorporates theories like arithmetic and arrays, allowing for more complex reasoning about the model's behavior.
  
- **Example:** Using SAT to verify that a model's predictions are consistent with specified constraints, such as ensuring that a self-driving car does not exceed speed limits.
  
- **Example:** Using LP to optimize the allocation of resources in a supply chain while adhering to constraints like demand and capacity.
  
- **Example:** Using SMT to verify that a model's predictions are consistent with specified constraints, such as ensuring that a self-driving car does not exceed speed limits.